{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Textbook Enhancements "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Start TensorBoard\n",
    "\n",
    "References: [Get started with TensorBoard](https://www.tensorflow.org/tensorboard/get_started)\n",
    "\n",
    "Clear any logs from previous runs with:\n",
    "\n",
    "```bash\n",
    "# rm -rf ./logs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Set TensorFlow environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_FLAGS\"]=\"--xla_gpu_cuda_data_dir=/usr/local/cuda-11.1\"\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from IPython.display import clear_output, Markdown\n",
    "from numba import cuda\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yellowbrick as yb\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "from keras import backend as K"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the source data\n",
    "df_source = pd.read_csv('data/raw/DataCoSupplyChainDataset.csv', encoding='unicode_escape')\n",
    "df_source.drop_duplicates(inplace=True)\n",
    "\n",
    "print(df_source.shape)\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_source.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Data Preparation & Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Add the Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_is_fraud(df_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a new column to the dataframe that indicates\n",
    "    0: No Fraud and 1: Fraud\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_data : pd.DataFrame\n",
    "        The source dataframe\n",
    "    \"\"\"\n",
    "    df_data['is_fraud'] = df_data['Order Status'].apply(lambda x: 1 if x == 'SUSPECTED_FRAUD' else 0)\n",
    "    return df_data\n",
    "\n",
    "df_data = df_source.reset_index(drop=True)\n",
    "add_is_fraud(df_data)\n",
    "df_data['is_fraud'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Hour-Month\n",
    "\n",
    "This variable is calculated based on the variable \"order-date\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hour_month(df_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"from sklearn.model_selection import train_test_split\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_data : pd.DataFrame\n",
    "        The source dataframe\n",
    "    \"\"\"\n",
    "    df_data['order_date'] = pd.to_datetime(df_data['order date (DateOrders)'])\n",
    "    df_data['hour_month'] = (df_data['order_date'].dt.day * 24.0) + df_data['order_date'].dt.hour\n",
    "    return df_data\n",
    "\n",
    "add_hour_month(df_data)\n",
    "df_data['hour_month'].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Data Selection\n",
    "\n",
    "Select the subset of columns to use for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data \\\n",
    "    .filter([\n",
    "        'hour_month',\n",
    "        'is_fraud',\n",
    "        'Type',\n",
    "        'Sales per customer',\n",
    "        'Customer State',\n",
    "        'Order State',\n",
    "        'order_date',\n",
    "    ]) \\\n",
    "    .rename(columns={\n",
    "        'Type': 'payment_type',\n",
    "        'Sales per customer': 'sales_per_customer',\n",
    "        'Customer State': 'customer_state',\n",
    "        'Order State': 'order_state',\n",
    "    })\n",
    "\n",
    "print(df_data.shape)\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_data.head())    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.4 Set Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = {\n",
    "        'payment_type',\n",
    "        'customer_state',\n",
    "        'order_state'\n",
    "}\n",
    "\n",
    "for col in cat_columns:\n",
    "    df_data[col] = df_data[col].astype('category')\n",
    "\n",
    "df_data.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 One Hot Encode Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform one-hot encoding on the categorical columns \n",
    "    in the dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_data : pd.DataFrame\n",
    "        The source dataframe\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The dataframe with the columns one-hot encoded.\n",
    "    \"\"\"\n",
    "    df_output = df_data.copy()\n",
    "\n",
    "    # one-hot encode the categorical columns\n",
    "    for column in df_output.select_dtypes(include='category').columns:\n",
    "        df_output = pd.concat([df_output, pd.get_dummies(df_output[column], prefix=column)], axis=1)\n",
    "        df_output.drop(columns=[column], inplace=True)\n",
    "        \n",
    "    return df_output\n",
    "    \n",
    "df_features = one_hot_encode(df_data) \\\n",
    "    .drop(columns=['order_date'])\n",
    "\n",
    "print(df_features.shape)\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_features.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Normalize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(df_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scale the features in the dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_data : pd.DataFrame\n",
    "        The source dataframe\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The dataframe with the columns scaled.\n",
    "    \"\"\"\n",
    "    df_output = df_data.copy()\n",
    "    float_columns = df_output.select_dtypes(include='float64').columns\n",
    "\n",
    "    # scale the features\n",
    "    scaler = StandardScaler()\n",
    "    df_output[float_columns] = scaler.fit_transform(df_output[float_columns])\n",
    "\n",
    "    return df_output\n",
    "\n",
    "df_features = scale_features(df_features)\n",
    "\n",
    "print(df_features.shape)\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_features.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 105"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test, and validation sets\n",
    "df_train, df_test = train_test_split(df_features, test_size=0.2, random_state=random_state)\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.3, random_state=random_state)\n",
    "\n",
    "Markdown(f'''\n",
    "| Dataset | Rows | Columns | Not Fraud | Fraud |\n",
    "| ------- | ---- | ------- | --------- | ----- |\n",
    "| Train | {df_train.shape[0]} | {df_train.shape[1]} | {df_train[df_train['is_fraud'] == 0].shape[0]} | {df_train[df_train['is_fraud'] == 1].shape[0]} |\n",
    "| Validation | {df_val.shape[0]} | {df_val.shape[1]} | {df_val[df_val['is_fraud'] == 0].shape[0]} | {df_val[df_val['is_fraud'] == 1].shape[0]} |\n",
    "| Test | {df_test.shape[0]} | {df_test.shape[1]} | {df_test[df_test['is_fraud'] == 0].shape[0]} | {df_test[df_test['is_fraud'] == 1].shape[0]} |\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the features and labels\n",
    "x_train = df_train.drop(columns=['is_fraud'])\n",
    "y_train = df_train['is_fraud']\n",
    "\n",
    "x_val = df_val.drop(columns=['is_fraud'])\n",
    "y_val = df_val['is_fraud']\n",
    "\n",
    "x_test = df_test.drop(columns=['is_fraud'])\n",
    "y_test = df_test['is_fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to reclaim some memory\n",
    "del df_source\n",
    "del df_data\n",
    "del df_features\n",
    "\n",
    "del df_train\n",
    "del df_val\n",
    "del df_test\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model() -> models.Sequential:\n",
    "    \"\"\"\n",
    "    Get the deep learning model to use for training.\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(512, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.3 Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizers.RMSprop(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.4 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure tensorboard log dir\n",
    "log_dir = 'logs/initial/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "history = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    epochs=30,\n",
    "    batch_size=512,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history: callbacks.History) -> None:\n",
    "    \"\"\"\n",
    "    Plot the loss and validation loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    history : keras.callbacks.History\n",
    "        The history of the model training.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(epochs, history.history['loss'])\n",
    "    \n",
    "    if ('val_loss' in history.history):\n",
    "        plt.plot(epochs, history.history['val_loss'])\n",
    "        plt.legend(['Training loss', 'Validation loss'], loc='upper left')\n",
    "        plt.title('Training and validation loss')\n",
    "    else:\n",
    "        plt.title('Training loss')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(history: callbacks.History) -> None:\n",
    "    \"\"\"\n",
    "    Plot the accuracy and validation accuracy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    history : keras.callbacks.History\n",
    "        The history of the model training.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(epochs, history.history['accuracy'])\n",
    "\n",
    "    if ('val_accuracy' in history.history):\n",
    "        plt.plot(epochs, history.history['val_accuracy'])\n",
    "        plt.legend(['Training acc', 'Validation acc'], loc='upper left')\n",
    "        plt.title('Training and validation accuracy')\n",
    "    else:\n",
    "        plt.title('Training accuracy')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)\n",
    "plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_loss_accuracy(model: models.Sequential) -> None:\n",
    "    \"\"\"\n",
    "    Show the loss and accuracy for the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : keras.models.Sequential\n",
    "        The model to evaluate.\n",
    "    \"\"\"\n",
    "    print('-- Training --')\n",
    "    train_loss, train_acc = model.evaluate(x_train, y_train)\n",
    "\n",
    "    print('-- Validation --')\n",
    "    val_loss, val_acc = model.evaluate(x_val, y_val)\n",
    "\n",
    "    print('-- Test --')\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the accuracy and loss on the data sets\n",
    "show_loss_accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open tensorboard\n",
    "%tensorboard --logdir logs/initial/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x: pd.DataFrame, y: pd.Series, model: models.Sequential, threshold: float = 0.5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Predict the labels for the features to use for model evaluation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : pd.DataFrame\n",
    "        The features\n",
    "    y : pd.Series\n",
    "        The labels\n",
    "    model : models.Sequential\n",
    "        The model\n",
    "    threshold : float, optional\n",
    "        The threshold to use for the predictions, by default 0.5\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The confusion matrix\n",
    "    \"\"\"\n",
    "    y_score = model.predict(x, use_multiprocessing=True).ravel()\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'y_true': y,\n",
    "        'y_score': y_score,\n",
    "        'y_pred': y_score > threshold,\n",
    "    })\n",
    "    \n",
    "\n",
    "#return pd.DataFrame(confusion_matrix(y, y_pred), columns=['Predicted Not Fraud', 'Predicted Fraud'], index=['Actual Not Fraud', 'Actual Fraud'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.2\n",
    "\n",
    "train_predict = predict(x_train, y_train, model, threshold)\n",
    "val_predict = predict(x_val, y_val, model, threshold)\n",
    "test_predict = predict(x_test, y_test, model, threshold)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.1. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(df_predict: pd.DataFrame, title: str, axes: plt.Axes = None):\n",
    "    \"\"\"\n",
    "    Plot the confusion matrix for the predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_predict : pd.DataFrame\n",
    "        The predictions\n",
    "    title : str\n",
    "        The title for the plot\n",
    "    axes : plt.Axes, optional\n",
    "        The axes to plot on, by default None\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(df_predict['y_true'], df_predict['y_pred'])\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        cbar=False,\n",
    "        xticklabels=['Not Fraud', 'Fraud'],\n",
    "        yticklabels=['Not Fraud', 'Fraud'],\n",
    "        linewidths=0.5,\n",
    "        linecolor='black',\n",
    "        square=True,\n",
    "        ax=axes,\n",
    "    ).set_title(f'{title} Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=False)\n",
    "\n",
    "plot_confusion_matrix(train_predict, 'Training', axes=axes[0])\n",
    "plot_confusion_matrix(val_predict, 'Validation', axes=axes[1])\n",
    "plot_confusion_matrix(test_predict, 'Test', axes=axes[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2 Classification Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_classification_report(df_predict: pd.DataFrame, title: str):\n",
    "    \"\"\"\n",
    "    Show the classification report for the predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_predict : pd.DataFrame\n",
    "        The predictions\n",
    "    title : str\n",
    "        The title for the report\n",
    "    \"\"\"\n",
    "    print(f'--- {title} Classification Report ---')\n",
    "    print(classification_report(df_predict['y_true'], df_predict['y_pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_classification_report(train_predict, 'Training')\n",
    "show_classification_report(val_predict, 'Validation')\n",
    "show_classification_report(test_predict, 'Test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.3 ROC Curve\n",
    "\n",
    "The idea to use Yelowbrick to plot the ROC Curve is taken from [Validating Fastai classifier with Yelowbrick](https://github.com/micstn/micstn.github.io/blob/master/nbs/fastai_yellowbrics.ipynb) and modified specifically to use Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnWrapper(BaseEstimator):\n",
    "    _estimator_type = \"classifier\"\n",
    "        \n",
    "    def __init__(self, model, classes, target_type:str='multiclass'):\n",
    "        self.model = model\n",
    "        self.classes = classes\n",
    "        \n",
    "        self.classes_ = classes\n",
    "        self.target_type_ = target_type\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        pass\n",
    "        #return accuracy_score(y, self.predict(X))\n",
    "    \n",
    "    # def get_new_preds(self, X):\n",
    "    #     new_to = self.model.dls.valid_ds.new(X)\n",
    "    #     new_to.conts = new_to.conts.astype(np.float32)\n",
    "    #     new_dl = self.model.dls.valid.new(new_to)\n",
    "    #     with self.model.no_bar():\n",
    "    #         preds,_,dec_preds = self.model.get_preds(dl=new_dl, with_decoded=True)\n",
    "    #     return (preds, dec_preds)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        proba = self.model.predict(X, use_multiprocessing=True).ravel()\n",
    "\n",
    "        return np.array([1-proba, proba]).T\n",
    "    \n",
    "    # def predict(self, X):\n",
    "    #     return self.get_new_preds(X)[1].numpy()\n",
    "\n",
    "\n",
    "def plot_roc(model: models.Sequential,\n",
    "             x: pd.DataFrame,\n",
    "             y: pd.DataFrame,\n",
    "             classes: list, \n",
    "             title: str):\n",
    "    \"\"\"\n",
    "    Plot the ROC curve for the predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : models.Sequential\n",
    "        The model\n",
    "    x : pd.DataFrame\n",
    "        The features\n",
    "    y : pd.Series\n",
    "        The labels\n",
    "    classes : list\n",
    "        The classes\n",
    "    title : str\n",
    "        The title for the plot\n",
    "    \"\"\"\n",
    "    visualizer = yb.classifier.ROCAUC(SklearnWrapper(model, classes),\n",
    "                                    classes=classes,\n",
    "                                    size=[500,500],\n",
    "                                    title=title)\n",
    "    visualizer.score(x_test, y_test)\n",
    "    visualizer.poof()\n",
    "\n",
    "\n",
    "# model_wrapper = SklearnWrapper(model, classes=['Not Fraud', 'Fraud'])\n",
    "# model_wrapper.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(model, x_test, y_test, ['Not Fraud', 'Fraud'], 'ROC Curves for Initial Model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Parameter Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.1 Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new model to tune\n",
    "tuned_model = get_model()\n",
    "\n",
    "# compile the model\n",
    "tuned_model.compile(\n",
    "    optimizer=optimizers.RMSprop(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.2 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_history = tuned_model.fit(\n",
    "    pd.concat([x_train, x_val]),\n",
    "    pd.concat([y_train, y_val]),\n",
    "    epochs=15,\n",
    "    batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(tuned_history)\n",
    "plot_accuracy(tuned_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the accuracy and loss on the data sets\n",
    "show_loss_accuracy(tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.2\n",
    "\n",
    "train_predict = predict(x_train, y_train, tuned_model, threshold)\n",
    "val_predict = predict(x_val, y_val, tuned_model, threshold)\n",
    "test_predict = predict(x_test, y_test, tuned_model, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=False)\n",
    "\n",
    "plot_confusion_matrix(train_predict, 'Training', axes=axes[0])\n",
    "plot_confusion_matrix(val_predict, 'Validation', axes=axes[1])\n",
    "plot_confusion_matrix(test_predict, 'Test', axes=axes[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_classification_report(train_predict, 'Training')\n",
    "show_classification_report(val_predict, 'Validation')\n",
    "show_classification_report(test_predict, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(tuned_model, x_test, y_test, ['Not Fraud', 'Fraud'], 'ROC Curves for Initial Model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Custom Hyperparameter Tuning\n",
    "\n",
    "Although the [Keras Tuner](https://keras.io/keras_tuner/) could be potentially used, the goal is to find the hyperparameters that maximizes the F1 score of the minority class. For this reason a simple custom grid search is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu_memory():\n",
    "    \"\"\"\n",
    "    Clear the GPU memory.\n",
    "    \"\"\"\n",
    "    K.clear_session()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    _ = gc.collect()\n",
    "\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_search(\n",
    "        x_train: pd.DataFrame,\n",
    "        y_train: pd.DataFrame,\n",
    "        x_val: pd.DataFrame,\n",
    "        y_val: pd.DataFrame,\n",
    "        x_test: pd.DataFrame,\n",
    "        y_test: pd.DataFrame,\n",
    "        learning_rates: list,\n",
    "        epochs: int,\n",
    "        thresholds: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a dataframe of the results of the hyperparameter search on\n",
    "    the learning rate, epocs and threshold.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_train : pd.DataFrame\n",
    "        The training features\n",
    "    y_train : pd.DataFrame\n",
    "        The training labels\n",
    "    x_val : pd.DataFrame\n",
    "        The validation features\n",
    "    y_val : pd.DataFrame\n",
    "        The validation labels\n",
    "    x_test : pd.DataFrame\n",
    "        The test features\n",
    "    y_test : pd.DataFrame\n",
    "        The test labels\n",
    "    learning_rates : list\n",
    "        The learning rates to try\n",
    "    epochs : int\n",
    "        The max number of epochs to train for\n",
    "    thresholds : list\n",
    "    \"\"\"\n",
    "    # create a dataframe to store the results\n",
    "    df_results = pd.DataFrame()\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        print(f'* Learning Rate: {learning_rate}')\n",
    "\n",
    "        # clear the session\n",
    "        clear_gpu_memory()\n",
    "\n",
    "        # create the new model\n",
    "        model = get_model()\n",
    "\n",
    "        # compile the model\n",
    "        model.compile(\n",
    "            optimizer=optimizers.RMSprop(learning_rate=learning_rate),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # train the model one epoch at a time\n",
    "        for epoch in range(epochs):\n",
    "            print(f'** Learning Rate: {learning_rate} Epoch: {epoch+1}')\n",
    "            model.fit(\n",
    "                x_train,\n",
    "                y_train,\n",
    "                epochs=1,\n",
    "                batch_size=512,\n",
    "                verbose=0)\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                print(f'*** Learning Rate: {learning_rate} Epoch: {epoch+1} Threshold: {threshold}')\n",
    "                _ = gc.collect()\n",
    "\n",
    "                # get the predictions\n",
    "                # train_predict = predict(x_train, y_train, tuned_model, threshold)\n",
    "                # val_predict = predict(x_val, y_val, tuned_model, threshold)\n",
    "                test_predict = predict(x_test, y_test, model, threshold)\n",
    "\n",
    "                # calculate the metrics\n",
    "                metrics = precision_recall_fscore_support(\n",
    "                    test_predict['y_true'], \n",
    "                    test_predict['y_pred'],\n",
    "                    zero_division=1)\n",
    "                \n",
    "                # format the metrics\n",
    "                df_metrics = pd.DataFrame(metrics) \\\n",
    "                    .set_axis(['precision', 'recall', 'f_score', 'true_sum'], axis=0) \\\n",
    "                    .T \\\n",
    "                    .assign(learning_rate=learning_rate, epoch=epoch+1, threshold=threshold) \\\n",
    "                    .reset_index() \\\n",
    "                    .rename(columns={'index': 'label'})\n",
    "\n",
    "                df_results = pd.concat([df_results, df_metrics])\n",
    "\n",
    "\n",
    "    return df_results\n",
    "\n",
    "# get the metic results\n",
    "df_results = hyperparameter_search(\n",
    "    x_train=x_train, \n",
    "    y_train=y_train, \n",
    "    x_val=x_val, \n",
    "    y_val=y_val, \n",
    "    x_test=x_test, \n",
    "    y_test=y_test, \n",
    "    learning_rates=[0.00001, 0.0001, 0.001, 0.01], \n",
    "    epochs=30,\n",
    "    thresholds=np.arange(0.1, 1, 0.1))\n",
    "\n",
    "# show the results\n",
    "clear_output(wait=True)\n",
    "\n",
    "display(\n",
    "    df_results \\\n",
    "        .query('label == 1') \\\n",
    "        .sort_values('f_score', ascending=False) \\\n",
    "        .head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv('grid_search_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.query('learning_rate == 0.001 and epoch == 23 and threshold == 0.4')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.1 Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model\n",
    "best_model = get_model()\n",
    "\n",
    "# compile the model\n",
    "best_model.compile(\n",
    "    optimizer=optimizers.RMSprop(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "history = best_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=23,\n",
    "    batch_size=512,\n",
    "    validation_data=(x_val, y_val))\n",
    "\n",
    "# plot the accuracy and loss\n",
    "plot_loss(history)\n",
    "plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predictions\n",
    "threshold = 0.4\n",
    "\n",
    "train_predict = predict(x_train, y_train, best_model, threshold)\n",
    "val_predict = predict(x_val, y_val, best_model, threshold)\n",
    "test_predict = predict(x_test, y_test, best_model, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=False)\n",
    "\n",
    "plot_confusion_matrix(train_predict, 'Training', axes=axes[0])\n",
    "plot_confusion_matrix(val_predict, 'Validation', axes=axes[1])\n",
    "plot_confusion_matrix(test_predict, 'Test', axes=axes[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the classification reports\n",
    "show_classification_report(train_predict, 'Training')\n",
    "show_classification_report(val_predict, 'Validation')\n",
    "show_classification_report(test_predict, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(best_model, x_test, y_test, ['Not Fraud', 'Fraud'], 'ROC Curves for Initial Model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
