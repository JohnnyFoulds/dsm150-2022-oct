{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_points = 11\n",
    "degree = 10\n",
    "x = np.linspace(0, 10, num_points)\n",
    "y = x + np.random.random(size=11) - 0.5\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "p = np.polyfit(x, y, degree)\n",
    "\n",
    "xx = np.linspace(0, 10, 101)\n",
    "def fit(xx, degree):\n",
    "    yy = np.zeros(len(xx))\n",
    "    for i in range(len(p)):\n",
    "        yy += xx ** (degree - i) * p[i]\n",
    "    return yy\n",
    "yy = fit(xx, degree)\n",
    "\n",
    "poly_str = ''     \n",
    "for i in range(degree):\n",
    "    poly_str += str(p[i]) + 'x^' + str(degree - i) + ' + '\n",
    "poly_str += str(p[degree])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x, y, 'bo')\n",
    "plt.title('Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here is some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print('y = x + u, u ~ U(-0.5, 0.5)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Which model do you prefer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Which model do you prefer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'bo')\n",
    "plt.plot(xx, yy)\n",
    "plt.title('Model 1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(poly_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The mathematical form of model 1. A tenth order polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'bo')\n",
    "plt.plot(x, x, 'r')\n",
    "plt.title('Model 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Model 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print('y = x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The mathematical form of model 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Prefer a *simple* model (Occam's razor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "I expect you prefer model 2. We prefer simple models - that's Occam's razor. The tenth order polynomial fits our data perfectly, but the fit is too good. Model 1 won't generalise because the shape of the fit is too specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A simple network has low capacity (small number of layers and units) and small weight and bias values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In analogy with polynomial fits of points, 'simple', means low capacity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Parameters are **regular** if they occupy a small interval close to zero\n",
    "\n",
    "- large weight parameters will amplify input noise\n",
    "\n",
    "- the network will attempt to fit to the noise\n",
    "\n",
    "- a model with smaller weights is more robust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Parameters are 'regular' if they occupy a small interval close to zero. Large weight parameters will amplify input noise. The network will attempt to fit to the noise. A model with smaller weights is therfore more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Weight regularisation**\n",
    "\n",
    "- L1: A cost proportional to the absolute value of the weight parameters is added to the loss function\n",
    "\n",
    "- L2 (aka weight decay): As above, but the cost is the square of the weight parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Weight regularisation is the attempt to prevent weights getting too big and dominant. In L1 regularisation, a cost proportional to the absolute value of the weight parameters is added to the loss function and in L2 regularisation (aka weight decay) the additional cost is the square of the weight parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "TensorFlow Keras weight parameter regularisation is achieved by adding a weight regularizer instance to the `add` argument list\n",
    "\n",
    "```\n",
    "model.add(layers.Dense(16, \n",
    "                       kernel_regularizer = regularizers.l2(0.001),\n",
    "                       activation='relu', \n",
    "                       input_shape=(10000,)))\n",
    "```                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "TensorFlow Keras weight parameter regularisation is achieved by adding a weight regularizer instance to the `add` argument list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`l2(0.001)` means $0.001 \\times w^T \\cdot w$ is added to the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "`l2(0.001)` means $0.001 \\times w^T \\cdot w$ is added to the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`regularizers.l1(0.001)` for L1 regularisation\n",
    "\n",
    "`regularizers.l1(0.001) + regularizers.l2(0.001)` for combined L1 and L2 regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "L1 regularisation and combine L1 and L2 regularisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "((train_data, train_labels), (test_data, test_labels)) = imdb.load_data(num_words = 10000)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension = 10000):\n",
    "    results = np.zeros( (len(sequences), dimension) )\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let us compare L2 regularisation to the unregularised IMDB network. \n",
    "\n",
    "Download and vectorise data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, regularizers\n",
    "\n",
    "def build_model(layer_1_units, layer_2_units, layer_3_units, reg):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    if (reg):\n",
    "        model.add(layers.Dense(layer_1_units, kernel_regularizer = regularizers.l2(0.001), \n",
    "                               activation='relu', input_shape=(10000,)))\n",
    "        model.add(layers.Dense(layer_2_units, kernel_regularizer=regularizers.l2(0.001), \n",
    "                               activation='relu'))\n",
    "    else:\n",
    "        model.add(layers.Dense(layer_1_units, activation='relu', input_shape=(10000,)))\n",
    "        model.add(layers.Dense(layer_2_units, activation='relu'))\n",
    "    \n",
    "\n",
    "    model.add(layers.Dense(layer_3_units, activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A model building function with optional regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = build_model(16, 16, 1, reg=False)\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Training the unregularised original IMDB network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "l2_model = build_model(16, 16, 1, reg=True)\n",
    "l2_history = l2_model.fit(partial_x_train,\n",
    "                          partial_y_train,\n",
    "                          epochs=20,\n",
    "                          batch_size=512,\n",
    "                          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Training the regularised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "def plot_loss_comparison(loss_a, label_a, loss_b, label_b, y_label):\n",
    "\n",
    "    epochs = range(1, len(loss_a) + 1)\n",
    "\n",
    "    plt.plot(epochs, loss_a, 'b+', label = label_a)\n",
    "    plt.plot(epochs, loss_b, 'bo', label = label_b)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Plot comparison function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_loss_comparison(history.history['val_loss'], 'No reg', \n",
    "                     l2_history.history['val_loss'], 'L2 reg', 'Validation Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The L2 regularised model resists overfitting even though both networks have the same capacity (160305 tuneable parameters)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
