{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dropout is a common and effective regularisation technique\n",
    "\n",
    "A fraction ('the dropout rate') of the output from a drop-out layer is zero'd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The third overfitting antidote is 'dropout'. It is a common and effective regularisation technique. A fraction ('the dropout rate') of the output from a drop-out layer is zero'd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask =  [1 0 1 0 0 0 1 0 1 1]\n",
      "layer_out =\n",
      " [0.7449127  0.00290134 0.66261154 0.43256324 0.55639191 0.90450226\n",
      " 0.51698345 0.91386481 0.74312511 0.85737055]\n",
      "mask * layer_out =\n",
      " [0.7449127  0.         0.66261154 0.         0.         0.\n",
      " 0.51698345 0.         0.74312511 0.85737055]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# dropout rate = 0.5\n",
    "length = 10\n",
    "mask = np.random.randint(0, high = 2, size = length) \n",
    "print('mask = ', mask)\n",
    "\n",
    "# layer output\n",
    "layer_out = np.random.rand(length)\n",
    "print('layer_out =\\n', layer_out)\n",
    " \n",
    "# regularised layer output \n",
    "print('mask * layer_out =\\n', mask * layer_out) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Mask is a vector with, on average, equal numbers of zeros and ones. Dropout is implemented by multiplying the layer output by the mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The dropout rate is usually berween 0.2 and 0.5\n",
    "\n",
    "Dropout is removed while evaluating the final network\n",
    "\n",
    "The output layers of the final network need to be scaled down by the dropout rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The dropout rate is usually berween 0.2 and 0.5. Dropout is removed while evaluating the final network. The output layers of the final network need to be scaled down by the dropout rate because one half of the layer output was effectively removed during training: the layer outputs will be twice as large when the network is restored to its full size and applied to the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Alternatively, scale the layer outputs up during training to compensate for the lost neurons:\n",
    "```\n",
    "layer_output *= np.random.random(0, high = 2, size = layer_output.shape)\n",
    "layer_output /= 0.5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Alternatively, scale the layer output up during training to compensate for tensor element zeroing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "((train_data, train_labels), (test_data, test_labels)) = imdb.load_data(num_words = 10000)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension = 10000):\n",
    "    results = np.zeros( (len(sequences), dimension) )\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let's try dropout on the IMDB network. Download and vectorise data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "\n",
    "def build_model(layer_1_units, layer_2_units, layer_3_units, drop):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(layers.Dense(layer_1_units, activation='relu', input_shape=(10000,)))\n",
    "    if(drop):\n",
    "        model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(layer_2_units, activation='relu'))\n",
    "    if(drop):\n",
    "        model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(layers.Dense(layer_3_units, activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A model building function with optional 50% dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        c = ['\\b|', '\\b/', '\\b-', '\\b\\\\'] \n",
    "        print(c[epoch % 4], end='')\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('\\b', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Our call back spinner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "model = build_model(16, 16, 1, drop=False)\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=0,\n",
    "                    callbacks=[CustomCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Train the original network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "do_model = build_model(16, 16, 1, drop=True)\n",
    "do_history = do_model.fit(partial_x_train,\n",
    "                          partial_y_train,\n",
    "                          epochs=20,\n",
    "                          batch_size=512,\n",
    "                          validation_data=(x_val, y_val),\n",
    "                          verbose=0, \n",
    "                          callbacks=[CustomCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Train a network with dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "def plot_loss_comparison(loss_a, label_a, loss_b, label_b, y_label):\n",
    "\n",
    "    epochs = range(1, len(loss_a) + 1)\n",
    "\n",
    "    plt.plot(epochs, loss_a, 'b+', label = label_a)\n",
    "    plt.plot(epochs, loss_b, 'bo', label = label_b)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A loss comparison plot function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_loss_comparison(history.history['val_loss'], 'No dropout', \n",
    "                     do_history.history['val_loss'], 'With dropout', 'Validation Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The plot show that dropout has successfully reduced overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dropout was (re-) discovered by Geoffrey Hinton\n",
    "\n",
    "Hinton says: 'I went to the bank. The tellers kept changing and I asked one of them why. He said he didn't know but they got moved around a lot. I figured it must be because it would require cooperation between employees to successfully defraud a bank. This made me realise that randomly removing a different subset of neurons on each example would prevent conspiracies and thus reduce overfitting.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Dropout was (re-) discovered by Geoffrey Hinton\n",
    "\n",
    "Hinton says: 'I went to the bank. The tellers kept changing and I asked one of them why. He said he didn't know but they got moved around a lot. I figured it must be because it would require cooperation between employees to successfully defraud a bank. This made me realise that randomly removing a different subset of neurons on each example would prevent conspiracies and thus reduce overfitting.'"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
