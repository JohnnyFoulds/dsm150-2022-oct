{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The layer is the fundamental neural network building block\n",
    "\n",
    "- layers map tensors to tensors\n",
    "\n",
    "- parameterised by weights and biases\n",
    "\n",
    "- weights and biases update in the SGD feedback loop  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The layer is the fundamental neural network building block. Layers map tensors to tensors and are parameterised by weights and biases. Layer parameters update in the SGD feedback loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Feedforward** networks\n",
    "\n",
    "- the sample is fed into the network\n",
    "- data is transformed by successive layers\n",
    "- an output emerges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A sample, in a 'feedforward' network, is fed into the network; data is transformed by successive layers and an output emerges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Dense/fully connected** layer\n",
    "\n",
    "- the layer weight matrix has size $n \\times m$ \n",
    "\n",
    "- the incoming vector has length $m$\n",
    "\n",
    "- the output vector has length $n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A dense or fully connected layer has an n x m weight matrix; the incoming vector has length m and the outgoing tramsformed vector has length n. The bias tensor is an n component vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Recurrent** layer\n",
    "\n",
    "- a recurrent neural network contains internal loops\n",
    "- data is processed by iterating through the sequence of elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A 'recurrent' neural network contains internal loops: ordered data is processed by iterating through the sequence of constituent elements. We don't consider recurrent networks here - Part 2 of Deep Learning With Python explains the concept and has several example networks. Recurrent layers are particularly useful when the samples have a time ordering, for example weather measurements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Convolutional** layer\n",
    "- a small window moves\n",
    "across a 2D image\n",
    "- a convolution\n",
    "- the response is mapped to layer weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Convolutional layers are the first choice for computer vision applications of deep learning. Convnets move a small window across a 2D image and the response, or convolution, is mapped to layer weights. We don't cover convnets in this module but the idea is explained DLWP part 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "|Data|Network|\n",
    "|:---|:---|\n",
    "Vector|Dense\n",
    "Sequence|Recurrent\n",
    "Image|Convolutional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A summary table matching type of layer and network. A dense network processes vector data; sequence data is processed by recurrent networks and convolutional layers are recommended for computer vision tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A layer performs a tensor operation on the input $x$ - an affine transformation followed by element-wise function application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We return to dense layers and feedforward networks. The dense layer performs a tensor operation on an input $x$. Namely, an affine transformation followed by element-wise function application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Affine transformation\n",
    "\n",
    "\\begin{align*}\n",
    "z = \\begin{pmatrix}w_{11} & w_{12} \\\\ w_{21} & w_{22} \\end{pmatrix} \\cdot \\begin{pmatrix}x_1 \\\\ x_2 \\end{pmatrix} + \\begin{pmatrix}b_1 \\\\ b_2 \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "- rotations, reflections, shears, dilations\n",
    "\n",
    "- affine transformation preserve straight lines and parallel lines remain parallel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "An affine transformation is a 2D matrix transformation such as a rotation, a reflection or a shear, followed by a translation. w dot x plus b. Straight lines remain straight and parallel lines remain parallel - although distances and angles may change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Affine transformations alone are insufficient for data uncrumpling \n",
    "\n",
    "We require a non-linear part -  the activation\n",
    "\n",
    "$f(z) = f\\begin{pmatrix} z_1 \\\\ z_2 \\end{pmatrix}\\ =\\begin{pmatrix} f(z_1) \\\\ f(z_2) \\end{pmatrix}$\n",
    "\n",
    "**sigmoid**, **rectified linear** and **tanh** are common activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Affine transformations alone are insufficient for data uncrumpling. We require a non-linear part -  the element-wise activation. 'Sigmoid', 'rectified linear' and 'tanh' are common activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\\begin{align*}\n",
    "\\text{relu}(x) &= \\max(0, x) \\\\\n",
    "\\\\\n",
    "\\tanh(x)&= \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\\\\n",
    "\\\\\n",
    "\\text{sigmoid}(x) &= \\frac{1}{1 + e^{-z}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The definitions of relu (recified linear), tanh and sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "def plot_sigmoid():\n",
    "  import matplotlib.pyplot as plt\n",
    "  x = np.arange(-5, 5, 0.1)\n",
    "  plt.plot(x, sigmoid(x), 'b')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here is the shape of the sigmoid function... It is like a a smoothed out (and therefore differentiable) step function. Large positive inputs are mapped to 1, large negative inputs are mapped to zero and there is smooth transition around 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose a layer performs a rotation, $R(x)$, with no translation ($b = 0$), and then sigmoids each element of the rotated vector\n",
    "\n",
    "- a rotation \n",
    "\n",
    "\\begin{align*}\n",
    "z =  \\begin{pmatrix} \n",
    "\\cos\\theta & -\\sin\\theta \\\\\n",
    "\\sin\\theta & \\cos\\theta \n",
    "\\end{pmatrix} \\cdot\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "- element wise sigmoid application \n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} = \\begin{pmatrix}\\sigma(z_1) \\\\ \\sigma(z_2)\\end{pmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Suppose a layer performs a rotation, $R(x)$, with no translation ($b = 0$), and then sigmoids each element of the rotated vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "theta = 3 * np.pi / 4\n",
    "w = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
    "\n",
    "o = np.array([0, 0])\n",
    "a = np.array([1, 0])\n",
    "b = np.array([1, 1])\n",
    "c = np.array([0, 1])\n",
    "\n",
    "points = np.array([o, a, b, c, o])\n",
    "x, y = points.T\n",
    "\n",
    "o_p = np.dot(w, o)\n",
    "a_p = np.dot(w, a)\n",
    "b_p = np.dot(w, b)\n",
    "c_p = np.dot(w, c)\n",
    "\n",
    "points_p = np.array([o_p, a_p, b_p, c_p, o_p])\n",
    "x_p, y_p = points_p.T\n",
    "\n",
    "o_pp = sigmoid(o_p)\n",
    "a_pp = sigmoid(a_p)\n",
    "b_pp = sigmoid(b_p)\n",
    "c_pp = sigmoid(c_p)\n",
    "\n",
    "points_pp = np.array([o_pp, a_pp, b_pp, c_pp, o_pp])\n",
    "x_pp, y_pp = points_pp.T\n",
    "\n",
    "def transform_square():\n",
    "  import matplotlib.pyplot as plt\n",
    "  plt.plot(x, y, 'b-')\n",
    "  plt.plot(x_p, y_p, 'r-')\n",
    "  plt.plot(x_pp, y_pp, 'g-')\n",
    "  plt.axis('equal')\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "transform_square()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "...the blue unit square has been rotated by $\\frac{3\\pi}{4}$. The vertices of the red square are then sigmoided - the green quadrilateral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The input and output tensor shapes of a TensorFlow Keras layer are fixed when the layer is created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The input and output tensor shapes of a TensorFlow Keras layer are fixed when the layer is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "layer = layers.Dense(32, input_shape = (784, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "...axis 1 of the input shape has dimension 784. The zero axis - the batch dimension - is unspecified and arbitrary. This layer outputs a vector of length 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, input_shape = (784, )))\n",
    "model.add(layers.Dense(16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "...the connecting downstream layer must expect a compatible input vector - 32 elements along the first axis. In fact Tensorflow is clever enough to know this. If you wish the second layer to output a tensor with 16 elements along the first axis you need only write `layers.Dense(16)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Axis zero has size 'None' - any value is accepted \n",
    "\n",
    "print(model.layers[0].input_shape, model.layers[0].output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The shapes of the input and output tensors of the first (index 0) layer... Axis  zero has size None, meaning that any value is accepted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# The weight and bias tensors\n",
    "def print_layer_tensor_shape(n):\n",
    "    weight_params = model.layers[n].get_weights()[0]\n",
    "    bias_params = model.layers[n].get_weights()[1]\n",
    "    print('layer: ', n, '\\tweight shape: ', weight_params.shape, '\\tbias shape', bias_params.shape)\n",
    "\n",
    "print_layer_tensor_shape(0)\n",
    "print_layer_tensor_shape(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Code to print the shape of layer tensors... The weight shape is the reverse of the matrix size. I said earlier that the layer zero weight matrix has size $n \\times m$ where $m$ is the length of the input vector and $n$ is the length of the outgoing vector. The layer weight tensors have shape `(m, n)` - they are matrix transposes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\\begin{align*}\n",
    "(w\\cdot x\n",
    ")_i &= \\sum_j w_{ij}x_j \\\\\n",
    "    &= \\sum_j x_j w_{ij} \\\\\n",
    "    &= \\sum_j x_j w_{ji}^T \\\\\n",
    "    &= (x^T \\cdot w^T)_i\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "It turns out - and it doesn't matter at all - that TensorFlow organises its tensor operations using the transposes of the conventional matrix and vector representations. This slide shows the relationship between the two ways of performing matrix-vector multiplication. It doesn't matter at all because all computational quantities are arrays (not tensors or matrices) and operations are loops with appropriate counters and end conditions. And it certainly doesn't matter to us because TensorFlow ensures that everything matches. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
