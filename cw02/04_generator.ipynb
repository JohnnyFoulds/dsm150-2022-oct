{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Use a generator for training\n",
    "\n",
    "Running out of memory, so a generator might help if it is not too slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-26 18:17:08.456067: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-26 18:17:08.935647: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-26 18:17:08.935690: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-26 18:17:08.935696: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "import gc\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from pqdm.threads import pqdm\n",
    "from typing import Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yellowbrick as yb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-26 18:17:09 INFO     Started\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        handlers=[\n",
    "        logging.FileHandler(\"ex05_06.log\"),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ])\n",
    "\n",
    "logging.info(\"Started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Generator Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[3 4 5]\n",
      "[6 7 8]\n",
      "[9]\n",
      "[0 1 2]\n",
      "[3 4 5]\n",
      "[6 7 8]\n",
      "[9]\n",
      "[0 1 2]\n",
      "[3 4 5]\n"
     ]
    }
   ],
   "source": [
    "x = list(range(10))\n",
    "y = [i*2 for i in x]\n",
    "\n",
    "def generator(x, y, batch_size):\n",
    "    while True:\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            yield np.array(x[i:i+batch_size]), np.array(y[i:i+batch_size])\n",
    "\n",
    "batch_size = 3\n",
    "train_gen = generator(x, y, batch_size)\n",
    "\n",
    "for i in range(10):\n",
    "    x, y = next(train_gen)\n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13174211, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>event_name</th>\n",
       "      <th>name</th>\n",
       "      <th>level</th>\n",
       "      <th>page</th>\n",
       "      <th>room_coor_x</th>\n",
       "      <th>room_coor_y</th>\n",
       "      <th>screen_coor_x</th>\n",
       "      <th>screen_coor_y</th>\n",
       "      <th>hover_duration</th>\n",
       "      <th>text</th>\n",
       "      <th>fqid</th>\n",
       "      <th>room_fqid</th>\n",
       "      <th>text_fqid</th>\n",
       "      <th>fullscreen</th>\n",
       "      <th>hq</th>\n",
       "      <th>music</th>\n",
       "      <th>level_group</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>0</td>\n",
       "      <td>cutscene_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>undefined</td>\n",
       "      <td>intro</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.intro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>1323</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Whatcha doing over there, Jo?</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>831</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just talking to Teddy.</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              session_id  elapsed_time      event_name   name  level  page  \\\n",
       "index                                                                        \n",
       "0      20090312431273200             0  cutscene_click  basic      0   NaN   \n",
       "1      20090312431273200          1323    person_click  basic      0   NaN   \n",
       "2      20090312431273200           831    person_click  basic      0   NaN   \n",
       "\n",
       "       room_coor_x  room_coor_y  screen_coor_x  screen_coor_y  hover_duration  \\\n",
       "index                                                                           \n",
       "0      -413.991405  -159.314686          380.0          494.0             NaN   \n",
       "1      -413.991405  -159.314686          380.0          494.0             NaN   \n",
       "2      -413.991405  -159.314686          380.0          494.0             NaN   \n",
       "\n",
       "                                text    fqid                       room_fqid  \\\n",
       "index                                                                          \n",
       "0                          undefined   intro  tunic.historicalsociety.closet   \n",
       "1      Whatcha doing over there, Jo?  gramps  tunic.historicalsociety.closet   \n",
       "2             Just talking to Teddy.  gramps  tunic.historicalsociety.closet   \n",
       "\n",
       "                                               text_fqid  fullscreen  hq  \\\n",
       "index                                                                      \n",
       "0                   tunic.historicalsociety.closet.intro         NaN NaN   \n",
       "1      tunic.historicalsociety.closet.gramps.intro_0_...         NaN NaN   \n",
       "2      tunic.historicalsociety.closet.gramps.intro_0_...         NaN NaN   \n",
       "\n",
       "       music level_group  \n",
       "index                     \n",
       "0        NaN         0-4  \n",
       "1        NaN         0-4  \n",
       "2        NaN         0-4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the source training set\n",
    "df_source = pd.read_csv('data/train.csv.gz', compression='gzip', index_col=1)\n",
    "\n",
    "print(df_source.shape)\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_source.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(212022, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200_q1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312433251036_q1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090314121766812_q1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             session_id  correct\n",
       "0  20090312431273200_q1        1\n",
       "1  20090312433251036_q1        0\n",
       "2  20090314121766812_q1        1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the source training labels\n",
    "df_source_labels = pd.read_csv('data/train_labels.csv')\n",
    "\n",
    "print(df_source_labels.shape)\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_source_labels.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-defined Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_columns = ['elapsed_time', 'room_coor_x', 'room_coor_y', 'screen_coor_x', 'screen_coor_y', 'event_name_checkpoint', 'event_name_cutscene_click', 'event_name_map_click', 'event_name_map_hover', 'event_name_navigate_click', 'event_name_notebook_click', 'event_name_notification_click', 'event_name_object_click', 'event_name_object_hover', 'event_name_observation_click', 'event_name_person_click', 'name_basic', 'name_close', 'name_next', 'name_open', 'name_prev', 'name_undefined', 'level_0', 'level_1', 'level_2', 'level_3', 'level_4', 'level_5', 'level_6', 'level_7', 'level_8', 'level_9', 'level_10', 'level_11', 'level_12', 'level_13', 'level_14', 'level_15', 'level_16', 'level_17', 'level_18', 'level_19', 'level_20', 'level_21', 'level_22', 'fqid_0', 'fqid_archivist', 'fqid_archivist_glasses', 'fqid_block', 'fqid_block_0', 'fqid_block_1', 'fqid_block_badge', 'fqid_block_badge_2', 'fqid_block_magnify', 'fqid_block_nelson', 'fqid_block_tocollection', 'fqid_block_tomap1', 'fqid_block_tomap2', 'fqid_boss', 'fqid_businesscards', 'fqid_businesscards.card_0.next', 'fqid_businesscards.card_1.next', 'fqid_businesscards.card_bingo.bingo', 'fqid_businesscards.card_bingo.next', 'fqid_ch3start', 'fqid_chap1_finale', 'fqid_chap1_finale_c', 'fqid_chap2_finale_c', 'fqid_chap4_finale_c', 'fqid_coffee', 'fqid_colorbook', 'fqid_confrontation', 'fqid_crane_ranger', 'fqid_cs', 'fqid_directory', 'fqid_directory.closeup.archivist', 'fqid_door_block_clean', 'fqid_door_block_talk', 'fqid_doorblock', 'fqid_expert', 'fqid_flag_girl', 'fqid_fox', 'fqid_glasses', 'fqid_gramps', 'fqid_groupconvo', 'fqid_groupconvo_flag', 'fqid_intro', 'fqid_janitor', 'fqid_journals', 'fqid_journals.hub.topics', 'fqid_journals.pic_0.next', 'fqid_journals.pic_1.next', 'fqid_journals.pic_2.bingo', 'fqid_journals.pic_2.next', 'fqid_journals_flag', 'fqid_journals_flag.hub.topics', 'fqid_journals_flag.hub.topics_old', 'fqid_journals_flag.pic_0.bingo', 'fqid_journals_flag.pic_0.next', 'fqid_journals_flag.pic_0_old.next', 'fqid_journals_flag.pic_1.bingo', 'fqid_journals_flag.pic_1.next', 'fqid_journals_flag.pic_1_old.next', 'fqid_journals_flag.pic_2.bingo', 'fqid_journals_flag.pic_2.next', 'fqid_journals_flag.pic_2_old.next', 'fqid_key', 'fqid_lockeddoor', 'fqid_logbook', 'fqid_logbook.page.bingo', 'fqid_magnify', 'fqid_need_glasses', 'fqid_notebook', 'fqid_outtolunch', 'fqid_photo', 'fqid_plaque', 'fqid_plaque.face.date', 'fqid_reader', 'fqid_reader.paper0.next', 'fqid_reader.paper0.prev', 'fqid_reader.paper1.next', 'fqid_reader.paper1.prev', 'fqid_reader.paper2.bingo', 'fqid_reader.paper2.next', 'fqid_reader.paper2.prev', 'fqid_reader_flag', 'fqid_reader_flag.paper0.next', 'fqid_reader_flag.paper0.prev', 'fqid_reader_flag.paper1.next', 'fqid_reader_flag.paper1.prev', 'fqid_reader_flag.paper2.bingo', 'fqid_reader_flag.paper2.next', 'fqid_reader_flag.paper2.prev', 'fqid_remove_cup', 'fqid_report', 'fqid_retirement_letter', 'fqid_savedteddy', 'fqid_seescratches', 'fqid_teddy', 'fqid_tobasement', 'fqid_tocage', 'fqid_tocloset', 'fqid_tocloset_dirty', 'fqid_tocollection', 'fqid_tocollectionflag', 'fqid_toentry', 'fqid_tofrontdesk', 'fqid_togrampa', 'fqid_tohallway', 'fqid_tomap', 'fqid_tomicrofiche', 'fqid_tostacks', 'fqid_tracks', 'fqid_tracks.hub.deer', 'fqid_trigger_coffee', 'fqid_trigger_scarf', 'fqid_tunic', 'fqid_tunic.capitol_0', 'fqid_tunic.capitol_1', 'fqid_tunic.capitol_2', 'fqid_tunic.drycleaner', 'fqid_tunic.flaghouse', 'fqid_tunic.historicalsociety', 'fqid_tunic.hub.slip', 'fqid_tunic.humanecology', 'fqid_tunic.kohlcenter', 'fqid_tunic.library', 'fqid_tunic.wildlife', 'fqid_unlockdoor', 'fqid_wells', 'fqid_wellsbadge', 'fqid_what_happened', 'fqid_worker', 'room_fqid_tunic.capitol_0.hall', 'room_fqid_tunic.capitol_1.hall', 'room_fqid_tunic.capitol_2.hall', 'room_fqid_tunic.drycleaner.frontdesk', 'room_fqid_tunic.flaghouse.entry', 'room_fqid_tunic.historicalsociety.basement', 'room_fqid_tunic.historicalsociety.cage', 'room_fqid_tunic.historicalsociety.closet', 'room_fqid_tunic.historicalsociety.closet_dirty', 'room_fqid_tunic.historicalsociety.collection', 'room_fqid_tunic.historicalsociety.collection_flag', 'room_fqid_tunic.historicalsociety.entry', 'room_fqid_tunic.historicalsociety.frontdesk', 'room_fqid_tunic.historicalsociety.stacks', 'room_fqid_tunic.humanecology.frontdesk', 'room_fqid_tunic.kohlcenter.halloffame', 'room_fqid_tunic.library.frontdesk', 'room_fqid_tunic.library.microfiche', 'room_fqid_tunic.wildlife.center', 'text_fqid_0', 'text_fqid_tunic.capitol_0.hall.boss.talktogramps', 'text_fqid_tunic.capitol_0.hall.chap1_finale_c', 'text_fqid_tunic.capitol_1.hall.boss.haveyougotit', 'text_fqid_tunic.capitol_1.hall.boss.writeitup', 'text_fqid_tunic.capitol_1.hall.chap2_finale_c', 'text_fqid_tunic.capitol_2.hall.boss.haveyougotit', 'text_fqid_tunic.capitol_2.hall.chap4_finale_c', 'text_fqid_tunic.drycleaner.frontdesk.block_0', 'text_fqid_tunic.drycleaner.frontdesk.block_1', 'text_fqid_tunic.drycleaner.frontdesk.logbook.page.bingo', 'text_fqid_tunic.drycleaner.frontdesk.worker.done', 'text_fqid_tunic.drycleaner.frontdesk.worker.done2', 'text_fqid_tunic.drycleaner.frontdesk.worker.hub', 'text_fqid_tunic.drycleaner.frontdesk.worker.takealook', 'text_fqid_tunic.flaghouse.entry.colorbook', 'text_fqid_tunic.flaghouse.entry.flag_girl.hello', 'text_fqid_tunic.flaghouse.entry.flag_girl.hello_recap', 'text_fqid_tunic.flaghouse.entry.flag_girl.symbol', 'text_fqid_tunic.flaghouse.entry.flag_girl.symbol_recap', 'text_fqid_tunic.historicalsociety.basement.ch3start', 'text_fqid_tunic.historicalsociety.basement.gramps.seeyalater', 'text_fqid_tunic.historicalsociety.basement.gramps.whatdo', 'text_fqid_tunic.historicalsociety.basement.janitor', 'text_fqid_tunic.historicalsociety.basement.savedteddy', 'text_fqid_tunic.historicalsociety.basement.seescratches', 'text_fqid_tunic.historicalsociety.cage.confrontation', 'text_fqid_tunic.historicalsociety.cage.glasses.afterteddy', 'text_fqid_tunic.historicalsociety.cage.glasses.beforeteddy', 'text_fqid_tunic.historicalsociety.cage.lockeddoor', 'text_fqid_tunic.historicalsociety.cage.need_glasses', 'text_fqid_tunic.historicalsociety.cage.teddy.trapped', 'text_fqid_tunic.historicalsociety.cage.unlockdoor', 'text_fqid_tunic.historicalsociety.closet.doorblock', 'text_fqid_tunic.historicalsociety.closet.gramps.intro_0_cs_0', 'text_fqid_tunic.historicalsociety.closet.intro', 'text_fqid_tunic.historicalsociety.closet.notebook', 'text_fqid_tunic.historicalsociety.closet.photo', 'text_fqid_tunic.historicalsociety.closet.retirement_letter.hub', 'text_fqid_tunic.historicalsociety.closet.teddy.intro_0_cs_0', 'text_fqid_tunic.historicalsociety.closet.teddy.intro_0_cs_5', 'text_fqid_tunic.historicalsociety.closet_dirty.door_block_clean', 'text_fqid_tunic.historicalsociety.closet_dirty.door_block_talk', 'text_fqid_tunic.historicalsociety.closet_dirty.gramps.archivist', 'text_fqid_tunic.historicalsociety.closet_dirty.gramps.helpclean', 'text_fqid_tunic.historicalsociety.closet_dirty.gramps.news', 'text_fqid_tunic.historicalsociety.closet_dirty.gramps.nothing', 'text_fqid_tunic.historicalsociety.closet_dirty.photo', 'text_fqid_tunic.historicalsociety.closet_dirty.trigger_coffee', 'text_fqid_tunic.historicalsociety.closet_dirty.trigger_scarf', 'text_fqid_tunic.historicalsociety.closet_dirty.what_happened', 'text_fqid_tunic.historicalsociety.collection.cs', 'text_fqid_tunic.historicalsociety.collection.gramps.found', 'text_fqid_tunic.historicalsociety.collection.gramps.look_0', 'text_fqid_tunic.historicalsociety.collection.gramps.lost', 'text_fqid_tunic.historicalsociety.collection.tunic', 'text_fqid_tunic.historicalsociety.collection.tunic.slip', 'text_fqid_tunic.historicalsociety.collection_flag.gramps.flag', 'text_fqid_tunic.historicalsociety.collection_flag.gramps.recap', 'text_fqid_tunic.historicalsociety.entry.block_tocollection', 'text_fqid_tunic.historicalsociety.entry.block_tomap1', 'text_fqid_tunic.historicalsociety.entry.block_tomap2', 'text_fqid_tunic.historicalsociety.entry.boss.flag', 'text_fqid_tunic.historicalsociety.entry.boss.flag_recap', 'text_fqid_tunic.historicalsociety.entry.boss.talktogramps', 'text_fqid_tunic.historicalsociety.entry.directory.closeup.archivist', 'text_fqid_tunic.historicalsociety.entry.gramps.hub', 'text_fqid_tunic.historicalsociety.entry.groupconvo', 'text_fqid_tunic.historicalsociety.entry.groupconvo_flag', 'text_fqid_tunic.historicalsociety.entry.wells.flag', 'text_fqid_tunic.historicalsociety.entry.wells.flag_recap', 'text_fqid_tunic.historicalsociety.entry.wells.talktogramps', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.foundtheodora', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.have_glass', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.have_glass_recap', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.hello', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.need_glass_0', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.need_glass_1', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.newspaper', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.newspaper_recap', 'text_fqid_tunic.historicalsociety.frontdesk.archivist_glasses.confrontation', 'text_fqid_tunic.historicalsociety.frontdesk.archivist_glasses.confrontation_recap', 'text_fqid_tunic.historicalsociety.frontdesk.block_magnify', 'text_fqid_tunic.historicalsociety.frontdesk.key', 'text_fqid_tunic.historicalsociety.frontdesk.magnify', 'text_fqid_tunic.historicalsociety.stacks.block', 'text_fqid_tunic.historicalsociety.stacks.journals.pic_2.bingo', 'text_fqid_tunic.historicalsociety.stacks.journals_flag.pic_0.bingo', 'text_fqid_tunic.historicalsociety.stacks.journals_flag.pic_1.bingo', 'text_fqid_tunic.historicalsociety.stacks.journals_flag.pic_2.bingo', 'text_fqid_tunic.historicalsociety.stacks.outtolunch', 'text_fqid_tunic.humanecology.frontdesk.block_0', 'text_fqid_tunic.humanecology.frontdesk.block_1', 'text_fqid_tunic.humanecology.frontdesk.businesscards.card_bingo.bingo', 'text_fqid_tunic.humanecology.frontdesk.worker.badger', 'text_fqid_tunic.humanecology.frontdesk.worker.intro', 'text_fqid_tunic.kohlcenter.halloffame.block_0', 'text_fqid_tunic.kohlcenter.halloffame.plaque.face.date', 'text_fqid_tunic.kohlcenter.halloffame.togrampa', 'text_fqid_tunic.library.frontdesk.block_badge', 'text_fqid_tunic.library.frontdesk.block_badge_2', 'text_fqid_tunic.library.frontdesk.block_nelson', 'text_fqid_tunic.library.frontdesk.wellsbadge.hub', 'text_fqid_tunic.library.frontdesk.worker.droppedbadge', 'text_fqid_tunic.library.frontdesk.worker.flag', 'text_fqid_tunic.library.frontdesk.worker.flag_recap', 'text_fqid_tunic.library.frontdesk.worker.hello', 'text_fqid_tunic.library.frontdesk.worker.hello_short', 'text_fqid_tunic.library.frontdesk.worker.nelson', 'text_fqid_tunic.library.frontdesk.worker.nelson_recap', 'text_fqid_tunic.library.frontdesk.worker.preflag', 'text_fqid_tunic.library.frontdesk.worker.wells', 'text_fqid_tunic.library.frontdesk.worker.wells_recap', 'text_fqid_tunic.library.microfiche.block_0', 'text_fqid_tunic.library.microfiche.reader.paper2.bingo', 'text_fqid_tunic.library.microfiche.reader_flag.paper2.bingo', 'text_fqid_tunic.wildlife.center.coffee', 'text_fqid_tunic.wildlife.center.crane_ranger.crane', 'text_fqid_tunic.wildlife.center.expert.recap', 'text_fqid_tunic.wildlife.center.expert.removed_cup', 'text_fqid_tunic.wildlife.center.fox.concern', 'text_fqid_tunic.wildlife.center.remove_cup', 'text_fqid_tunic.wildlife.center.tracks.hub.deer', 'text_fqid_tunic.wildlife.center.wells.animals', 'text_fqid_tunic.wildlife.center.wells.animals2', 'text_fqid_tunic.wildlife.center.wells.nodeer', 'text_fqid_tunic.wildlife.center.wells.nodeer_recap', 'level_group_0-4', 'level_group_13-22', 'level_group_5-12', \n",
    "'question_1',\n",
    "'question_2',\n",
    "'question_3',\n",
    "'question_4',\n",
    "'question_5',\n",
    "'question_6',\n",
    "'question_7',\n",
    "'question_8',\n",
    "'question_9',\n",
    "'question_10',\n",
    "'question_11',\n",
    "'question_12',\n",
    "'question_13',\n",
    "'question_14',\n",
    "'question_15',\n",
    "'question_16',\n",
    "'question_17',\n",
    "'question_18',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_question_to_level_group(question_number):\n",
    "    \"\"\"\n",
    "    Maps the question number to the level group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question_number : int\n",
    "        The question number.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The level group.\n",
    "    \"\"\"\n",
    "    if question_number in [1, 2, 3]:\n",
    "        return '0-4'\n",
    "    elif question_number in [4, 5, 6, 7, 8, 9, 10, 11, 12, 13]:\n",
    "        return '5-12'\n",
    "    elif question_number in [14, 15, 16, 17, 18]:\n",
    "        return '13-22'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def prepare_label_dataset(data : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the label dataset and add columns for the level group \n",
    "    and the question number.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The label dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The prepared label dataset.\n",
    "    \"\"\"\n",
    "    # add the columns to determine the level group\n",
    "    df_labels = data \\\n",
    "        .rename(columns={'session_id': 'id'}) \\\n",
    "        .assign(session_id=lambda df: df['id'].str.split('_').str[0].astype(int)) \\\n",
    "        .assign(question_id=lambda df: df['id'].str.split('_').str[1]) \\\n",
    "        .assign(question_num=lambda df: df['question_id'].str[1:].astype(int)) \\\n",
    "        [['session_id', 'question_num', 'correct']]\n",
    "    \n",
    "    # add the level group column\n",
    "    df_labels['level_group'] = df_labels['question_num'].apply(map_question_to_level_group) \n",
    "\n",
    "        \n",
    "    return df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_main_dataset(data : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the main dataset by removing duplicates and removing \n",
    "    columns that are not needed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The main dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The prepared main dataset.\n",
    "    \"\"\"\n",
    "    empty_columns = ['fullscreen', 'hq', 'music', 'page', 'hover_duration']\n",
    "\n",
    "    df_main = data \\\n",
    "        .drop_duplicates() \\\n",
    "        .reset_index(drop=True) \\\n",
    "        .drop(empty_columns, axis=1) \\\n",
    "        .drop('text', axis=1)\n",
    "\n",
    "    return df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_dataset(data: pd.DataFrame, standardize_coordinates: bool=True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Vectorizes the dataset for deep learning by one-hot encoding and standardizing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The dataset to prepare.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The vectorized dataset.\n",
    "    \"\"\"\n",
    "    categorical_cols = ['event_name', 'name', 'level', 'fqid', 'room_fqid', 'text_fqid', 'level_group']\n",
    "    numerical_cols = ['elapsed_time']    \n",
    "    coordinates_cols = ['room_coor_x', 'room_coor_y', 'screen_coor_x', 'screen_coor_y']\n",
    "\n",
    "    df_vectorized = data \\\n",
    "        .drop('session_id', axis=1) \\\n",
    "        .fillna(0)\n",
    "\n",
    "    # standardize the numerical variables\n",
    "    df_vectorized[numerical_cols] = (df_vectorized[numerical_cols] - df_vectorized[numerical_cols].mean()) / df_vectorized[numerical_cols].std()\n",
    "\n",
    "    # standardize the coordinates\n",
    "    if standardize_coordinates:\n",
    "        df_vectorized[coordinates_cols] = (df_vectorized[coordinates_cols] - df_vectorized[coordinates_cols].mean()) / df_vectorized[coordinates_cols].std()\n",
    "\n",
    "    # one-hot encode the categorical variables\n",
    "    df_vectorized = pd.get_dummies(df_vectorized, columns=categorical_cols)\n",
    "    \n",
    "    return df_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_matrix(data: pd.DataFrame, vector_columns: list, standardize_coordinates: bool=True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare a sequence matrix from a DataFrame for a specific session and level group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The dataframe containing the data.\n",
    "\n",
    "    vector_columns : list\n",
    "        The columns that should appear in the sequence matrix.\n",
    "\n",
    "    standardize_coordinates : bool, optional\n",
    "        Whether to standardize the coordinates.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The sequence matrix.\n",
    "    \"\"\"\n",
    "    df_sequence_matrix = vectorize_dataset(data, standardize_coordinates=standardize_coordinates)\n",
    "\n",
    "    # add the missing columns\n",
    "    missing_columns = [column for column in vector_columns if column not in df_sequence_matrix.columns]\n",
    "    df_sequence_matrix = pd.concat([\n",
    "        df_sequence_matrix, \n",
    "        pd.DataFrame(columns=missing_columns)], axis=1).fillna(0)\n",
    "\n",
    "    return df_sequence_matrix[vector_columns]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_array(session_data: pd.DataFrame, \n",
    "                        level_group: int,\n",
    "                        event_count:int = 1000) -> np.array:\n",
    "    \"\"\"\n",
    "    Creates a vector array for a specific session and question number.\n",
    "    \"\"\"\n",
    "    # get the data for the session and level group\n",
    "    df_session = session_data.query('level_group == @level_group')\n",
    "\n",
    "    # prepare the sequence matrix\n",
    "    df_sequence_matrix = prepare_sequence_matrix(df_session, vector_columns)\n",
    "\n",
    "    # # set the question number value\n",
    "    # df_sequence_matrix[f'question_{question_number}'] = 1\n",
    "\n",
    "    # convert it to a numpy array\n",
    "    vector_array = df_sequence_matrix.to_numpy()\n",
    "\n",
    "    # the array cannot have more events than the event count\n",
    "    vector_array = vector_array[:event_count]\n",
    "\n",
    "    # pad the array with zeros if it has less events than the event count\n",
    "    if vector_array.shape[0] < event_count:\n",
    "        vector_array = np.pad(vector_array, ((0, event_count - vector_array.shape[0]), (0, 0)), 'constant')\n",
    "\n",
    "    del df_session\n",
    "    del df_sequence_matrix\n",
    "\n",
    "    return vector_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X: pd.DataFrame,\n",
    "                   y: pd.DataFrame,\n",
    "                   session_list: list,\n",
    "                   event_count:int = 1000) -> Tuple[np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Creates a dataset for a specific set of sessions and question numbers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        The main dataset.\n",
    "\n",
    "    y : pd.DataFrame\n",
    "        The label dataset.\n",
    "\n",
    "    session_ids : list\n",
    "        The list of session ids.\n",
    "\n",
    "    event_count : int, optional\n",
    "        The number of events to include in the dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.array, np.array]\n",
    "        The dataset and the labels.\n",
    "    \"\"\"\n",
    "    X_dataset = []\n",
    "    y_dataset = []\n",
    "\n",
    "    for session_id in session_list:\n",
    "        # get the session labels\n",
    "        logging.info(f'create_dataset:getting session data for session {session_id}')\n",
    "        df_session_labels = y.query('session_id == @session_id')\n",
    "        df_session = X.query('session_id == @session_id')\n",
    "\n",
    "        # create the level groups\n",
    "        #logging.info(f'create_dataset:create vector arrays')\n",
    "        vector_arrays = {\n",
    "            '0-4': create_vector_array(df_session, '0-4', event_count),\n",
    "            '5-12': create_vector_array(df_session, '5-12', event_count),\n",
    "            '13-22': create_vector_array(df_session, '13-22', event_count),\n",
    "        }\n",
    "\n",
    "        # iterate over all the questions answered in the session\n",
    "        for _, row in df_session_labels.iterrows():\n",
    "            #logging.info(f'create_dataset:processing question {row[\"question_num\"]}')\n",
    "            question_number = row['question_num']\n",
    "            correct = row['correct']\n",
    "            level_group = map_question_to_level_group(question_number)\n",
    "\n",
    "            # get the vector array\n",
    "            vector_array = vector_arrays[level_group].copy()\n",
    "\n",
    "            # set the question number value\n",
    "            column_index = vector_columns.index(f'question_{question_number}')\n",
    "            #print(f'Question: {question_number}, Column: {column_index}')\n",
    "            vector_array[:, column_index] = 1\n",
    "\n",
    "            # add the vector array to the dataset\n",
    "            X_dataset.append(vector_array)\n",
    "            del vector_array\n",
    "\n",
    "            # add the label to the dataset\n",
    "            y_dataset.append(correct)\n",
    "\n",
    "    logging.info(f'create_dataset: converting to numpy arrays')\n",
    "    X_out = np.array(X_dataset, dtype=np.float64)\n",
    "    y_out = np.array(y_dataset)\n",
    "\n",
    "    logging.info(f'create_dataset: returning results')\n",
    "    return X_out, y_out\n",
    "\n",
    "#X_train, y_train = create_dataset(X=df_source, y=df_source_labels, session_list=train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-26 18:20:15 INFO     dataset_generator:Creating dataset for sessions 0 to 500\n",
      "2023-02-26 18:20:15 INFO     create_dataset:getting session data for session 21100112512445870\n",
      "2023-02-26 18:20:15 INFO     create_dataset:getting session data for session 21040411034781364\n",
      "2023-02-26 18:20:16 INFO     create_dataset:getting session data for session 21010509390206944\n",
      "2023-02-26 18:20:16 INFO     create_dataset:getting session data for session 21090311322306420\n",
      "2023-02-26 18:20:16 INFO     create_dataset:getting session data for session 22070210293928428\n",
      "2023-02-26 18:20:16 INFO     create_dataset:getting session data for session 20100215344879850\n",
      "2023-02-26 18:20:16 INFO     create_dataset:getting session data for session 21010510230587460\n",
      "2023-02-26 18:20:16 INFO     create_dataset:getting session data for session 22070609035078864\n",
      "2023-02-26 18:20:16 INFO     create_dataset:getting session data for session 21020318123676228\n",
      "2023-02-26 18:20:16 INFO     create_dataset:getting session data for session 21020014002232610\n",
      "2023-02-26 18:20:16 INFO     create_dataset:getting session data for session 21040215135652300\n",
      "2023-02-26 18:20:16 INFO     create_dataset:getting session data for session 21070513045227784\n",
      "2023-02-26 18:20:17 INFO     create_dataset:getting session data for session 22000413561784070\n",
      "2023-02-26 18:20:17 INFO     create_dataset:getting session data for session 21050411033210710\n",
      "2023-02-26 18:20:17 INFO     create_dataset:getting session data for session 20110513361207548\n",
      "2023-02-26 18:20:17 INFO     create_dataset:getting session data for session 22030015272116716\n",
      "2023-02-26 18:20:17 INFO     create_dataset:getting session data for session 21050516590213464\n",
      "2023-02-26 18:20:17 INFO     create_dataset:getting session data for session 21030509254527740\n",
      "2023-02-26 18:20:17 INFO     create_dataset:getting session data for session 22010408554729756\n",
      "2023-02-26 18:20:17 INFO     create_dataset:getting session data for session 21000416583095070\n",
      "2023-02-26 18:20:17 INFO     create_dataset:getting session data for session 21010511462266430\n",
      "2023-02-26 18:20:18 INFO     create_dataset:getting session data for session 22090510381132896\n",
      "2023-02-26 18:20:18 INFO     create_dataset:getting session data for session 21000310111261484\n",
      "2023-02-26 18:20:18 INFO     create_dataset:getting session data for session 21000010173236224\n",
      "2023-02-26 18:20:18 INFO     create_dataset:getting session data for session 21030311264960450\n",
      "2023-02-26 18:20:18 INFO     create_dataset:getting session data for session 22080607462722040\n",
      "2023-02-26 18:20:18 INFO     create_dataset:getting session data for session 22010515564709772\n",
      "2023-02-26 18:20:18 INFO     create_dataset:getting session data for session 21060417090733030\n",
      "2023-02-26 18:20:18 INFO     create_dataset:getting session data for session 21000510412386890\n",
      "2023-02-26 18:20:18 INFO     create_dataset:getting session data for session 21040413254656108\n",
      "2023-02-26 18:20:18 INFO     create_dataset:getting session data for session 22070614245650628\n",
      "2023-02-26 18:20:18 INFO     create_dataset:getting session data for session 22030214012407816\n",
      "2023-02-26 18:20:19 INFO     create_dataset:getting session data for session 21090316263043576\n",
      "2023-02-26 18:20:19 INFO     create_dataset:getting session data for session 21040313353271370\n",
      "2023-02-26 18:20:19 INFO     create_dataset:getting session data for session 21030210094449196\n",
      "2023-02-26 18:20:19 INFO     create_dataset:getting session data for session 21010120391936410\n",
      "2023-02-26 18:20:19 INFO     create_dataset:getting session data for session 20100213300913772\n",
      "2023-02-26 18:20:19 INFO     create_dataset:getting session data for session 22000413032041096\n",
      "2023-02-26 18:20:19 INFO     create_dataset:getting session data for session 22000517180185932\n",
      "2023-02-26 18:20:19 INFO     create_dataset:getting session data for session 21010309394335084\n",
      "2023-02-26 18:20:19 INFO     create_dataset:getting session data for session 21000209594437430\n",
      "2023-02-26 18:20:19 INFO     create_dataset:getting session data for session 21030312444159504\n",
      "2023-02-26 18:20:19 INFO     create_dataset:getting session data for session 21040211421018000\n",
      "2023-02-26 18:20:20 INFO     create_dataset:getting session data for session 21040308302564530\n",
      "2023-02-26 18:20:20 INFO     create_dataset:getting session data for session 21090115162701604\n",
      "2023-02-26 18:20:20 INFO     create_dataset:getting session data for session 21060013135476532\n",
      "2023-02-26 18:20:20 INFO     create_dataset:getting session data for session 22060517515596010\n",
      "2023-02-26 18:20:20 INFO     create_dataset:getting session data for session 21090521513307404\n",
      "2023-02-26 18:20:20 INFO     create_dataset:getting session data for session 22000009242414744\n",
      "2023-02-26 18:20:20 INFO     create_dataset:getting session data for session 21040610390393572\n",
      "2023-02-26 18:20:20 INFO     create_dataset:getting session data for session 22080017292486590\n",
      "2023-02-26 18:20:20 INFO     create_dataset:getting session data for session 21030410305813684\n",
      "2023-02-26 18:20:20 INFO     create_dataset:getting session data for session 21000209024718680\n",
      "2023-02-26 18:20:21 INFO     create_dataset:getting session data for session 22030517202431716\n",
      "2023-02-26 18:20:21 INFO     create_dataset:getting session data for session 20110315362979936\n",
      "2023-02-26 18:20:21 INFO     create_dataset:getting session data for session 21010311594927604\n",
      "2023-02-26 18:20:21 INFO     create_dataset:getting session data for session 21000509592223412\n",
      "2023-02-26 18:20:21 INFO     create_dataset:getting session data for session 21040512235474304\n",
      "2023-02-26 18:20:21 INFO     create_dataset:getting session data for session 21080219460631616\n",
      "2023-02-26 18:20:21 INFO     create_dataset:getting session data for session 22090510420281604\n",
      "2023-02-26 18:20:21 INFO     create_dataset:getting session data for session 21000312144193148\n",
      "2023-02-26 18:20:21 INFO     create_dataset:getting session data for session 21040110121447210\n",
      "2023-02-26 18:20:21 INFO     create_dataset:getting session data for session 21040510343733796\n",
      "2023-02-26 18:20:22 INFO     create_dataset:getting session data for session 20100015040218188\n",
      "2023-02-26 18:20:22 INFO     create_dataset:getting session data for session 22070414552506370\n",
      "2023-02-26 18:20:22 INFO     create_dataset:getting session data for session 20100411093406960\n",
      "2023-02-26 18:20:22 INFO     create_dataset:getting session data for session 21060312012299228\n",
      "2023-02-26 18:20:22 INFO     create_dataset:getting session data for session 22050216094036844\n",
      "2023-02-26 18:20:22 INFO     create_dataset:getting session data for session 21010614312638324\n",
      "2023-02-26 18:20:22 INFO     create_dataset:getting session data for session 22080415071708012\n",
      "2023-02-26 18:20:22 INFO     create_dataset:getting session data for session 21010017025583540\n",
      "2023-02-26 18:20:22 INFO     create_dataset:getting session data for session 21010208030950212\n",
      "2023-02-26 18:20:22 INFO     create_dataset:getting session data for session 21060409385717544\n",
      "2023-02-26 18:20:23 INFO     create_dataset:getting session data for session 22040212094658516\n",
      "2023-02-26 18:20:23 INFO     create_dataset:getting session data for session 21030113024338210\n",
      "2023-02-26 18:20:23 INFO     create_dataset:getting session data for session 21020014345920744\n",
      "2023-02-26 18:20:23 INFO     create_dataset:getting session data for session 22030211293307704\n",
      "2023-02-26 18:20:23 INFO     create_dataset:getting session data for session 21000409535817280\n",
      "2023-02-26 18:20:23 INFO     create_dataset:getting session data for session 22060420535392340\n",
      "2023-02-26 18:20:23 INFO     create_dataset:getting session data for session 20110309405201104\n",
      "2023-02-26 18:20:23 INFO     create_dataset:getting session data for session 20110409163454652\n",
      "2023-02-26 18:20:23 INFO     create_dataset:getting session data for session 21030313492809508\n",
      "2023-02-26 18:20:23 INFO     create_dataset:getting session data for session 22030010263153610\n",
      "2023-02-26 18:20:24 INFO     create_dataset:getting session data for session 21020315514776476\n",
      "2023-02-26 18:20:24 INFO     create_dataset:getting session data for session 21000510070660650\n",
      "2023-02-26 18:20:24 INFO     create_dataset:getting session data for session 21010412492332090\n",
      "2023-02-26 18:20:24 INFO     create_dataset:getting session data for session 22070420552530440\n",
      "2023-02-26 18:20:24 INFO     create_dataset:getting session data for session 21030310255978850\n",
      "2023-02-26 18:20:24 INFO     create_dataset:getting session data for session 21000314470579224\n",
      "2023-02-26 18:20:24 INFO     create_dataset:getting session data for session 21020409314402508\n",
      "2023-02-26 18:20:24 INFO     create_dataset:getting session data for session 20100313534893336\n",
      "2023-02-26 18:20:24 INFO     create_dataset:getting session data for session 20100512471961880\n",
      "2023-02-26 18:20:24 INFO     create_dataset:getting session data for session 20100212405834360\n",
      "2023-02-26 18:20:25 INFO     create_dataset:getting session data for session 21110410590647340\n",
      "2023-02-26 18:20:25 INFO     create_dataset:getting session data for session 21070109403405610\n",
      "2023-02-26 18:20:25 INFO     create_dataset:getting session data for session 21100420480273310\n",
      "2023-02-26 18:20:25 INFO     create_dataset:getting session data for session 22000515583089010\n",
      "2023-02-26 18:20:25 INFO     create_dataset:getting session data for session 21020411494654796\n",
      "2023-02-26 18:20:25 INFO     create_dataset:getting session data for session 20110214204257000\n",
      "2023-02-26 18:20:25 INFO     create_dataset:getting session data for session 21000513215954290\n",
      "2023-02-26 18:20:25 INFO     create_dataset:getting session data for session 21070417374026520\n",
      "2023-02-26 18:20:25 INFO     create_dataset:getting session data for session 20100408302965844\n",
      "2023-02-26 18:20:25 INFO     create_dataset:getting session data for session 22050411021202560\n",
      "2023-02-26 18:20:25 INFO     create_dataset:getting session data for session 21070222400631972\n",
      "2023-02-26 18:20:26 INFO     create_dataset:getting session data for session 20110114503907668\n",
      "2023-02-26 18:20:26 INFO     create_dataset:getting session data for session 21030411334007904\n",
      "2023-02-26 18:20:26 INFO     create_dataset:getting session data for session 22010416072399400\n",
      "2023-02-26 18:20:26 INFO     create_dataset:getting session data for session 21090414521533492\n",
      "2023-02-26 18:20:26 INFO     create_dataset:getting session data for session 22090216580365810\n",
      "2023-02-26 18:20:26 INFO     create_dataset:getting session data for session 21010210565746210\n",
      "2023-02-26 18:20:26 INFO     create_dataset:getting session data for session 21070108032202730\n",
      "2023-02-26 18:20:26 INFO     create_dataset:getting session data for session 21010412530195196\n",
      "2023-02-26 18:20:26 INFO     create_dataset:getting session data for session 21110416581133350\n",
      "2023-02-26 18:20:26 INFO     create_dataset:getting session data for session 22050514171734616\n",
      "2023-02-26 18:20:27 INFO     create_dataset:getting session data for session 22030117393232176\n",
      "2023-02-26 18:20:27 INFO     create_dataset:getting session data for session 22030212391224324\n",
      "2023-02-26 18:20:27 INFO     create_dataset:getting session data for session 21110314033140690\n",
      "2023-02-26 18:20:27 INFO     create_dataset:getting session data for session 20110220101134092\n",
      "2023-02-26 18:20:27 INFO     create_dataset:getting session data for session 21010313182088680\n",
      "2023-02-26 18:20:27 INFO     create_dataset:getting session data for session 20110214422317876\n",
      "2023-02-26 18:20:27 INFO     create_dataset:getting session data for session 22020508245547376\n",
      "2023-02-26 18:20:27 INFO     create_dataset:getting session data for session 22040415061024656\n",
      "2023-02-26 18:20:27 INFO     create_dataset:getting session data for session 22010211583065910\n",
      "2023-02-26 18:20:27 INFO     create_dataset:getting session data for session 21020512323181864\n",
      "2023-02-26 18:20:28 INFO     create_dataset:getting session data for session 20110309322254036\n",
      "2023-02-26 18:20:28 INFO     create_dataset:getting session data for session 20110520105418256\n",
      "2023-02-26 18:20:28 INFO     create_dataset:getting session data for session 21090211124928910\n",
      "2023-02-26 18:20:28 INFO     create_dataset:getting session data for session 21020410440161760\n",
      "2023-02-26 18:20:28 INFO     create_dataset:getting session data for session 21000313563883390\n",
      "2023-02-26 18:20:28 INFO     create_dataset:getting session data for session 21000415260375144\n",
      "2023-02-26 18:20:28 INFO     create_dataset:getting session data for session 21070109343149670\n",
      "2023-02-26 18:20:28 INFO     create_dataset:getting session data for session 21050508592606016\n",
      "2023-02-26 18:20:28 INFO     create_dataset:getting session data for session 21070117361766484\n",
      "2023-02-26 18:20:28 INFO     create_dataset:getting session data for session 21040309180950450\n",
      "2023-02-26 18:20:29 INFO     create_dataset:getting session data for session 20110416463346348\n",
      "2023-02-26 18:20:29 INFO     create_dataset:getting session data for session 21050213230219580\n",
      "2023-02-26 18:20:29 INFO     create_dataset:getting session data for session 21000309071681410\n",
      "2023-02-26 18:20:29 INFO     create_dataset:getting session data for session 20110109472831836\n",
      "2023-02-26 18:20:29 INFO     create_dataset:getting session data for session 21050309090282550\n",
      "2023-02-26 18:20:29 INFO     create_dataset:getting session data for session 22040109512339816\n",
      "2023-02-26 18:20:29 INFO     create_dataset:getting session data for session 21060608254727624\n",
      "2023-02-26 18:20:29 INFO     create_dataset:getting session data for session 21020209045862924\n",
      "2023-02-26 18:20:29 INFO     create_dataset:getting session data for session 21000016105557010\n",
      "2023-02-26 18:20:29 INFO     create_dataset:getting session data for session 20100510583655616\n",
      "2023-02-26 18:20:30 INFO     create_dataset:getting session data for session 21050115005697976\n",
      "2023-02-26 18:20:30 INFO     create_dataset:getting session data for session 22000612420888110\n",
      "2023-02-26 18:20:30 INFO     create_dataset:getting session data for session 21030409171790740\n",
      "2023-02-26 18:20:30 INFO     create_dataset:getting session data for session 22080316180778892\n",
      "2023-02-26 18:20:30 INFO     create_dataset:getting session data for session 22070315095273770\n",
      "2023-02-26 18:20:30 INFO     create_dataset:getting session data for session 21030508192117416\n",
      "2023-02-26 18:20:30 INFO     create_dataset:getting session data for session 21000612091936990\n",
      "2023-02-26 18:20:30 INFO     create_dataset:getting session data for session 21110514204953130\n",
      "2023-02-26 18:20:30 INFO     create_dataset:getting session data for session 21090116551982668\n",
      "2023-02-26 18:20:30 INFO     create_dataset:getting session data for session 21010318044133436\n",
      "2023-02-26 18:20:31 INFO     create_dataset:getting session data for session 21050214094026330\n",
      "2023-02-26 18:20:31 INFO     create_dataset:getting session data for session 21030312275186504\n",
      "2023-02-26 18:20:31 INFO     create_dataset:getting session data for session 21110310061035940\n",
      "2023-02-26 18:20:31 INFO     create_dataset:getting session data for session 21040212153034300\n",
      "2023-02-26 18:20:31 INFO     create_dataset:getting session data for session 21040508313102800\n",
      "2023-02-26 18:20:31 INFO     create_dataset:getting session data for session 21060313350013450\n",
      "2023-02-26 18:20:31 INFO     create_dataset:getting session data for session 21050520443635476\n",
      "2023-02-26 18:20:31 INFO     create_dataset:getting session data for session 22030310542403680\n",
      "2023-02-26 18:20:31 INFO     create_dataset:getting session data for session 21000415103389480\n",
      "2023-02-26 18:20:31 INFO     create_dataset:getting session data for session 21000114000840560\n",
      "2023-02-26 18:20:31 INFO     create_dataset:getting session data for session 21030410103095616\n",
      "2023-02-26 18:20:32 INFO     create_dataset:getting session data for session 21020310440234812\n",
      "2023-02-26 18:20:32 INFO     create_dataset:getting session data for session 21050418324251956\n",
      "2023-02-26 18:20:32 INFO     create_dataset:getting session data for session 22010513460891276\n",
      "2023-02-26 18:20:32 INFO     create_dataset:getting session data for session 21070111565402220\n",
      "2023-02-26 18:20:32 INFO     create_dataset:getting session data for session 21080310114947356\n",
      "2023-02-26 18:20:32 INFO     create_dataset:getting session data for session 20100113123469780\n",
      "2023-02-26 18:20:32 INFO     create_dataset:getting session data for session 22050310281772490\n",
      "2023-02-26 18:20:32 INFO     create_dataset:getting session data for session 21050317394483692\n",
      "2023-02-26 18:20:32 INFO     create_dataset:getting session data for session 20100113302320896\n",
      "2023-02-26 18:20:33 INFO     create_dataset:getting session data for session 20110209155526932\n",
      "2023-02-26 18:20:33 INFO     create_dataset:getting session data for session 21090110403296550\n",
      "2023-02-26 18:20:33 INFO     create_dataset:getting session data for session 21110418195429850\n",
      "2023-02-26 18:20:33 INFO     create_dataset:getting session data for session 22040212561428740\n",
      "2023-02-26 18:20:33 INFO     create_dataset:getting session data for session 21030513134153840\n",
      "2023-02-26 18:20:33 INFO     create_dataset:getting session data for session 21060211121990920\n",
      "2023-02-26 18:20:33 INFO     create_dataset:getting session data for session 21040220121058130\n",
      "2023-02-26 18:20:33 INFO     create_dataset:getting session data for session 22050310385585084\n",
      "2023-02-26 18:20:33 INFO     create_dataset:getting session data for session 21100313144836376\n",
      "2023-02-26 18:20:33 INFO     create_dataset:getting session data for session 21040309481211116\n",
      "2023-02-26 18:20:34 INFO     create_dataset:getting session data for session 21020511402008524\n",
      "2023-02-26 18:20:34 INFO     create_dataset:getting session data for session 21080002110529812\n",
      "2023-02-26 18:20:34 INFO     create_dataset:getting session data for session 21020506570756920\n",
      "2023-02-26 18:20:34 INFO     create_dataset:getting session data for session 22090410273056856\n",
      "2023-02-26 18:20:34 INFO     create_dataset:getting session data for session 21000510370631090\n",
      "2023-02-26 18:20:34 INFO     create_dataset:getting session data for session 22010107585684490\n",
      "2023-02-26 18:20:34 INFO     create_dataset:getting session data for session 21010411561952676\n",
      "2023-02-26 18:20:34 INFO     create_dataset:getting session data for session 20110410594805388\n",
      "2023-02-26 18:20:34 INFO     create_dataset:getting session data for session 22010309393230964\n",
      "2023-02-26 18:20:34 INFO     create_dataset:getting session data for session 20110512025373468\n",
      "2023-02-26 18:20:35 INFO     create_dataset:getting session data for session 22020112131325012\n",
      "2023-02-26 18:20:35 INFO     create_dataset:getting session data for session 20110508454075570\n",
      "2023-02-26 18:20:35 INFO     create_dataset:getting session data for session 21040211111893076\n",
      "2023-02-26 18:20:35 INFO     create_dataset:getting session data for session 21110519521612628\n",
      "2023-02-26 18:20:35 INFO     create_dataset:getting session data for session 21010312003609064\n",
      "2023-02-26 18:20:35 INFO     create_dataset:getting session data for session 21030117515650590\n",
      "2023-02-26 18:20:35 INFO     create_dataset:getting session data for session 21020209190976030\n",
      "2023-02-26 18:20:35 INFO     create_dataset:getting session data for session 21000512372000308\n",
      "2023-02-26 18:20:35 INFO     create_dataset:getting session data for session 21050312001474890\n",
      "2023-02-26 18:20:35 INFO     create_dataset:getting session data for session 21030522254701264\n",
      "2023-02-26 18:20:36 INFO     create_dataset:getting session data for session 21040409561344456\n",
      "2023-02-26 18:20:36 INFO     create_dataset:getting session data for session 21050016284319500\n",
      "2023-02-26 18:20:36 INFO     create_dataset:getting session data for session 21060121352623252\n",
      "2023-02-26 18:20:36 INFO     create_dataset:getting session data for session 21020210410932576\n",
      "2023-02-26 18:20:36 INFO     create_dataset:getting session data for session 21050512491208544\n",
      "2023-02-26 18:20:36 INFO     create_dataset:getting session data for session 21050510040874584\n",
      "2023-02-26 18:20:36 INFO     create_dataset:getting session data for session 21100613503987840\n",
      "2023-02-26 18:20:36 INFO     create_dataset:getting session data for session 21110415311856636\n",
      "2023-02-26 18:20:36 INFO     create_dataset:getting session data for session 21000212495428800\n",
      "2023-02-26 18:20:36 INFO     create_dataset:getting session data for session 21110607373922956\n",
      "2023-02-26 18:20:37 INFO     create_dataset:getting session data for session 21040613490156770\n",
      "2023-02-26 18:20:37 INFO     create_dataset:getting session data for session 22010407543090330\n",
      "2023-02-26 18:20:37 INFO     create_dataset:getting session data for session 21000108443399060\n",
      "2023-02-26 18:20:37 INFO     create_dataset:getting session data for session 20110510200463710\n",
      "2023-02-26 18:20:37 INFO     create_dataset:getting session data for session 22100211280762644\n",
      "2023-02-26 18:20:37 INFO     create_dataset:getting session data for session 21000514361620030\n",
      "2023-02-26 18:20:37 INFO     create_dataset:getting session data for session 22040609184932292\n",
      "2023-02-26 18:20:37 INFO     create_dataset:getting session data for session 21090207533519370\n",
      "2023-02-26 18:20:37 INFO     create_dataset:getting session data for session 21030415205995200\n",
      "2023-02-26 18:20:37 INFO     create_dataset:getting session data for session 21000217394731896\n",
      "2023-02-26 18:20:37 INFO     create_dataset:getting session data for session 21080013471570292\n",
      "2023-02-26 18:20:38 INFO     create_dataset:getting session data for session 20100014024611770\n",
      "2023-02-26 18:20:38 INFO     create_dataset:getting session data for session 21000116364360060\n",
      "2023-02-26 18:20:38 INFO     create_dataset:getting session data for session 21020118593755332\n",
      "2023-02-26 18:20:38 INFO     create_dataset:getting session data for session 21100315062982220\n",
      "2023-02-26 18:20:38 INFO     create_dataset:getting session data for session 21030310020310904\n",
      "2023-02-26 18:20:38 INFO     create_dataset:getting session data for session 21030416305200344\n",
      "2023-02-26 18:20:38 INFO     create_dataset:getting session data for session 21000513133925812\n",
      "2023-02-26 18:20:38 INFO     create_dataset:getting session data for session 22080308213783036\n",
      "2023-02-26 18:20:38 INFO     create_dataset:getting session data for session 21020515342967308\n",
      "2023-02-26 18:20:38 INFO     create_dataset:getting session data for session 22090109370824390\n",
      "2023-02-26 18:20:38 INFO     create_dataset:getting session data for session 21090617084210056\n",
      "2023-02-26 18:20:39 INFO     create_dataset:getting session data for session 20110310304708730\n",
      "2023-02-26 18:20:39 INFO     create_dataset:getting session data for session 21030308273845052\n",
      "2023-02-26 18:20:39 INFO     create_dataset:getting session data for session 20100317235835180\n",
      "2023-02-26 18:20:39 INFO     create_dataset:getting session data for session 21100509450825550\n",
      "2023-02-26 18:20:39 INFO     create_dataset:getting session data for session 21010411170424680\n",
      "2023-02-26 18:20:39 INFO     create_dataset:getting session data for session 20110411560620150\n",
      "2023-02-26 18:20:39 INFO     create_dataset:getting session data for session 21090218305797068\n",
      "2023-02-26 18:20:39 INFO     create_dataset:getting session data for session 22010612523894370\n",
      "2023-02-26 18:20:39 INFO     create_dataset:getting session data for session 21030115455697504\n",
      "2023-02-26 18:20:39 INFO     create_dataset:getting session data for session 20100112123609416\n",
      "2023-02-26 18:20:39 INFO     create_dataset:getting session data for session 21000110040970850\n",
      "2023-02-26 18:20:40 INFO     create_dataset:getting session data for session 21040312351610868\n",
      "2023-02-26 18:20:40 INFO     create_dataset:getting session data for session 20100517243747264\n",
      "2023-02-26 18:20:40 INFO     create_dataset:getting session data for session 20100514013578640\n",
      "2023-02-26 18:20:40 INFO     create_dataset:getting session data for session 21040422040530944\n",
      "2023-02-26 18:20:40 INFO     create_dataset:getting session data for session 21030512041335320\n",
      "2023-02-26 18:20:40 INFO     create_dataset:getting session data for session 21040015265862830\n",
      "2023-02-26 18:20:40 INFO     create_dataset:getting session data for session 21050510254628010\n",
      "2023-02-26 18:20:40 INFO     create_dataset:getting session data for session 21070414163887664\n",
      "2023-02-26 18:20:40 INFO     create_dataset:getting session data for session 21110011040306396\n",
      "2023-02-26 18:20:40 INFO     create_dataset:getting session data for session 20110409540648956\n",
      "2023-02-26 18:20:41 INFO     create_dataset:getting session data for session 22030520132196092\n",
      "2023-02-26 18:20:41 INFO     create_dataset:getting session data for session 20100409192824536\n",
      "2023-02-26 18:20:41 INFO     create_dataset:getting session data for session 22000015354602440\n",
      "2023-02-26 18:20:41 INFO     create_dataset:getting session data for session 21010413170133092\n",
      "2023-02-26 18:20:41 INFO     create_dataset:getting session data for session 21030209535840748\n",
      "2023-02-26 18:20:41 INFO     create_dataset:getting session data for session 20110415474623424\n",
      "2023-02-26 18:20:41 INFO     create_dataset:getting session data for session 22080516304705064\n",
      "2023-02-26 18:20:41 INFO     create_dataset:getting session data for session 21050613182377904\n",
      "2023-02-26 18:20:41 INFO     create_dataset:getting session data for session 20100408584364452\n",
      "2023-02-26 18:20:41 INFO     create_dataset:getting session data for session 21050608591090710\n",
      "2023-02-26 18:20:41 INFO     create_dataset:getting session data for session 20110413302249640\n",
      "2023-02-26 18:20:42 INFO     create_dataset:getting session data for session 21030309170264704\n",
      "2023-02-26 18:20:42 INFO     create_dataset:getting session data for session 21010208483665184\n",
      "2023-02-26 18:20:42 INFO     create_dataset:getting session data for session 22030308575046348\n",
      "2023-02-26 18:20:42 INFO     create_dataset:getting session data for session 21050614470265536\n",
      "2023-02-26 18:20:42 INFO     create_dataset:getting session data for session 21010309210365372\n",
      "2023-02-26 18:20:42 INFO     create_dataset:getting session data for session 20110210112648310\n",
      "2023-02-26 18:20:42 INFO     create_dataset:getting session data for session 21060414210907000\n",
      "2023-02-26 18:20:42 INFO     create_dataset:getting session data for session 20110216571922120\n",
      "2023-02-26 18:20:42 INFO     create_dataset:getting session data for session 22030618541988892\n",
      "2023-02-26 18:20:42 INFO     create_dataset:getting session data for session 20110415520564910\n",
      "2023-02-26 18:20:43 INFO     create_dataset:getting session data for session 21030108571684228\n",
      "2023-02-26 18:20:43 INFO     create_dataset:getting session data for session 21000117042293830\n",
      "2023-02-26 18:20:43 INFO     create_dataset:getting session data for session 20100108574283104\n",
      "2023-02-26 18:20:43 INFO     create_dataset:getting session data for session 21030508542311120\n",
      "2023-02-26 18:20:43 INFO     create_dataset:getting session data for session 22010608182326820\n",
      "2023-02-26 18:20:43 INFO     create_dataset:getting session data for session 21100312155839024\n",
      "2023-02-26 18:20:43 INFO     create_dataset:getting session data for session 20110209271469090\n",
      "2023-02-26 18:20:43 INFO     create_dataset:getting session data for session 22000317232476988\n",
      "2023-02-26 18:20:43 INFO     create_dataset:getting session data for session 22070307504616904\n",
      "2023-02-26 18:20:43 INFO     create_dataset:getting session data for session 21020416132919770\n",
      "2023-02-26 18:20:43 INFO     create_dataset:getting session data for session 20110515175592716\n",
      "2023-02-26 18:20:44 INFO     create_dataset:getting session data for session 21100113333213160\n",
      "2023-02-26 18:20:44 INFO     create_dataset:getting session data for session 21100413381486244\n",
      "2023-02-26 18:20:44 INFO     create_dataset:getting session data for session 21050509440780496\n",
      "2023-02-26 18:20:44 INFO     create_dataset:getting session data for session 21050211060265880\n",
      "2023-02-26 18:20:44 INFO     create_dataset:getting session data for session 21000211412522700\n",
      "2023-02-26 18:20:44 INFO     create_dataset:getting session data for session 21030309440797164\n",
      "2023-02-26 18:20:44 INFO     create_dataset:getting session data for session 21050510085176896\n",
      "2023-02-26 18:20:44 INFO     create_dataset:getting session data for session 21010312031065908\n",
      "2023-02-26 18:20:44 INFO     create_dataset:getting session data for session 21040414360150184\n",
      "2023-02-26 18:20:44 INFO     create_dataset:getting session data for session 22000115025483788\n",
      "2023-02-26 18:20:44 INFO     create_dataset:getting session data for session 21100212330632284\n",
      "2023-02-26 18:20:45 INFO     create_dataset:getting session data for session 21030210232994470\n",
      "2023-02-26 18:20:45 INFO     create_dataset:getting session data for session 21040215371464104\n",
      "2023-02-26 18:20:45 INFO     create_dataset:getting session data for session 20100509215927520\n",
      "2023-02-26 18:20:45 INFO     create_dataset:getting session data for session 21100113283666830\n",
      "2023-02-26 18:20:45 INFO     create_dataset:getting session data for session 21050417591752260\n",
      "2023-02-26 18:20:45 INFO     create_dataset:getting session data for session 20110209411889930\n",
      "2023-02-26 18:20:45 INFO     create_dataset:getting session data for session 22030516013480668\n",
      "2023-02-26 18:20:45 INFO     create_dataset:getting session data for session 21020216382617130\n",
      "2023-02-26 18:20:45 INFO     create_dataset:getting session data for session 21010313390492816\n",
      "2023-02-26 18:20:45 INFO     create_dataset:getting session data for session 21100509140084344\n",
      "2023-02-26 18:20:45 INFO     create_dataset:getting session data for session 22010520093220390\n",
      "2023-02-26 18:20:46 INFO     create_dataset:getting session data for session 21040512040612584\n",
      "2023-02-26 18:20:46 INFO     create_dataset:getting session data for session 20110211212136930\n",
      "2023-02-26 18:20:46 INFO     create_dataset:getting session data for session 21020113594322660\n",
      "2023-02-26 18:20:46 INFO     create_dataset:getting session data for session 21030313104625484\n",
      "2023-02-26 18:20:46 INFO     create_dataset:getting session data for session 20100507183976492\n",
      "2023-02-26 18:20:46 INFO     create_dataset:getting session data for session 21030615494185540\n",
      "2023-02-26 18:20:46 INFO     create_dataset:getting session data for session 21040511281817252\n",
      "2023-02-26 18:20:46 INFO     create_dataset:getting session data for session 21090110474133668\n",
      "2023-02-26 18:20:46 INFO     create_dataset:getting session data for session 21040210194013024\n",
      "2023-02-26 18:20:46 INFO     create_dataset:getting session data for session 21000321064909480\n",
      "2023-02-26 18:20:46 INFO     create_dataset:getting session data for session 22040215393281720\n",
      "2023-02-26 18:20:47 INFO     create_dataset:getting session data for session 21020209023783640\n",
      "2023-02-26 18:20:47 INFO     create_dataset:getting session data for session 21030610473086856\n",
      "2023-02-26 18:20:47 INFO     create_dataset:getting session data for session 21100415163279390\n",
      "2023-02-26 18:20:47 INFO     create_dataset:getting session data for session 22080015402731950\n",
      "2023-02-26 18:20:47 INFO     create_dataset:getting session data for session 21050114512257464\n",
      "2023-02-26 18:20:47 INFO     create_dataset:getting session data for session 21040313173287830\n",
      "2023-02-26 18:20:47 INFO     create_dataset:getting session data for session 20110112130838736\n",
      "2023-02-26 18:20:47 INFO     create_dataset:getting session data for session 20110010372790620\n",
      "2023-02-26 18:20:47 INFO     create_dataset:getting session data for session 21030610474612896\n",
      "2023-02-26 18:20:47 INFO     create_dataset:getting session data for session 21090210502325530\n",
      "2023-02-26 18:20:47 INFO     create_dataset:getting session data for session 21090016495602520\n",
      "2023-02-26 18:20:48 INFO     create_dataset:getting session data for session 22020416091601812\n",
      "2023-02-26 18:20:48 INFO     create_dataset:getting session data for session 21010410484176520\n",
      "2023-02-26 18:20:48 INFO     create_dataset:getting session data for session 21010009591790924\n",
      "2023-02-26 18:20:48 INFO     create_dataset:getting session data for session 22000318161532776\n",
      "2023-02-26 18:20:48 INFO     create_dataset:getting session data for session 21020510175059190\n",
      "2023-02-26 18:20:48 INFO     create_dataset:getting session data for session 21020511375289140\n",
      "2023-02-26 18:20:48 INFO     create_dataset:getting session data for session 21030110085100430\n",
      "2023-02-26 18:20:48 INFO     create_dataset:getting session data for session 21020410144770800\n",
      "2023-02-26 18:20:48 INFO     create_dataset:getting session data for session 22010413122202950\n",
      "2023-02-26 18:20:48 INFO     create_dataset:getting session data for session 21030412493930036\n",
      "2023-02-26 18:20:48 INFO     create_dataset:getting session data for session 22010517352596704\n",
      "2023-02-26 18:20:49 INFO     create_dataset:getting session data for session 21070511375476310\n",
      "2023-02-26 18:20:49 INFO     create_dataset:getting session data for session 20110208343365296\n",
      "2023-02-26 18:20:49 INFO     create_dataset:getting session data for session 21000616204463236\n",
      "2023-02-26 18:20:49 INFO     create_dataset:getting session data for session 21100518255577210\n",
      "2023-02-26 18:20:49 INFO     create_dataset:getting session data for session 21030511373220468\n",
      "2023-02-26 18:20:49 INFO     create_dataset:getting session data for session 21010012014199012\n",
      "2023-02-26 18:20:49 INFO     create_dataset:getting session data for session 21040513164533496\n",
      "2023-02-26 18:20:49 INFO     create_dataset:getting session data for session 21050414471187064\n",
      "2023-02-26 18:20:49 INFO     create_dataset:getting session data for session 21070615320318156\n",
      "2023-02-26 18:20:49 INFO     create_dataset:getting session data for session 21090007130823420\n",
      "2023-02-26 18:20:50 INFO     create_dataset:getting session data for session 21000208383465640\n",
      "2023-02-26 18:20:50 INFO     create_dataset:getting session data for session 22080215531776344\n",
      "2023-02-26 18:20:50 INFO     create_dataset:getting session data for session 21010016555024356\n",
      "2023-02-26 18:20:50 INFO     create_dataset:getting session data for session 20100011290663880\n",
      "2023-02-26 18:20:50 INFO     create_dataset:getting session data for session 21010211135945760\n",
      "2023-02-26 18:20:50 INFO     create_dataset:getting session data for session 20110210563561736\n",
      "2023-02-26 18:20:50 INFO     create_dataset:getting session data for session 21050221031639852\n",
      "2023-02-26 18:20:50 INFO     create_dataset:getting session data for session 21030308174130532\n",
      "2023-02-26 18:20:50 INFO     create_dataset:getting session data for session 21100218384814170\n",
      "2023-02-26 18:20:50 INFO     create_dataset:getting session data for session 20100209033283424\n",
      "2023-02-26 18:20:50 INFO     create_dataset:getting session data for session 21050215063610800\n",
      "2023-02-26 18:20:51 INFO     create_dataset:getting session data for session 22040208395907304\n",
      "2023-02-26 18:20:51 INFO     create_dataset:getting session data for session 22020517251647132\n",
      "2023-02-26 18:20:51 INFO     create_dataset:getting session data for session 21080308202932504\n",
      "2023-02-26 18:20:51 INFO     create_dataset:getting session data for session 22070408321659480\n",
      "2023-02-26 18:20:51 INFO     create_dataset:getting session data for session 20110508462583196\n",
      "2023-02-26 18:20:51 INFO     create_dataset:getting session data for session 21040410274042476\n",
      "2023-02-26 18:20:51 INFO     create_dataset:getting session data for session 21050211370845184\n",
      "2023-02-26 18:20:51 INFO     create_dataset:getting session data for session 21110111441431910\n",
      "2023-02-26 18:20:51 INFO     create_dataset:getting session data for session 20110113272712476\n",
      "2023-02-26 18:20:51 INFO     create_dataset:getting session data for session 21080309491604936\n",
      "2023-02-26 18:20:51 INFO     create_dataset:getting session data for session 21010311111950780\n",
      "2023-02-26 18:20:52 INFO     create_dataset:getting session data for session 21080108263705280\n",
      "2023-02-26 18:20:52 INFO     create_dataset:getting session data for session 21010515032438656\n",
      "2023-02-26 18:20:52 INFO     create_dataset:getting session data for session 21010421123692696\n",
      "2023-02-26 18:20:52 INFO     create_dataset:getting session data for session 21030412040880012\n",
      "2023-02-26 18:20:52 INFO     create_dataset:getting session data for session 21050209372343084\n",
      "2023-02-26 18:20:52 INFO     create_dataset:getting session data for session 21090411094552404\n",
      "2023-02-26 18:20:52 INFO     create_dataset:getting session data for session 21110310354624630\n",
      "2023-02-26 18:20:52 INFO     create_dataset:getting session data for session 21040213480695650\n",
      "2023-02-26 18:20:52 INFO     create_dataset:getting session data for session 21010408442736760\n",
      "2023-02-26 18:20:53 INFO     create_dataset:getting session data for session 21110617122748860\n",
      "2023-02-26 18:20:53 INFO     create_dataset:getting session data for session 21020412071544644\n",
      "2023-02-26 18:20:53 INFO     create_dataset:getting session data for session 21030318221234270\n",
      "2023-02-26 18:20:53 INFO     create_dataset:getting session data for session 21010317070070650\n",
      "2023-02-26 18:20:53 INFO     create_dataset:getting session data for session 21030318351668108\n",
      "2023-02-26 18:20:53 INFO     create_dataset:getting session data for session 22090116163166540\n",
      "2023-02-26 18:20:53 INFO     create_dataset:getting session data for session 20110614281477240\n",
      "2023-02-26 18:20:53 INFO     create_dataset:getting session data for session 21010108321792052\n",
      "2023-02-26 18:20:53 INFO     create_dataset:getting session data for session 21070522470902664\n",
      "2023-02-26 18:20:53 INFO     create_dataset:getting session data for session 21040410035728360\n",
      "2023-02-26 18:20:54 INFO     create_dataset:getting session data for session 21030409071880910\n",
      "2023-02-26 18:20:54 INFO     create_dataset:getting session data for session 21090016325129976\n",
      "2023-02-26 18:20:54 INFO     create_dataset:getting session data for session 21000209355211664\n",
      "2023-02-26 18:20:54 INFO     create_dataset:getting session data for session 21080318324821828\n",
      "2023-02-26 18:20:54 INFO     create_dataset:getting session data for session 21090509350921284\n",
      "2023-02-26 18:20:54 INFO     create_dataset:getting session data for session 22050413264814252\n",
      "2023-02-26 18:20:54 INFO     create_dataset:getting session data for session 21110109212397610\n",
      "2023-02-26 18:20:54 INFO     create_dataset:getting session data for session 21010619232552656\n",
      "2023-02-26 18:20:54 INFO     create_dataset:getting session data for session 22040409302725096\n",
      "2023-02-26 18:20:55 INFO     create_dataset:getting session data for session 21080415571870412\n",
      "2023-02-26 18:20:55 INFO     create_dataset:getting session data for session 21000513282420696\n",
      "2023-02-26 18:20:55 INFO     create_dataset:getting session data for session 22090420233066924\n",
      "2023-02-26 18:20:55 INFO     create_dataset:getting session data for session 21090007491895068\n",
      "2023-02-26 18:20:55 INFO     create_dataset:getting session data for session 21020102161655892\n",
      "2023-02-26 18:20:55 INFO     create_dataset:getting session data for session 22000607593149772\n",
      "2023-02-26 18:20:55 INFO     create_dataset:getting session data for session 21090317265061510\n",
      "2023-02-26 18:20:55 INFO     create_dataset:getting session data for session 22040115522407610\n",
      "2023-02-26 18:20:55 INFO     create_dataset:getting session data for session 21040208033472908\n",
      "2023-02-26 18:20:55 INFO     create_dataset:getting session data for session 20100212300638140\n",
      "2023-02-26 18:20:56 INFO     create_dataset:getting session data for session 21000214593225530\n",
      "2023-02-26 18:20:56 INFO     create_dataset:getting session data for session 22090608111504856\n",
      "2023-02-26 18:20:56 INFO     create_dataset:getting session data for session 20110114125281890\n",
      "2023-02-26 18:20:56 INFO     create_dataset:getting session data for session 20110109053633490\n",
      "2023-02-26 18:20:56 INFO     create_dataset:getting session data for session 20100111262209730\n",
      "2023-02-26 18:20:56 INFO     create_dataset:getting session data for session 22070111022202932\n",
      "2023-02-26 18:20:56 INFO     create_dataset:getting session data for session 21000211242185710\n",
      "2023-02-26 18:20:56 INFO     create_dataset:getting session data for session 21050512395969400\n",
      "2023-02-26 18:20:56 INFO     create_dataset:getting session data for session 22020317531665430\n",
      "2023-02-26 18:20:56 INFO     create_dataset:getting session data for session 21100407250625110\n",
      "2023-02-26 18:20:57 INFO     create_dataset:getting session data for session 21040214522595896\n",
      "2023-02-26 18:20:57 INFO     create_dataset:getting session data for session 21080314341081492\n",
      "2023-02-26 18:20:57 INFO     create_dataset:getting session data for session 21020018394028170\n",
      "2023-02-26 18:20:57 INFO     create_dataset:getting session data for session 21010210364230172\n",
      "2023-02-26 18:20:57 INFO     create_dataset:getting session data for session 21050413103059068\n",
      "2023-02-26 18:20:57 INFO     create_dataset:getting session data for session 20110113242155850\n",
      "2023-02-26 18:20:57 INFO     create_dataset:getting session data for session 21080509540913244\n",
      "2023-02-26 18:20:57 INFO     create_dataset:getting session data for session 21070114251100376\n",
      "2023-02-26 18:20:57 INFO     create_dataset:getting session data for session 21080315241168810\n",
      "2023-02-26 18:20:58 INFO     create_dataset:getting session data for session 20100115195541796\n",
      "2023-02-26 18:20:58 INFO     create_dataset:getting session data for session 21080509493611704\n",
      "2023-02-26 18:20:58 INFO     create_dataset:getting session data for session 22010513111563070\n",
      "2023-02-26 18:20:58 INFO     create_dataset:getting session data for session 22100208551963804\n",
      "2023-02-26 18:20:58 INFO     create_dataset:getting session data for session 21070513032547188\n",
      "2023-02-26 18:20:58 INFO     create_dataset:getting session data for session 21030212442746804\n",
      "2023-02-26 18:20:58 INFO     create_dataset:getting session data for session 21020120040894536\n",
      "2023-02-26 18:20:58 INFO     create_dataset:getting session data for session 21040310202171452\n",
      "2023-02-26 18:20:58 INFO     create_dataset:getting session data for session 21080408241735716\n",
      "2023-02-26 18:20:58 INFO     create_dataset:getting session data for session 21030515015629452\n",
      "2023-02-26 18:20:59 INFO     create_dataset:getting session data for session 21010019075568292\n",
      "2023-02-26 18:20:59 INFO     create_dataset:getting session data for session 21090515181364548\n",
      "2023-02-26 18:20:59 INFO     create_dataset:getting session data for session 22030310440373084\n",
      "2023-02-26 18:20:59 INFO     create_dataset:getting session data for session 20100511362544820\n",
      "2023-02-26 18:20:59 INFO     create_dataset:getting session data for session 22010511150486460\n",
      "2023-02-26 18:20:59 INFO     create_dataset:getting session data for session 21100209420527460\n",
      "2023-02-26 18:20:59 INFO     create_dataset:getting session data for session 21030414084447204\n",
      "2023-02-26 18:20:59 INFO     create_dataset:getting session data for session 22000111004437220\n",
      "2023-02-26 18:20:59 INFO     create_dataset:getting session data for session 21040310465012476\n",
      "2023-02-26 18:21:00 INFO     create_dataset:getting session data for session 21080412323184380\n",
      "2023-02-26 18:21:00 INFO     create_dataset:getting session data for session 21080114244696680\n",
      "2023-02-26 18:21:00 INFO     create_dataset:getting session data for session 21050211501590224\n",
      "2023-02-26 18:21:00 INFO     create_dataset:getting session data for session 21000511354127510\n",
      "2023-02-26 18:21:00 INFO     create_dataset:getting session data for session 21020109012610784\n",
      "2023-02-26 18:21:00 INFO     create_dataset:getting session data for session 22000617005704650\n",
      "2023-02-26 18:21:00 INFO     create_dataset:getting session data for session 21070210121785590\n",
      "2023-02-26 18:21:00 INFO     create_dataset:getting session data for session 21090008583686810\n",
      "2023-02-26 18:21:00 INFO     create_dataset:getting session data for session 22050112440689324\n",
      "2023-02-26 18:21:00 INFO     create_dataset:getting session data for session 21070217430978804\n",
      "2023-02-26 18:21:01 INFO     create_dataset:getting session data for session 20110510350351948\n",
      "2023-02-26 18:21:01 INFO     create_dataset:getting session data for session 21090209091189600\n",
      "2023-02-26 18:21:01 INFO     create_dataset:getting session data for session 21030513213316920\n",
      "2023-02-26 18:21:01 INFO     create_dataset:getting session data for session 20100311022013350\n",
      "2023-02-26 18:21:01 INFO     create_dataset:getting session data for session 21000209082333980\n",
      "2023-02-26 18:21:01 INFO     create_dataset:getting session data for session 21030311182865230\n",
      "2023-02-26 18:21:01 INFO     create_dataset:getting session data for session 21050313475227540\n",
      "2023-02-26 18:21:01 INFO     create_dataset:getting session data for session 20110213445603908\n",
      "2023-02-26 18:21:01 INFO     create_dataset:getting session data for session 21040409293711110\n",
      "2023-02-26 18:21:02 INFO     create_dataset:getting session data for session 21010410424721230\n",
      "2023-02-26 18:21:02 INFO     create_dataset:getting session data for session 21100515501816250\n",
      "2023-02-26 18:21:02 INFO     create_dataset:getting session data for session 21090316150500116\n",
      "2023-02-26 18:21:02 INFO     create_dataset:getting session data for session 22000410293060230\n",
      "2023-02-26 18:21:02 INFO     create_dataset:getting session data for session 22050212063135470\n",
      "2023-02-26 18:21:02 INFO     create_dataset:getting session data for session 21040414021866736\n",
      "2023-02-26 18:21:02 INFO     create_dataset:getting session data for session 21100410241808760\n",
      "2023-02-26 18:21:02 INFO     create_dataset:getting session data for session 22030515340510800\n",
      "2023-02-26 18:21:02 INFO     create_dataset:getting session data for session 21030508472329810\n",
      "2023-02-26 18:21:02 INFO     create_dataset:getting session data for session 20100316454852148\n",
      "2023-02-26 18:21:03 INFO     create_dataset:getting session data for session 22010620305838160\n",
      "2023-02-26 18:21:03 INFO     create_dataset:getting session data for session 21110310212870984\n",
      "2023-02-26 18:21:03 INFO     create_dataset:getting session data for session 21030416365852590\n",
      "2023-02-26 18:21:03 INFO     create_dataset:getting session data for session 21000215241966160\n",
      "2023-02-26 18:21:03 INFO     create_dataset:getting session data for session 21100413234453064\n",
      "2023-02-26 18:21:03 INFO     create_dataset:getting session data for session 21030223443612610\n",
      "2023-02-26 18:21:03 INFO     create_dataset:getting session data for session 21020312050289950\n",
      "2023-02-26 18:21:03 INFO     create_dataset:getting session data for session 21110007293879990\n",
      "2023-02-26 18:21:03 INFO     create_dataset:getting session data for session 21090610423242720\n",
      "2023-02-26 18:21:04 INFO     create_dataset:getting session data for session 21060114420269390\n",
      "2023-02-26 18:21:04 INFO     create_dataset:getting session data for session 21080216282962304\n",
      "2023-02-26 18:21:04 INFO     create_dataset:getting session data for session 21020309541885184\n",
      "2023-02-26 18:21:04 INFO     create_dataset:getting session data for session 21030514193807464\n",
      "2023-02-26 18:21:04 INFO     create_dataset:getting session data for session 22080315334727640\n",
      "2023-02-26 18:21:04 INFO     create_dataset:getting session data for session 22040115272482584\n",
      "2023-02-26 18:21:04 INFO     create_dataset:getting session data for session 21030212583973070\n",
      "2023-02-26 18:21:04 INFO     create_dataset:getting session data for session 21030408501428510\n",
      "2023-02-26 18:21:04 INFO     create_dataset:getting session data for session 22010213045692320\n",
      "2023-02-26 18:21:05 INFO     create_dataset:getting session data for session 21000209333244864\n",
      "2023-02-26 18:21:05 INFO     create_dataset: converting to numpy arrays\n",
      "2023-02-26 18:21:05 INFO     create_dataset: returning results\n",
      "2023-02-26 18:21:05 INFO     dataset_generator:returning output\n"
     ]
    }
   ],
   "source": [
    "def dataset_generator(X: pd.DataFrame,\n",
    "                      y: pd.DataFrame,\n",
    "                      session_list: list,\n",
    "                      batch_size: int,\n",
    "                      event_count:int = 1000) -> Tuple[np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Creates a dataset generator for a specific set of sessions and question numbers.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        The main dataset.\n",
    "\n",
    "    y : pd.DataFrame\n",
    "        The label dataset.\n",
    "\n",
    "    session_list : list\n",
    "        The list of session ids.\n",
    "\n",
    "    batch_size : int\n",
    "        The batch size.\n",
    "\n",
    "    event_count : int, optional\n",
    "        The number of events to include in the dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.array, np.array]\n",
    "        The dataset and the labels.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        for i in range(0, len(session_list), batch_size):\n",
    "            logging.info(f'dataset_generator:Creating dataset for sessions {i} to {i+batch_size}')\n",
    "            batch_sessions = session_list[i:i+batch_size]\n",
    "            data_batch = create_dataset(X, y, batch_sessions, event_count)\n",
    "            \n",
    "            logging.info(f'dataset_generator:returning output')\n",
    "            yield data_batch\n",
    "\n",
    "# test the generator\n",
    "if 'train' in globals():\n",
    "    train_generator = dataset_generator(\n",
    "        X=df_source,\n",
    "        y=df_source_labels,\n",
    "        session_list=train,\n",
    "        batch_size=500,\n",
    "        event_count=100)\n",
    "\n",
    "    batch = next(train_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13173445, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>event_name</th>\n",
       "      <th>name</th>\n",
       "      <th>level</th>\n",
       "      <th>room_coor_x</th>\n",
       "      <th>room_coor_y</th>\n",
       "      <th>screen_coor_x</th>\n",
       "      <th>screen_coor_y</th>\n",
       "      <th>fqid</th>\n",
       "      <th>room_fqid</th>\n",
       "      <th>text_fqid</th>\n",
       "      <th>level_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>0</td>\n",
       "      <td>cutscene_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>intro</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.intro</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>1323</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>831</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          session_id  elapsed_time      event_name   name  level  room_coor_x  \\\n",
       "0  20090312431273200             0  cutscene_click  basic      0  -413.991405   \n",
       "1  20090312431273200          1323    person_click  basic      0  -413.991405   \n",
       "2  20090312431273200           831    person_click  basic      0  -413.991405   \n",
       "\n",
       "   room_coor_y  screen_coor_x  screen_coor_y    fqid  \\\n",
       "0  -159.314686          380.0          494.0   intro   \n",
       "1  -159.314686          380.0          494.0  gramps   \n",
       "2  -159.314686          380.0          494.0  gramps   \n",
       "\n",
       "                        room_fqid  \\\n",
       "0  tunic.historicalsociety.closet   \n",
       "1  tunic.historicalsociety.closet   \n",
       "2  tunic.historicalsociety.closet   \n",
       "\n",
       "                                           text_fqid level_group  \n",
       "0               tunic.historicalsociety.closet.intro         0-4  \n",
       "1  tunic.historicalsociety.closet.gramps.intro_0_...         0-4  \n",
       "2  tunic.historicalsociety.closet.gramps.intro_0_...         0-4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare the main dataset\n",
    "df_source = prepare_main_dataset(df_source)\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    print(df_source.shape)\n",
    "    display(df_source.head(3))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>question_num</th>\n",
       "      <th>correct</th>\n",
       "      <th>level_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21476</th>\n",
       "      <td>22010116250792520</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84068</th>\n",
       "      <td>21000111433937450</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171219</th>\n",
       "      <td>21040510125933256</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>13-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               session_id  question_num  correct level_group\n",
       "21476   22010116250792520             2        1         0-4\n",
       "84068   21000111433937450             8        1        5-12\n",
       "171219  21040510125933256            15        0       13-22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare the label dataset\n",
    "df_source_labels = prepare_label_dataset(df_source_labels)\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_source_labels.sample(n=3, random_state=51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 51\n",
    "sample_size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Session IDs: 11779\n",
      "Sample Session IDs: 2000\n"
     ]
    }
   ],
   "source": [
    "# select all the unique session ids\n",
    "all_session_ids = df_source_labels['session_id'].unique()\n",
    "print('All Session IDs:', len(all_session_ids))\n",
    "\n",
    "# create a sample for testing\n",
    "session_ids = np.random.choice(all_session_ids, size=sample_size, replace=False)\n",
    "print('Sample Session IDs:', len(session_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1400\n",
      "Validation: 300\n",
      "Test: 300\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into train, validation and test sets\n",
    "train, test = train_test_split(session_ids, test_size=0.3)\n",
    "test, val = train_test_split(test, test_size=0.5)\n",
    "\n",
    "# print the number of sessions in each set\n",
    "print(f'Train: {len(train)}')\n",
    "print(f'Validation: {len(val)}')\n",
    "print(f'Test: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_count = 100\n",
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = dataset_generator(\n",
    "    X=df_source,\n",
    "    y=df_source_labels,\n",
    "    session_list=train,\n",
    "    batch_size=batch_size,\n",
    "    event_count=event_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator = dataset_generator(\n",
    "    X=df_source,\n",
    "    y=df_source_labels,\n",
    "    session_list=val,\n",
    "    batch_size=batch_size,\n",
    "    event_count=event_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = dataset_generator(\n",
    "    X=df_source,\n",
    "    y=df_source_labels,\n",
    "    session_list=test,\n",
    "    batch_size=batch_size,\n",
    "    event_count=event_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history: callbacks.History) -> None:\n",
    "    \"\"\"\n",
    "    Plot the loss and validation loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    history : keras.callbacks.History\n",
    "        The history of the model training.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(epochs, history.history['loss'])\n",
    "    \n",
    "    if ('val_loss' in history.history):\n",
    "        plt.plot(epochs, history.history['val_loss'])\n",
    "        plt.legend(['Training loss', 'Validation loss'], loc='upper left')\n",
    "        plt.title('Training and validation loss')\n",
    "    else:\n",
    "        plt.title('Training loss')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(history: callbacks.History) -> None:\n",
    "    \"\"\"\n",
    "    Plot the accuracy and validation accuracy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    history : keras.callbacks.History\n",
    "        The history of the model training.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(epochs, history.history['accuracy'])\n",
    "\n",
    "    if ('val_accuracy' in history.history):\n",
    "        plt.plot(epochs, history.history['val_accuracy'])\n",
    "        plt.legend(['Training acc', 'Validation acc'], loc='upper left')\n",
    "        plt.title('Training and validation accuracy')\n",
    "    else:\n",
    "        plt.title('Training accuracy')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "input_shape = (event_count, len(vector_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(layers.Flatten(input_shape=input_shape))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizers.RMSprop(learning_rate=learning_rate),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('batch_size:', batch_size)\n",
    "print('train_length:', len(train))\n",
    "print('train_steps:', len(train) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train) // batch_size,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)\n",
    "plot_accuracy(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
