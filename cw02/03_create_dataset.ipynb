{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Create Dataset\n",
    "\n",
    "Optimize the time to create the dataset as it is unreasonable at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-25 08:03:47.692446: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-25 08:03:48.553043: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-25 08:03:48.553137: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-25 08:03:48.553144: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from pqdm.threads import pqdm\n",
    "from typing import Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13174211, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>event_name</th>\n",
       "      <th>name</th>\n",
       "      <th>level</th>\n",
       "      <th>page</th>\n",
       "      <th>room_coor_x</th>\n",
       "      <th>room_coor_y</th>\n",
       "      <th>screen_coor_x</th>\n",
       "      <th>screen_coor_y</th>\n",
       "      <th>hover_duration</th>\n",
       "      <th>text</th>\n",
       "      <th>fqid</th>\n",
       "      <th>room_fqid</th>\n",
       "      <th>text_fqid</th>\n",
       "      <th>fullscreen</th>\n",
       "      <th>hq</th>\n",
       "      <th>music</th>\n",
       "      <th>level_group</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>0</td>\n",
       "      <td>cutscene_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>undefined</td>\n",
       "      <td>intro</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.intro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>1323</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Whatcha doing over there, Jo?</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>831</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just talking to Teddy.</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              session_id  elapsed_time      event_name   name  level  page  \\\n",
       "index                                                                        \n",
       "0      20090312431273200             0  cutscene_click  basic      0   NaN   \n",
       "1      20090312431273200          1323    person_click  basic      0   NaN   \n",
       "2      20090312431273200           831    person_click  basic      0   NaN   \n",
       "\n",
       "       room_coor_x  room_coor_y  screen_coor_x  screen_coor_y  hover_duration  \\\n",
       "index                                                                           \n",
       "0      -413.991405  -159.314686          380.0          494.0             NaN   \n",
       "1      -413.991405  -159.314686          380.0          494.0             NaN   \n",
       "2      -413.991405  -159.314686          380.0          494.0             NaN   \n",
       "\n",
       "                                text    fqid                       room_fqid  \\\n",
       "index                                                                          \n",
       "0                          undefined   intro  tunic.historicalsociety.closet   \n",
       "1      Whatcha doing over there, Jo?  gramps  tunic.historicalsociety.closet   \n",
       "2             Just talking to Teddy.  gramps  tunic.historicalsociety.closet   \n",
       "\n",
       "                                               text_fqid  fullscreen  hq  \\\n",
       "index                                                                      \n",
       "0                   tunic.historicalsociety.closet.intro         NaN NaN   \n",
       "1      tunic.historicalsociety.closet.gramps.intro_0_...         NaN NaN   \n",
       "2      tunic.historicalsociety.closet.gramps.intro_0_...         NaN NaN   \n",
       "\n",
       "       music level_group  \n",
       "index                     \n",
       "0        NaN         0-4  \n",
       "1        NaN         0-4  \n",
       "2        NaN         0-4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the source training set\n",
    "df_source = pd.read_csv('data/train.csv.gz', compression='gzip', index_col=1)\n",
    "\n",
    "print(df_source.shape)\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_source.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(212022, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200_q1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312433251036_q1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090314121766812_q1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             session_id  correct\n",
       "0  20090312431273200_q1        1\n",
       "1  20090312433251036_q1        0\n",
       "2  20090314121766812_q1        1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the source training labels\n",
    "df_source_labels = pd.read_csv('data/train_labels.csv')\n",
    "\n",
    "print(df_source_labels.shape)\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_source_labels.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-defined Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_columns = ['elapsed_time', 'room_coor_x', 'room_coor_y', 'screen_coor_x', 'screen_coor_y', 'event_name_checkpoint', 'event_name_cutscene_click', 'event_name_map_click', 'event_name_map_hover', 'event_name_navigate_click', 'event_name_notebook_click', 'event_name_notification_click', 'event_name_object_click', 'event_name_object_hover', 'event_name_observation_click', 'event_name_person_click', 'name_basic', 'name_close', 'name_next', 'name_open', 'name_prev', 'name_undefined', 'level_0', 'level_1', 'level_2', 'level_3', 'level_4', 'level_5', 'level_6', 'level_7', 'level_8', 'level_9', 'level_10', 'level_11', 'level_12', 'level_13', 'level_14', 'level_15', 'level_16', 'level_17', 'level_18', 'level_19', 'level_20', 'level_21', 'level_22', 'fqid_0', 'fqid_archivist', 'fqid_archivist_glasses', 'fqid_block', 'fqid_block_0', 'fqid_block_1', 'fqid_block_badge', 'fqid_block_badge_2', 'fqid_block_magnify', 'fqid_block_nelson', 'fqid_block_tocollection', 'fqid_block_tomap1', 'fqid_block_tomap2', 'fqid_boss', 'fqid_businesscards', 'fqid_businesscards.card_0.next', 'fqid_businesscards.card_1.next', 'fqid_businesscards.card_bingo.bingo', 'fqid_businesscards.card_bingo.next', 'fqid_ch3start', 'fqid_chap1_finale', 'fqid_chap1_finale_c', 'fqid_chap2_finale_c', 'fqid_chap4_finale_c', 'fqid_coffee', 'fqid_colorbook', 'fqid_confrontation', 'fqid_crane_ranger', 'fqid_cs', 'fqid_directory', 'fqid_directory.closeup.archivist', 'fqid_door_block_clean', 'fqid_door_block_talk', 'fqid_doorblock', 'fqid_expert', 'fqid_flag_girl', 'fqid_fox', 'fqid_glasses', 'fqid_gramps', 'fqid_groupconvo', 'fqid_groupconvo_flag', 'fqid_intro', 'fqid_janitor', 'fqid_journals', 'fqid_journals.hub.topics', 'fqid_journals.pic_0.next', 'fqid_journals.pic_1.next', 'fqid_journals.pic_2.bingo', 'fqid_journals.pic_2.next', 'fqid_journals_flag', 'fqid_journals_flag.hub.topics', 'fqid_journals_flag.hub.topics_old', 'fqid_journals_flag.pic_0.bingo', 'fqid_journals_flag.pic_0.next', 'fqid_journals_flag.pic_0_old.next', 'fqid_journals_flag.pic_1.bingo', 'fqid_journals_flag.pic_1.next', 'fqid_journals_flag.pic_1_old.next', 'fqid_journals_flag.pic_2.bingo', 'fqid_journals_flag.pic_2.next', 'fqid_journals_flag.pic_2_old.next', 'fqid_key', 'fqid_lockeddoor', 'fqid_logbook', 'fqid_logbook.page.bingo', 'fqid_magnify', 'fqid_need_glasses', 'fqid_notebook', 'fqid_outtolunch', 'fqid_photo', 'fqid_plaque', 'fqid_plaque.face.date', 'fqid_reader', 'fqid_reader.paper0.next', 'fqid_reader.paper0.prev', 'fqid_reader.paper1.next', 'fqid_reader.paper1.prev', 'fqid_reader.paper2.bingo', 'fqid_reader.paper2.next', 'fqid_reader.paper2.prev', 'fqid_reader_flag', 'fqid_reader_flag.paper0.next', 'fqid_reader_flag.paper0.prev', 'fqid_reader_flag.paper1.next', 'fqid_reader_flag.paper1.prev', 'fqid_reader_flag.paper2.bingo', 'fqid_reader_flag.paper2.next', 'fqid_reader_flag.paper2.prev', 'fqid_remove_cup', 'fqid_report', 'fqid_retirement_letter', 'fqid_savedteddy', 'fqid_seescratches', 'fqid_teddy', 'fqid_tobasement', 'fqid_tocage', 'fqid_tocloset', 'fqid_tocloset_dirty', 'fqid_tocollection', 'fqid_tocollectionflag', 'fqid_toentry', 'fqid_tofrontdesk', 'fqid_togrampa', 'fqid_tohallway', 'fqid_tomap', 'fqid_tomicrofiche', 'fqid_tostacks', 'fqid_tracks', 'fqid_tracks.hub.deer', 'fqid_trigger_coffee', 'fqid_trigger_scarf', 'fqid_tunic', 'fqid_tunic.capitol_0', 'fqid_tunic.capitol_1', 'fqid_tunic.capitol_2', 'fqid_tunic.drycleaner', 'fqid_tunic.flaghouse', 'fqid_tunic.historicalsociety', 'fqid_tunic.hub.slip', 'fqid_tunic.humanecology', 'fqid_tunic.kohlcenter', 'fqid_tunic.library', 'fqid_tunic.wildlife', 'fqid_unlockdoor', 'fqid_wells', 'fqid_wellsbadge', 'fqid_what_happened', 'fqid_worker', 'room_fqid_tunic.capitol_0.hall', 'room_fqid_tunic.capitol_1.hall', 'room_fqid_tunic.capitol_2.hall', 'room_fqid_tunic.drycleaner.frontdesk', 'room_fqid_tunic.flaghouse.entry', 'room_fqid_tunic.historicalsociety.basement', 'room_fqid_tunic.historicalsociety.cage', 'room_fqid_tunic.historicalsociety.closet', 'room_fqid_tunic.historicalsociety.closet_dirty', 'room_fqid_tunic.historicalsociety.collection', 'room_fqid_tunic.historicalsociety.collection_flag', 'room_fqid_tunic.historicalsociety.entry', 'room_fqid_tunic.historicalsociety.frontdesk', 'room_fqid_tunic.historicalsociety.stacks', 'room_fqid_tunic.humanecology.frontdesk', 'room_fqid_tunic.kohlcenter.halloffame', 'room_fqid_tunic.library.frontdesk', 'room_fqid_tunic.library.microfiche', 'room_fqid_tunic.wildlife.center', 'text_fqid_0', 'text_fqid_tunic.capitol_0.hall.boss.talktogramps', 'text_fqid_tunic.capitol_0.hall.chap1_finale_c', 'text_fqid_tunic.capitol_1.hall.boss.haveyougotit', 'text_fqid_tunic.capitol_1.hall.boss.writeitup', 'text_fqid_tunic.capitol_1.hall.chap2_finale_c', 'text_fqid_tunic.capitol_2.hall.boss.haveyougotit', 'text_fqid_tunic.capitol_2.hall.chap4_finale_c', 'text_fqid_tunic.drycleaner.frontdesk.block_0', 'text_fqid_tunic.drycleaner.frontdesk.block_1', 'text_fqid_tunic.drycleaner.frontdesk.logbook.page.bingo', 'text_fqid_tunic.drycleaner.frontdesk.worker.done', 'text_fqid_tunic.drycleaner.frontdesk.worker.done2', 'text_fqid_tunic.drycleaner.frontdesk.worker.hub', 'text_fqid_tunic.drycleaner.frontdesk.worker.takealook', 'text_fqid_tunic.flaghouse.entry.colorbook', 'text_fqid_tunic.flaghouse.entry.flag_girl.hello', 'text_fqid_tunic.flaghouse.entry.flag_girl.hello_recap', 'text_fqid_tunic.flaghouse.entry.flag_girl.symbol', 'text_fqid_tunic.flaghouse.entry.flag_girl.symbol_recap', 'text_fqid_tunic.historicalsociety.basement.ch3start', 'text_fqid_tunic.historicalsociety.basement.gramps.seeyalater', 'text_fqid_tunic.historicalsociety.basement.gramps.whatdo', 'text_fqid_tunic.historicalsociety.basement.janitor', 'text_fqid_tunic.historicalsociety.basement.savedteddy', 'text_fqid_tunic.historicalsociety.basement.seescratches', 'text_fqid_tunic.historicalsociety.cage.confrontation', 'text_fqid_tunic.historicalsociety.cage.glasses.afterteddy', 'text_fqid_tunic.historicalsociety.cage.glasses.beforeteddy', 'text_fqid_tunic.historicalsociety.cage.lockeddoor', 'text_fqid_tunic.historicalsociety.cage.need_glasses', 'text_fqid_tunic.historicalsociety.cage.teddy.trapped', 'text_fqid_tunic.historicalsociety.cage.unlockdoor', 'text_fqid_tunic.historicalsociety.closet.doorblock', 'text_fqid_tunic.historicalsociety.closet.gramps.intro_0_cs_0', 'text_fqid_tunic.historicalsociety.closet.intro', 'text_fqid_tunic.historicalsociety.closet.notebook', 'text_fqid_tunic.historicalsociety.closet.photo', 'text_fqid_tunic.historicalsociety.closet.retirement_letter.hub', 'text_fqid_tunic.historicalsociety.closet.teddy.intro_0_cs_0', 'text_fqid_tunic.historicalsociety.closet.teddy.intro_0_cs_5', 'text_fqid_tunic.historicalsociety.closet_dirty.door_block_clean', 'text_fqid_tunic.historicalsociety.closet_dirty.door_block_talk', 'text_fqid_tunic.historicalsociety.closet_dirty.gramps.archivist', 'text_fqid_tunic.historicalsociety.closet_dirty.gramps.helpclean', 'text_fqid_tunic.historicalsociety.closet_dirty.gramps.news', 'text_fqid_tunic.historicalsociety.closet_dirty.gramps.nothing', 'text_fqid_tunic.historicalsociety.closet_dirty.photo', 'text_fqid_tunic.historicalsociety.closet_dirty.trigger_coffee', 'text_fqid_tunic.historicalsociety.closet_dirty.trigger_scarf', 'text_fqid_tunic.historicalsociety.closet_dirty.what_happened', 'text_fqid_tunic.historicalsociety.collection.cs', 'text_fqid_tunic.historicalsociety.collection.gramps.found', 'text_fqid_tunic.historicalsociety.collection.gramps.look_0', 'text_fqid_tunic.historicalsociety.collection.gramps.lost', 'text_fqid_tunic.historicalsociety.collection.tunic', 'text_fqid_tunic.historicalsociety.collection.tunic.slip', 'text_fqid_tunic.historicalsociety.collection_flag.gramps.flag', 'text_fqid_tunic.historicalsociety.collection_flag.gramps.recap', 'text_fqid_tunic.historicalsociety.entry.block_tocollection', 'text_fqid_tunic.historicalsociety.entry.block_tomap1', 'text_fqid_tunic.historicalsociety.entry.block_tomap2', 'text_fqid_tunic.historicalsociety.entry.boss.flag', 'text_fqid_tunic.historicalsociety.entry.boss.flag_recap', 'text_fqid_tunic.historicalsociety.entry.boss.talktogramps', 'text_fqid_tunic.historicalsociety.entry.directory.closeup.archivist', 'text_fqid_tunic.historicalsociety.entry.gramps.hub', 'text_fqid_tunic.historicalsociety.entry.groupconvo', 'text_fqid_tunic.historicalsociety.entry.groupconvo_flag', 'text_fqid_tunic.historicalsociety.entry.wells.flag', 'text_fqid_tunic.historicalsociety.entry.wells.flag_recap', 'text_fqid_tunic.historicalsociety.entry.wells.talktogramps', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.foundtheodora', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.have_glass', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.have_glass_recap', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.hello', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.need_glass_0', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.need_glass_1', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.newspaper', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.newspaper_recap', 'text_fqid_tunic.historicalsociety.frontdesk.archivist_glasses.confrontation', 'text_fqid_tunic.historicalsociety.frontdesk.archivist_glasses.confrontation_recap', 'text_fqid_tunic.historicalsociety.frontdesk.block_magnify', 'text_fqid_tunic.historicalsociety.frontdesk.key', 'text_fqid_tunic.historicalsociety.frontdesk.magnify', 'text_fqid_tunic.historicalsociety.stacks.block', 'text_fqid_tunic.historicalsociety.stacks.journals.pic_2.bingo', 'text_fqid_tunic.historicalsociety.stacks.journals_flag.pic_0.bingo', 'text_fqid_tunic.historicalsociety.stacks.journals_flag.pic_1.bingo', 'text_fqid_tunic.historicalsociety.stacks.journals_flag.pic_2.bingo', 'text_fqid_tunic.historicalsociety.stacks.outtolunch', 'text_fqid_tunic.humanecology.frontdesk.block_0', 'text_fqid_tunic.humanecology.frontdesk.block_1', 'text_fqid_tunic.humanecology.frontdesk.businesscards.card_bingo.bingo', 'text_fqid_tunic.humanecology.frontdesk.worker.badger', 'text_fqid_tunic.humanecology.frontdesk.worker.intro', 'text_fqid_tunic.kohlcenter.halloffame.block_0', 'text_fqid_tunic.kohlcenter.halloffame.plaque.face.date', 'text_fqid_tunic.kohlcenter.halloffame.togrampa', 'text_fqid_tunic.library.frontdesk.block_badge', 'text_fqid_tunic.library.frontdesk.block_badge_2', 'text_fqid_tunic.library.frontdesk.block_nelson', 'text_fqid_tunic.library.frontdesk.wellsbadge.hub', 'text_fqid_tunic.library.frontdesk.worker.droppedbadge', 'text_fqid_tunic.library.frontdesk.worker.flag', 'text_fqid_tunic.library.frontdesk.worker.flag_recap', 'text_fqid_tunic.library.frontdesk.worker.hello', 'text_fqid_tunic.library.frontdesk.worker.hello_short', 'text_fqid_tunic.library.frontdesk.worker.nelson', 'text_fqid_tunic.library.frontdesk.worker.nelson_recap', 'text_fqid_tunic.library.frontdesk.worker.preflag', 'text_fqid_tunic.library.frontdesk.worker.wells', 'text_fqid_tunic.library.frontdesk.worker.wells_recap', 'text_fqid_tunic.library.microfiche.block_0', 'text_fqid_tunic.library.microfiche.reader.paper2.bingo', 'text_fqid_tunic.library.microfiche.reader_flag.paper2.bingo', 'text_fqid_tunic.wildlife.center.coffee', 'text_fqid_tunic.wildlife.center.crane_ranger.crane', 'text_fqid_tunic.wildlife.center.expert.recap', 'text_fqid_tunic.wildlife.center.expert.removed_cup', 'text_fqid_tunic.wildlife.center.fox.concern', 'text_fqid_tunic.wildlife.center.remove_cup', 'text_fqid_tunic.wildlife.center.tracks.hub.deer', 'text_fqid_tunic.wildlife.center.wells.animals', 'text_fqid_tunic.wildlife.center.wells.animals2', 'text_fqid_tunic.wildlife.center.wells.nodeer', 'text_fqid_tunic.wildlife.center.wells.nodeer_recap', 'level_group_0-4', 'level_group_13-22', 'level_group_5-12', \n",
    "'question_1',\n",
    "'question_2',\n",
    "'question_3',\n",
    "'question_4',\n",
    "'question_5',\n",
    "'question_6',\n",
    "'question_7',\n",
    "'question_8',\n",
    "'question_9',\n",
    "'question_10',\n",
    "'question_11',\n",
    "'question_12',\n",
    "'question_13',\n",
    "'question_14',\n",
    "'question_15',\n",
    "'question_16',\n",
    "'question_17',\n",
    "'question_18',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_question_to_level_group(question_number):\n",
    "    \"\"\"\n",
    "    Maps the question number to the level group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question_number : int\n",
    "        The question number.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The level group.\n",
    "    \"\"\"\n",
    "    if question_number in [1, 2, 3]:\n",
    "        return '0-4'\n",
    "    elif question_number in [4, 5, 6, 7, 8, 9, 10, 11, 12, 13]:\n",
    "        return '5-12'\n",
    "    elif question_number in [14, 15, 16, 17, 18]:\n",
    "        return '13-22'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def prepare_label_dataset(data : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the label dataset and add columns for the level group \n",
    "    and the question number.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The label dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The prepared label dataset.\n",
    "    \"\"\"\n",
    "    # add the columns to determine the level group\n",
    "    df_labels = data \\\n",
    "        .rename(columns={'session_id': 'id'}) \\\n",
    "        .assign(session_id=lambda df: df['id'].str.split('_').str[0].astype(int)) \\\n",
    "        .assign(question_id=lambda df: df['id'].str.split('_').str[1]) \\\n",
    "        .assign(question_num=lambda df: df['question_id'].str[1:].astype(int)) \\\n",
    "        [['session_id', 'question_num', 'correct']]\n",
    "    \n",
    "    # add the level group column\n",
    "    df_labels['level_group'] = df_labels['question_num'].apply(map_question_to_level_group) \n",
    "\n",
    "        \n",
    "    return df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_main_dataset(data : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the main dataset by removing duplicates and removing \n",
    "    columns that are not needed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The main dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The prepared main dataset.\n",
    "    \"\"\"\n",
    "    empty_columns = ['fullscreen', 'hq', 'music', 'page', 'hover_duration']\n",
    "\n",
    "    df_main = data \\\n",
    "        .drop_duplicates() \\\n",
    "        .reset_index(drop=True) \\\n",
    "        .drop(empty_columns, axis=1) \\\n",
    "        .drop('text', axis=1)\n",
    "\n",
    "    return df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_dataset(data: pd.DataFrame, standardize_coordinates: bool=True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Vectorizes the dataset for deep learning by one-hot encoding and standardizing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The dataset to prepare.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The vectorized dataset.\n",
    "    \"\"\"\n",
    "    categorical_cols = ['event_name', 'name', 'level', 'fqid', 'room_fqid', 'text_fqid', 'level_group']\n",
    "    numerical_cols = ['elapsed_time']    \n",
    "    coordinates_cols = ['room_coor_x', 'room_coor_y', 'screen_coor_x', 'screen_coor_y']\n",
    "\n",
    "    df_vectorized = data \\\n",
    "        .drop('session_id', axis=1) \\\n",
    "        .fillna(0)\n",
    "\n",
    "    # standardize the numerical variables\n",
    "    df_vectorized[numerical_cols] = (df_vectorized[numerical_cols] - df_vectorized[numerical_cols].mean()) / df_vectorized[numerical_cols].std()\n",
    "\n",
    "    # standardize the coordinates\n",
    "    if standardize_coordinates:\n",
    "        df_vectorized[coordinates_cols] = (df_vectorized[coordinates_cols] - df_vectorized[coordinates_cols].mean()) / df_vectorized[coordinates_cols].std()\n",
    "\n",
    "    # one-hot encode the categorical variables\n",
    "    df_vectorized = pd.get_dummies(df_vectorized, columns=categorical_cols)\n",
    "    \n",
    "    return df_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_matrix(data: pd.DataFrame, vector_columns: list, standardize_coordinates: bool=True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare a sequence matrix from a DataFrame for a specific session and level group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The dataframe containing the data.\n",
    "\n",
    "    vector_columns : list\n",
    "        The columns that should appear in the sequence matrix.\n",
    "\n",
    "    standardize_coordinates : bool, optional\n",
    "        Whether to standardize the coordinates.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The sequence matrix.\n",
    "    \"\"\"\n",
    "    df_sequence_matrix = vectorize_dataset(data, standardize_coordinates=standardize_coordinates)\n",
    "\n",
    "    # add the missing columns\n",
    "    missing_columns = [column for column in vector_columns if column not in df_sequence_matrix.columns]\n",
    "    df_sequence_matrix = pd.concat([\n",
    "        df_sequence_matrix, \n",
    "        pd.DataFrame(columns=missing_columns)], axis=1).fillna(0)\n",
    "\n",
    "    return df_sequence_matrix[vector_columns]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_array(X: pd.DataFrame, \n",
    "                        session_id: int,\n",
    "                        question_number: int,\n",
    "                        event_count:int = 1000) -> np.array:\n",
    "    \"\"\"\n",
    "    Creates a vector array for a specific session and question number.\n",
    "    \"\"\"\n",
    "    # get the level group\n",
    "    level_group = map_question_to_level_group(question_number)\n",
    "\n",
    "    # get the data for the session and level group\n",
    "    df_session = X.query('session_id == @session_id and level_group == @level_group')\n",
    "\n",
    "    # prepare the sequence matrix\n",
    "    df_sequence_matrix = prepare_sequence_matrix(df_session, vector_columns)\n",
    "\n",
    "    # set the question number value\n",
    "    df_sequence_matrix[f'question_{question_number}'] = 1\n",
    "\n",
    "    # convert it to a numpy array\n",
    "    vector_array = df_sequence_matrix.to_numpy()\n",
    "\n",
    "    # the array cannot have more events than the event count\n",
    "    vector_array = vector_array[:event_count]\n",
    "\n",
    "    # pad the array with zeros if it has less events than the event count\n",
    "    if vector_array.shape[0] < event_count:\n",
    "        vector_array = np.pad(vector_array, ((0, event_count - vector_array.shape[0]), (0, 0)), 'constant')\n",
    "\n",
    "    return vector_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd97fb8cb1d4f2a8f521118ddc95898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_dataset(X: pd.DataFrame,\n",
    "                   y: pd.DataFrame,\n",
    "                   session_list: list,\n",
    "                   event_count:int = 1000) -> Tuple[np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Creates a dataset for a specific set of sessions and question numbers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        The main dataset.\n",
    "\n",
    "    y : pd.DataFrame\n",
    "        The label dataset.\n",
    "\n",
    "    session_ids : list\n",
    "        The list of session ids.\n",
    "\n",
    "    event_count : int, optional\n",
    "        The number of events to include in the dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.array, np.array]\n",
    "        The dataset and the labels.\n",
    "    \"\"\"\n",
    "    X_dataset = []\n",
    "    y_dataset = []\n",
    "\n",
    "    def create_session_dataset(session_id: str) -> Tuple[np.array, np.array]:\n",
    "        X_dataset = []\n",
    "        y_dataset = []\n",
    "\n",
    "        # get the session labels\n",
    "        df_session_labels = y.query('session_id == @session_id')\n",
    "\n",
    "        # iterate over all the questions answered in the session\n",
    "        for _, row in df_session_labels.iterrows():\n",
    "            question_number = row['question_num']\n",
    "            correct = row['correct']\n",
    "\n",
    "            # get the vector array\n",
    "            vector_array = create_vector_array(X, session_id, question_number, event_count)\n",
    "\n",
    "            # add the vector array to the dataset\n",
    "            X_dataset.append(vector_array)\n",
    "\n",
    "            # add the label to the dataset\n",
    "            y_dataset.append(correct)\n",
    "\n",
    "\n",
    "    #result = pqdm(session_list, create_session_dataset, n_jobs=50)\n",
    "\n",
    "    for session_id in tqdm(session_list):\n",
    "        # get the session labels\n",
    "        df_session_labels = y.query('session_id == @session_id')\n",
    "\n",
    "        # iterate over all the questions answered in the session\n",
    "        for _, row in df_session_labels.iterrows():\n",
    "            question_number = row['question_num']\n",
    "            correct = row['correct']\n",
    "\n",
    "            # get the vector array\n",
    "            vector_array = create_vector_array(X, session_id, question_number, event_count)\n",
    "\n",
    "            # add the vector array to the dataset\n",
    "            X_dataset.append(vector_array)\n",
    "\n",
    "            # add the label to the dataset\n",
    "            y_dataset.append(correct)\n",
    "\n",
    "    return np.array(X_dataset), np.array(y_dataset)\n",
    "\n",
    "X_train, y_train = create_dataset(X=df_source, y=df_source_labels, session_list=train[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13173445, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>event_name</th>\n",
       "      <th>name</th>\n",
       "      <th>level</th>\n",
       "      <th>room_coor_x</th>\n",
       "      <th>room_coor_y</th>\n",
       "      <th>screen_coor_x</th>\n",
       "      <th>screen_coor_y</th>\n",
       "      <th>fqid</th>\n",
       "      <th>room_fqid</th>\n",
       "      <th>text_fqid</th>\n",
       "      <th>level_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>0</td>\n",
       "      <td>cutscene_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>intro</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.intro</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>1323</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>831</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          session_id  elapsed_time      event_name   name  level  room_coor_x  \\\n",
       "0  20090312431273200             0  cutscene_click  basic      0  -413.991405   \n",
       "1  20090312431273200          1323    person_click  basic      0  -413.991405   \n",
       "2  20090312431273200           831    person_click  basic      0  -413.991405   \n",
       "\n",
       "   room_coor_y  screen_coor_x  screen_coor_y    fqid  \\\n",
       "0  -159.314686          380.0          494.0   intro   \n",
       "1  -159.314686          380.0          494.0  gramps   \n",
       "2  -159.314686          380.0          494.0  gramps   \n",
       "\n",
       "                        room_fqid  \\\n",
       "0  tunic.historicalsociety.closet   \n",
       "1  tunic.historicalsociety.closet   \n",
       "2  tunic.historicalsociety.closet   \n",
       "\n",
       "                                           text_fqid level_group  \n",
       "0               tunic.historicalsociety.closet.intro         0-4  \n",
       "1  tunic.historicalsociety.closet.gramps.intro_0_...         0-4  \n",
       "2  tunic.historicalsociety.closet.gramps.intro_0_...         0-4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare the main dataset\n",
    "df_source = prepare_main_dataset(df_source)\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    print(df_source.shape)\n",
    "    display(df_source.head(3))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>question_num</th>\n",
       "      <th>correct</th>\n",
       "      <th>level_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21476</th>\n",
       "      <td>22010116250792520</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84068</th>\n",
       "      <td>21000111433937450</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171219</th>\n",
       "      <td>21040510125933256</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>13-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               session_id  question_num  correct level_group\n",
       "21476   22010116250792520             2        1         0-4\n",
       "84068   21000111433937450             8        1        5-12\n",
       "171219  21040510125933256            15        0       13-22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare the label dataset\n",
    "df_source_labels = prepare_label_dataset(df_source_labels)\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_source_labels.sample(n=3, random_state=51))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 8245\n",
      "Validation: 1767\n",
      "Test: 1767\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into train, validation and test sets\n",
    "train, test = train_test_split(df_source_labels['session_id'].unique(), test_size=0.3)\n",
    "test, val = train_test_split(test, test_size=0.5)\n",
    "\n",
    "# print the number of sessions in each set\n",
    "print(f'Train: {len(train)}')\n",
    "print(f'Validation: {len(val)}')\n",
    "print(f'Test: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training dataset\n",
    "#X_train, y_train = create_dataset(X=df_source, y=df_source_labels, session_list=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the validation dataset\n",
    "#X_train, y_train = create_dataset(X=df_source, y=df_source_labels, session_list=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-25 08:04:43.594034: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-25 08:04:43.718289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-25 08:04:43.718466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-25 08:04:43.719565: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-25 08:04:43.720813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-25 08:04:43.720958: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-25 08:04:43.721299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-25 08:04:44.492948: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-25 08:04:44.493607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-25 08:04:44.493748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-25 08:04:44.493873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9893 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:29:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lookback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_669/3119428465.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# define the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlookback\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lookback' is not defined"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(layers.Flatten(input_shape=(lookback // step, float_data.shape[-1])))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
