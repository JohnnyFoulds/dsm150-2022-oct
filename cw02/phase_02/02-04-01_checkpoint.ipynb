{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02-04-01 : Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/jo_wilder/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-03-18 12:18:42.021090: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "from typing import Iterable\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import jo_wilder\n",
    "\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_question_to_level_group(question_number):\n",
    "    \"\"\"\n",
    "    Maps the question number to the level group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question_number : int\n",
    "        The question number.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The level group.\n",
    "    \"\"\"\n",
    "    if question_number in [1, 2, 3]:\n",
    "        return '0-4'\n",
    "    elif question_number in [4, 5, 6, 7, 8, 9, 10, 11, 12, 13]:\n",
    "        return '5-12'\n",
    "    elif question_number in [14, 15, 16, 17, 18]:\n",
    "        return '13-22'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_label_dataset(data : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the label dataset and add columns for the level group \n",
    "    and the question number.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The label dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The prepared label dataset.\n",
    "    \"\"\"\n",
    "    # add the columns to determine the level group\n",
    "    df_labels = data \\\n",
    "        .rename(columns={'session_id': 'id'}) \\\n",
    "        .assign(session_id=lambda df: df['id'].str.split('_').str[0].astype(int)) \\\n",
    "        .assign(question_id=lambda df: df['id'].str.split('_').str[1]) \\\n",
    "        .assign(question_num=lambda df: df['question_id'].str[1:].astype(int)) \\\n",
    "        [['session_id', 'question_num', 'correct']]\n",
    "    \n",
    "    # add the level group column\n",
    "    df_labels['level_group'] = df_labels['question_num'].apply(map_question_to_level_group) \n",
    "        \n",
    "    return df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_main_dataset(data : pd.DataFrame,\n",
    "                         elapsed_time_min_clip:int=0,\n",
    "                         elapsed_time_max_clip:int=3691298) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the main dataset by removing duplicates and removing \n",
    "    columns that are not needed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The main dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The prepared main dataset.\n",
    "    \"\"\"\n",
    "    empty_columns = ['fullscreen', 'hq', 'music', 'page', 'hover_duration']\n",
    "\n",
    "    df_main = data \\\n",
    "        .drop_duplicates() \\\n",
    "        .reset_index(drop=True) \\\n",
    "        .drop(empty_columns, axis=1) \\\n",
    "        .drop('text', axis=1)\n",
    "    \n",
    "    # clip the elapsed time to remove outliers\n",
    "    df_main['elapsed_time'] = df_main['elapsed_time'].clip(\n",
    "        lower=elapsed_time_min_clip,\n",
    "        upper=elapsed_time_max_clip)\n",
    "    \n",
    "    return df_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_initial_features(X:pd.DataFrame,\n",
    "                            y:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates the initial dataset to which additional features will be added.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        The main dataset.\n",
    "    y : pd.DataFrame\n",
    "        The label dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The initial feature dataset.\n",
    "    \"\"\"\n",
    "    df_features =  y \\\n",
    "        .groupby(['session_id', 'level_group']) \\\n",
    "        .agg({'correct': ['count']}) \\\n",
    "        .reset_index() \\\n",
    "        .droplevel(1, axis=1) \\\n",
    "        .drop(columns=['correct']) \\\n",
    "        .sort_values(['session_id', 'level_group']) \\\n",
    "        \n",
    "    # set the session_id to be an integer\n",
    "    df_features['session_id'] = df_features['session_id'].astype(int)\n",
    "        \n",
    "    return df_features        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_numeric_column_features(features:pd.DataFrame,\n",
    "                                X:pd.DataFrame,\n",
    "                                column:str,\n",
    "                                min_values:dict=None,\n",
    "                                max_values:dict=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add the maximum elapsed time feature to the features dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : pd.DataFrame\n",
    "        The features dataset.\n",
    "    X : pd.DataFrame\n",
    "        The main dataset.\n",
    "    column : str\n",
    "        The name of the numeric column to add to the features for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Define a function to calculate mode\n",
    "    def mode(series):\n",
    "        return series.mode().iat[0]\n",
    "\n",
    "    # calculate the maximum, minimum and mean for the column\n",
    "    df_result = X \\\n",
    "        .groupby(['session_id', 'level_group']) \\\n",
    "        .agg({column: ['sum', 'max', 'min', 'mean', mode]}) \\\n",
    "        .reset_index()\n",
    "    \n",
    "    # flatten the multi-index columns\n",
    "    df_result.columns = ['_'.join(col).rstrip('_') for col in df_result.columns.values]\n",
    "\n",
    "    # normalize the values\n",
    "    if min_values is None or max_values is None:\n",
    "        logging.warning('Not normalizing the values, min_value and max_values are not set.')\n",
    "    else:\n",
    "        metric_list = ['sum', 'max', 'min', 'mean', 'mode']\n",
    "        for metric in metric_list:\n",
    "            current_column = f'{column}_{metric}'\n",
    "            df_result[current_column] = (df_result[current_column] - min_values[metric]) / (max_values[metric] - min_values[metric])       \n",
    "\n",
    "    # join the features to the result   \n",
    "    df_result = features.set_index(['session_id', 'level_group']) \\\n",
    "        .join(df_result.set_index(['session_id', 'level_group']), how='left') \\\n",
    "        .reset_index()\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df_source:pd.DataFrame,\n",
    "                        df_source_labels:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates the feature dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_source : pd.DataFrame\n",
    "        The main dataset.\n",
    "    df_source_labels : pd.DataFrame\n",
    "        The label dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The feature dataset.\n",
    "    \"\"\"\n",
    "    # create the initial features\n",
    "    df_features = create_initial_features(df_source, df_source_labels)\n",
    "\n",
    "    # add the feature to the features dataset\n",
    "    df_features = add_numeric_column_features(\n",
    "        features=df_features,\n",
    "        X=df_source,\n",
    "        column='elapsed_time',\n",
    "        min_values={\n",
    "            'sum': 61395.0,\n",
    "            'max':  990.0,\n",
    "            'min':  0.0,\n",
    "            'mean': 526.447,\n",
    "            'mode': 0.0},\n",
    "        max_values={\n",
    "            'sum':  9990648000,\n",
    "            'max':  3691298.0,\n",
    "            'min':  3691298.0,\n",
    "            'mean': 3691298.0,\n",
    "            'mode': 3691298.0})\n",
    "    \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_dataset(df_features:pd.DataFrame,\n",
    "                           df_source_labels:pd.DataFrame,\n",
    "                           session_list: list,\n",
    "                           feature_list:list,\n",
    "                           level_group:str=None,\n",
    "                           include_question:bool=True,\n",
    "                           expand_question:bool=False) -> np.array:\n",
    "    \"\"\"\n",
    "    Creates the feature dataset for the given level group and session list.\n",
    "    If the level group is not specified it will create the dataset for all level groups.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_features : pd.DataFrame\n",
    "        The dataset of prepared features (by session_id and level_group).\n",
    "    df_source_labels : pd.DataFrame\n",
    "        The dataset containing the training labels (y_True).\n",
    "    session_list : list\n",
    "        The list of session ids to create the dataset for.\n",
    "    level_group : str, optional\n",
    "        The level group to create the dataset for, by default None\n",
    "    feature_list : list\n",
    "        The list of features to include in the dataset.\n",
    "    include_question : bool, optional\n",
    "        Whether to include the question number in the dataset as the first set of\n",
    "        columns, by default True\n",
    "    expand_question : bool, optional\n",
    "        Whether to expand the question number into a one-hot vector to each item in the \n",
    "        case of a multi-dimensional feature, by default False\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        The feature dataset.\n",
    "    \"\"\"\n",
    "    # get the features and labels for the given level group\n",
    "    if level_group is None:\n",
    "        logging.info('Creating the dataset for all level groups')\n",
    "        df_features_group = df_features.query('session_id in @session_list')\n",
    "        df_labels_group = df_source_labels.query('session_id in @session_list')\n",
    "    else:\n",
    "        logging.info('Creating the dataset for level group: %s', level_group)\n",
    "        df_features_group = df_features.query('level_group == @level_group and session_id in @session_list')\n",
    "        df_labels_group = df_source_labels.query('level_group == @level_group and session_id in @session_list')\n",
    "\n",
    "    # sort the df_labels_group\n",
    "    df_labels_group = df_labels_group.sort_values(['session_id', 'question_num'])\n",
    "\n",
    "    feature_dataset = []\n",
    "\n",
    "    # get the features for each row in the level group labels dataset\n",
    "    current_session_id = None\n",
    "    df_session_features = None\n",
    "\n",
    "    for index, row in tqdm(df_labels_group.iterrows(), total=df_labels_group.shape[0]):        \n",
    "        session_id = int(row['session_id'])\n",
    "        session_level_group = row['level_group']\n",
    "        question_num = int(row['question_num'])\n",
    "\n",
    "        # get the features for the session\n",
    "        if session_id != current_session_id:\n",
    "            current_session_id = session_id\n",
    "            df_session_features = df_features_group.query('session_id == @session_id')\n",
    "\n",
    "        # get the level group features\n",
    "        df_level_group_features = df_session_features.query('level_group == @session_level_group')\n",
    "\n",
    "        # check if the session has features\n",
    "        if df_level_group_features.shape[0] == 0:\n",
    "            raise Exception(f'No features for session {session_id}, level group {session_level_group}!')\n",
    "                            \n",
    "        # get the features for the row\n",
    "        row_features = []\n",
    "\n",
    "        # get the question number one-hot encoded\n",
    "        question_num_one_hot = np.zeros(18, dtype=np.int8)\n",
    "        question_num_one_hot[question_num-1] = 1\n",
    "\n",
    "        if include_question:\n",
    "            row_features.extend(question_num_one_hot)\n",
    "\n",
    "        for feature in feature_list:\n",
    "            feature_value = df_level_group_features[feature].values[0]\n",
    "\n",
    "            # check if the feature value is iterable\n",
    "            if isinstance(feature_value, Iterable):\n",
    "                if expand_question:\n",
    "                    # reshape the question array to match the feature array shape\n",
    "                    question_reshaped = np.tile(\n",
    "                        question_num_one_hot, \n",
    "                        (feature_value.shape[0], 1))\n",
    "                    \n",
    "                    # add the question columns to the feature array\n",
    "                    feature_value = np.hstack((question_reshaped, feature_value))\n",
    "\n",
    "                row_features.extend(feature_value)\n",
    "            else:\n",
    "                row_features.append(feature_value)\n",
    "\n",
    "        # add the row features to the output dataset\n",
    "        feature_dataset.append(row_features)\n",
    "\n",
    "    return np.array(feature_dataset, dtype=np.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 23)]              0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 1024)              24576     \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,601\n",
      "Trainable params: 25,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-18 12:18:43.646411: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-18 12:18:43.647363: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model_path = '../checkpoints/'\n",
    "#model_path = '/kaggle/input/jw-02-04-91'\n",
    "\n",
    "model = keras.models.load_model(f'{model_path}/02-04-01.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = jo_wilder.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,\n",
    "            X: pd.DataFrame,\n",
    "            y: pd.DataFrame,\n",
    "            threshold:float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare the input dataset for prediction and return the predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object\n",
    "        The model to be used for prediction.\n",
    "    X : pd.DataFrame\n",
    "        The input dataset.\n",
    "    y : pd.DataFrame\n",
    "        The target dataset. Here the correct value should be updated\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The updated y dataframe with the predictions.\n",
    "    \"\"\"\n",
    "    feature_list = ['elapsed_time_sum', 'elapsed_time_max',\n",
    "                    'elapsed_time_min', 'elapsed_time_mean',\n",
    "                    'elapsed_time_mode']\n",
    "\n",
    "    # prepare the main dataset\n",
    "    df_source = prepare_main_dataset(X, elapsed_time_min_clip=0, elapsed_time_max_clip=3691298)\n",
    "\n",
    "    # prepare the label dataset\n",
    "    df_source_labels = prepare_label_dataset(y)\n",
    "\n",
    "    # perform the feature engineering\n",
    "    df_features = feature_engineering(df_source, df_source_labels)\n",
    "\n",
    "    # create the dataset to be used for prediction\n",
    "    X_pred = create_feature_dataset(\n",
    "        df_features=df_features,\n",
    "        df_source_labels=df_source_labels,\n",
    "        session_list=df_source_labels['session_id'].unique(),\n",
    "        feature_list=feature_list,\n",
    "        level_group=None,\n",
    "        include_question=True,\n",
    "        expand_question=False)\n",
    "    \n",
    "    # get the predictions\n",
    "    y_pred = (model.predict(X_pred) > threshold).astype(int)\n",
    "\n",
    "    # update the y dataframe with the predictions\n",
    "    df_pred = y \\\n",
    "        .copy() \\\n",
    "        .drop(columns=['correct']) \\\n",
    "        .assign(correct=y_pred)\n",
    "\n",
    "    # display(df_source)\n",
    "    # display(df_source_labels)\n",
    "    # display(df_features)\n",
    "    # display(X_pred)\n",
    "    # display(y_pred)\n",
    "    # display(df_pred)\n",
    "\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the submission.csv file if\n",
    "!rm submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 766.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 53ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 685.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 702.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 895.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 768.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 829.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 766.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 642.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 770.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for (df_label, df_data) in iter_test:\n",
    "    df_predicted = predict(\n",
    "        model=model,\n",
    "        X=df_data, \n",
    "        y=df_label,\n",
    "        threshold=0.47)\n",
    "    env.predict(df_predicted)\n",
    "    #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20090312331414616_q1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20090109393214576_q6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20090312143683264_q8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>20090312331414616_q6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>20090312331414616_q18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>20090312143683264_q16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>20090312331414616_q17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20090109393214576_q5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>20090109393214576_q14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>20090312331414616_q8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               session_id  correct\n",
       "6    20090312331414616_q1        1\n",
       "11   20090109393214576_q6        0\n",
       "23   20090312143683264_q8        0\n",
       "31   20090312331414616_q6        1\n",
       "53  20090312331414616_q18        1\n",
       "46  20090312143683264_q16        1\n",
       "52  20090312331414616_q17        1\n",
       "10   20090109393214576_q5        0\n",
       "39  20090109393214576_q14        1\n",
       "33   20090312331414616_q8        1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm submission file\n",
    "df = pd.read_csv('submission.csv')\n",
    "print(df.shape)\n",
    "df.sample(n=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
