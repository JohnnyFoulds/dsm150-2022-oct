{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03-101 : Mlflow Experiments\n",
    "\n",
    "### MLflow\n",
    "\n",
    "- [mlflow.keras](https://mlflow.org/docs/1.20.0/python_api/mlflow.keras.html)\n",
    "- [Keras Integration Example](https://github.com/mlflow/mlflow/blob/master/examples/keras/train.py)\n",
    "- [An Experiment Tracking Tutorial with Mlflow and Keras](https://www.youtube.com/watch?v=carXIinrmOc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras as k\n",
    "\n",
    "from competition import source_data as sd\n",
    "from competition import data_preparation as dp\n",
    "from competition import feature_engineering as fe\n",
    "from competition import model_data as md\n",
    "from competition import model_training as mt\n",
    "from competition import model_layers as ml\n",
    "from competition import model_definitions as mm\n",
    "\n",
    "import mlflow\n",
    "import mlflow.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Set the GPU memory for growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-22 18:43:53 INFO     Started\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        handlers=[\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ])\n",
    "\n",
    "logging.info(\"Started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13174211, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>index</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>event_name</th>\n",
       "      <th>name</th>\n",
       "      <th>level</th>\n",
       "      <th>page</th>\n",
       "      <th>room_coor_x</th>\n",
       "      <th>room_coor_y</th>\n",
       "      <th>screen_coor_x</th>\n",
       "      <th>screen_coor_y</th>\n",
       "      <th>hover_duration</th>\n",
       "      <th>text</th>\n",
       "      <th>fqid</th>\n",
       "      <th>room_fqid</th>\n",
       "      <th>text_fqid</th>\n",
       "      <th>fullscreen</th>\n",
       "      <th>hq</th>\n",
       "      <th>music</th>\n",
       "      <th>level_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cutscene_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991394</td>\n",
       "      <td>-159.314682</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>undefined</td>\n",
       "      <td>intro</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.intro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>1</td>\n",
       "      <td>1323</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991394</td>\n",
       "      <td>-159.314682</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Whatcha doing over there, Jo?</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>2</td>\n",
       "      <td>831</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991394</td>\n",
       "      <td>-159.314682</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just talking to Teddy.</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          session_id  index  elapsed_time      event_name   name  level page  \\\n",
       "0  20090312431273200      0             0  cutscene_click  basic      0  NaN   \n",
       "1  20090312431273200      1          1323    person_click  basic      0  NaN   \n",
       "2  20090312431273200      2           831    person_click  basic      0  NaN   \n",
       "\n",
       "   room_coor_x  room_coor_y  screen_coor_x  screen_coor_y  hover_duration  \\\n",
       "0  -413.991394  -159.314682          380.0          494.0             NaN   \n",
       "1  -413.991394  -159.314682          380.0          494.0             NaN   \n",
       "2  -413.991394  -159.314682          380.0          494.0             NaN   \n",
       "\n",
       "                            text    fqid                       room_fqid  \\\n",
       "0                      undefined   intro  tunic.historicalsociety.closet   \n",
       "1  Whatcha doing over there, Jo?  gramps  tunic.historicalsociety.closet   \n",
       "2         Just talking to Teddy.  gramps  tunic.historicalsociety.closet   \n",
       "\n",
       "                                           text_fqid fullscreen   hq music  \\\n",
       "0               tunic.historicalsociety.closet.intro        NaN  NaN   NaN   \n",
       "1  tunic.historicalsociety.closet.gramps.intro_0_...        NaN  NaN   NaN   \n",
       "2  tunic.historicalsociety.closet.gramps.intro_0_...        NaN  NaN   NaN   \n",
       "\n",
       "  level_group  \n",
       "0         0-4  \n",
       "1         0-4  \n",
       "2         0-4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the source training set\n",
    "df_source = sd.read_csv('../data/train.csv.gz',\n",
    "                        compression='gzip',\n",
    "                        dtype=sd.source_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(212022, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200_q1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312433251036_q1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090314121766812_q1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             session_id  correct\n",
       "0  20090312431273200_q1        1\n",
       "1  20090312433251036_q1        0\n",
       "2  20090314121766812_q1        1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the source training labels\n",
    "df_source_labels = sd.read_csv('../data/train_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the main dataset\n",
    "df_source = dp.prepare_main_dataset(df_source,\n",
    "                                    elapsed_time_min_clip=0,\n",
    "                                    elapsed_time_max_clip=3691298)\n",
    "\n",
    "# remove sessions with problems\n",
    "problem_sessions = dp.find_problem_sessions(df_source)\n",
    "df_source = df_source[~df_source['session_id'].isin(problem_sessions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the label dataset\n",
    "df_source_labels = dp.prepare_label_dataset(df_source_labels)\n",
    "\n",
    "# remove sessions with problems\n",
    "df_source_labels = df_source_labels[~df_source_labels['session_id'].isin(problem_sessions)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the initial features\n",
    "df_features = fe.create_initial_features(df_source, df_source_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the feature to the features dataset\n",
    "df_features = fe.add_elapsed_time_features(\n",
    "    features=df_features,\n",
    "    X=df_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the total count features to the features dataset\n",
    "df_features = fe.add_count_total_features(\n",
    "    features=df_features,\n",
    "    X=df_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the unique count features to the features dataset\n",
    "df_features = fe.add_count_unique_features(\n",
    "    features=df_features,\n",
    "    X=df_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add the screen heatmap feature to the features dataset\n",
    "# df_features = fe.add_screen_heatmap_feature(df_features, df_source)\n",
    "\n",
    "# with pd.option_context('display.max_columns', None):\n",
    "#     display(df_features.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>level_group</th>\n",
       "      <th>elapsed_time_sum</th>\n",
       "      <th>elapsed_time_max</th>\n",
       "      <th>elapsed_time_min</th>\n",
       "      <th>elapsed_time_mean</th>\n",
       "      <th>elapsed_time_mode</th>\n",
       "      <th>count_total_event_name</th>\n",
       "      <th>count_total_name</th>\n",
       "      <th>count_total_fqid</th>\n",
       "      <th>count_total_room_fqid</th>\n",
       "      <th>count_total_text_fqid</th>\n",
       "      <th>count_unique_event_name</th>\n",
       "      <th>count_unique_name</th>\n",
       "      <th>count_unique_fqid</th>\n",
       "      <th>count_unique_room_fqid</th>\n",
       "      <th>count_unique_text_fqid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>0-4</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>0.052535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088782</td>\n",
       "      <td>0.088782</td>\n",
       "      <td>0.064620</td>\n",
       "      <td>0.088782</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.203390</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>13-22</td>\n",
       "      <td>0.043740</td>\n",
       "      <td>0.344602</td>\n",
       "      <td>0.226677</td>\n",
       "      <td>0.281804</td>\n",
       "      <td>0.301320</td>\n",
       "      <td>0.394721</td>\n",
       "      <td>0.394721</td>\n",
       "      <td>0.402262</td>\n",
       "      <td>0.394721</td>\n",
       "      <td>0.480127</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525424</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>5-12</td>\n",
       "      <td>0.010577</td>\n",
       "      <td>0.135014</td>\n",
       "      <td>0.060002</td>\n",
       "      <td>0.096641</td>\n",
       "      <td>0.060002</td>\n",
       "      <td>0.245951</td>\n",
       "      <td>0.245951</td>\n",
       "      <td>0.276252</td>\n",
       "      <td>0.245951</td>\n",
       "      <td>0.257552</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20090312433251036</td>\n",
       "      <td>0-4</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.063074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057588</td>\n",
       "      <td>0.057588</td>\n",
       "      <td>0.053312</td>\n",
       "      <td>0.057588</td>\n",
       "      <td>0.050874</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20090312433251036</td>\n",
       "      <td>13-22</td>\n",
       "      <td>0.324157</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.318718</td>\n",
       "      <td>0.676403</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.585056</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.932203</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20090312433251036</td>\n",
       "      <td>5-12</td>\n",
       "      <td>0.021933</td>\n",
       "      <td>0.221287</td>\n",
       "      <td>0.072301</td>\n",
       "      <td>0.150206</td>\n",
       "      <td>0.072301</td>\n",
       "      <td>0.364727</td>\n",
       "      <td>0.364727</td>\n",
       "      <td>0.400646</td>\n",
       "      <td>0.364727</td>\n",
       "      <td>0.238474</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.457627</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          session_id level_group  elapsed_time_sum  elapsed_time_max  \\\n",
       "0  20090312431273200         0-4          0.001411          0.052535   \n",
       "1  20090312431273200       13-22          0.043740          0.344602   \n",
       "2  20090312431273200        5-12          0.010577          0.135014   \n",
       "3  20090312433251036         0-4          0.001352          0.063074   \n",
       "4  20090312433251036       13-22          0.324157          1.000000   \n",
       "5  20090312433251036        5-12          0.021933          0.221287   \n",
       "\n",
       "   elapsed_time_min  elapsed_time_mean  elapsed_time_mode  \\\n",
       "0          0.000000           0.023103           0.000000   \n",
       "1          0.226677           0.281804           0.301320   \n",
       "2          0.060002           0.096641           0.060002   \n",
       "3          0.000000           0.026311           0.000000   \n",
       "4          0.318718           0.676403           1.000000   \n",
       "5          0.072301           0.150206           0.072301   \n",
       "\n",
       "   count_total_event_name  count_total_name  count_total_fqid  \\\n",
       "0                0.088782          0.088782          0.064620   \n",
       "1                0.394721          0.394721          0.402262   \n",
       "2                0.245951          0.245951          0.276252   \n",
       "3                0.057588          0.057588          0.053312   \n",
       "4                1.000000          1.000000          1.000000   \n",
       "5                0.364727          0.364727          0.400646   \n",
       "\n",
       "   count_total_room_fqid  count_total_text_fqid  count_unique_event_name  \\\n",
       "0               0.088782               0.054054                     0.75   \n",
       "1               0.394721               0.480127                     0.75   \n",
       "2               0.245951               0.257552                     0.75   \n",
       "3               0.057588               0.050874                     1.00   \n",
       "4               1.000000               0.585056                     1.00   \n",
       "5               0.364727               0.238474                     1.00   \n",
       "\n",
       "   count_unique_name  count_unique_fqid  count_unique_room_fqid  \\\n",
       "0           0.000000           0.203390                0.090909   \n",
       "1           0.000000           0.525424                0.545455   \n",
       "2           0.000000           0.355932                0.454545   \n",
       "3           0.333333           0.067797                0.000000   \n",
       "4           1.000000           0.932203                0.909091   \n",
       "5           0.333333           0.457627                0.454545   \n",
       "\n",
       "   count_unique_text_fqid  \n",
       "0                   0.225  \n",
       "1                   0.675  \n",
       "2                   0.400  \n",
       "3                   0.075  \n",
       "4                   0.875  \n",
       "5                   0.350  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display the features dataset\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_features.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3495\n",
      "Validation: 1165\n",
      "Test: 6988\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into train, validation and test sets\n",
    "train, val, test = md.select_sessions(\n",
    "    y=df_source_labels,\n",
    "    random_state=random_state,\n",
    "    test_size=0.60,\n",
    "    train_size=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-22 18:44:35 INFO     -- Creating the train dataset\n",
      "2023-03-22 18:44:35 INFO     ---- Creating the X dataset\n",
      "2023-03-22 18:44:35 INFO     Creating the dataset for all level groups\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94208ab3ad804f799a419c592da82fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-22 18:45:22 INFO     -- Creating the val dataset\n",
      "2023-03-22 18:45:22 INFO     ---- Creating the X dataset\n",
      "2023-03-22 18:45:22 INFO     Creating the dataset for all level groups\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8612f239057541b8ba49bd23e8e78809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20970 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-22 18:45:37 INFO     -- Creating the test dataset\n",
      "2023-03-22 18:45:37 INFO     ---- Creating the X dataset\n",
      "2023-03-22 18:45:37 INFO     Creating the dataset for all level groups\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d22fb57eab44431993e0eb5ffc20a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125784 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set the feature list\n",
    "feature_list = ['elapsed_time_sum', 'elapsed_time_max', 'elapsed_time_min', 'elapsed_time_mean', 'elapsed_time_mode']\n",
    "\n",
    "# create the simple model dataset\n",
    "simple_model_dataset = md.get_feature_dataset(\n",
    "    features=df_features,\n",
    "    y=df_source_labels,\n",
    "    feature_list=feature_list,\n",
    "    train=train,\n",
    "    val=val,\n",
    "    test=test,\n",
    "    include_question=True,\n",
    "    expand_question=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X_shape: (62910, 23)\n",
      "train_y_shape: (62910,)\n",
      "\n",
      "val_X_shape: (20970, 23)\n",
      "val_y_shape: (20970,)\n",
      "\n",
      "test_X_shape: (125784, 23)\n",
      "test_y_shape: (125784,)\n"
     ]
    }
   ],
   "source": [
    "print('train_X_shape:', simple_model_dataset['train']['X'].shape)\n",
    "print('train_y_shape:', simple_model_dataset['train']['y'].shape)\n",
    "print()\n",
    "\n",
    "print('val_X_shape:', simple_model_dataset['val']['X'].shape)\n",
    "print('val_y_shape:', simple_model_dataset['val']['y'].shape)\n",
    "print()\n",
    "\n",
    "print('test_X_shape:', simple_model_dataset['test']['X'].shape)\n",
    "print('test_y_shape:', simple_model_dataset['test']['y'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_model_shape: 23\n",
      "output_shape 1\n"
     ]
    }
   ],
   "source": [
    "# define the simple model input shape\n",
    "input_data = simple_model_dataset['train']['X']\n",
    "simple_model_shape = input_data.shape[1]\n",
    "print('simple_model_shape:', simple_model_shape)\n",
    "\n",
    "# define the output shape\n",
    "output_data = simple_model_dataset['train']['y']\n",
    "simple_model_output_shape = 1\n",
    "print('output_shape', simple_model_output_shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure mlflow\n",
    "mlflow.set_experiment(\"student-performance\")\n",
    "mlflow.keras.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_14 (InputLayer)       [(None, 23)]              0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 1024)              24576     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,075,201\n",
      "Trainable params: 1,075,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      " 1/63 [..............................] - ETA: 25s - loss: 0.9984 - accuracy: 0.3280WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_train_batch_end` time: 0.0022s). Check your callbacks.\n",
      "2023-03-22 19:24:45 WARNING  Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_train_batch_end` time: 0.0022s). Check your callbacks.\n",
      "63/63 [==============================] - 1s 4ms/step - loss: 0.8488 - accuracy: 0.7133 - val_loss: 0.7928 - val_accuracy: 0.7363\n",
      "Epoch 2/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7738 - accuracy: 0.7361 - val_loss: 0.7513 - val_accuracy: 0.7383\n",
      "Epoch 3/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7395 - accuracy: 0.7373 - val_loss: 0.7211 - val_accuracy: 0.7394\n",
      "Epoch 4/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.7112 - accuracy: 0.7381 - val_loss: 0.6946 - val_accuracy: 0.7398\n",
      "Epoch 5/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6858 - accuracy: 0.7381 - val_loss: 0.6720 - val_accuracy: 0.7382\n",
      "Epoch 6/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6638 - accuracy: 0.7387 - val_loss: 0.6510 - val_accuracy: 0.7385\n",
      "Epoch 7/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6447 - accuracy: 0.7389 - val_loss: 0.6332 - val_accuracy: 0.7404\n",
      "Epoch 8/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6292 - accuracy: 0.7404 - val_loss: 0.6195 - val_accuracy: 0.7410\n",
      "Epoch 9/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6166 - accuracy: 0.7397 - val_loss: 0.6082 - val_accuracy: 0.7412\n",
      "Epoch 10/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6061 - accuracy: 0.7410 - val_loss: 0.5993 - val_accuracy: 0.7409\n",
      "Epoch 11/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5972 - accuracy: 0.7412 - val_loss: 0.5907 - val_accuracy: 0.7423\n",
      "Epoch 12/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5895 - accuracy: 0.7405 - val_loss: 0.5834 - val_accuracy: 0.7415\n",
      "Epoch 13/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5833 - accuracy: 0.7408 - val_loss: 0.5777 - val_accuracy: 0.7423\n",
      "Epoch 14/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5777 - accuracy: 0.7403 - val_loss: 0.5759 - val_accuracy: 0.7387\n",
      "Epoch 15/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5731 - accuracy: 0.7404 - val_loss: 0.5680 - val_accuracy: 0.7430\n",
      "Epoch 16/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5686 - accuracy: 0.7414 - val_loss: 0.5641 - val_accuracy: 0.7429\n",
      "Epoch 17/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5649 - accuracy: 0.7413 - val_loss: 0.5604 - val_accuracy: 0.7430\n",
      "Epoch 18/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5614 - accuracy: 0.7419 - val_loss: 0.5574 - val_accuracy: 0.7439\n",
      "Epoch 19/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5583 - accuracy: 0.7418 - val_loss: 0.5545 - val_accuracy: 0.7423\n",
      "Epoch 20/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5557 - accuracy: 0.7423 - val_loss: 0.5521 - val_accuracy: 0.7417\n",
      "Epoch 21/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5532 - accuracy: 0.7420 - val_loss: 0.5500 - val_accuracy: 0.7438\n",
      "Epoch 22/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5513 - accuracy: 0.7410 - val_loss: 0.5478 - val_accuracy: 0.7424\n",
      "Epoch 23/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5495 - accuracy: 0.7411 - val_loss: 0.5460 - val_accuracy: 0.7439\n",
      "Epoch 24/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5476 - accuracy: 0.7425 - val_loss: 0.5443 - val_accuracy: 0.7434\n",
      "Epoch 25/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5463 - accuracy: 0.7416 - val_loss: 0.5434 - val_accuracy: 0.7440\n",
      "Epoch 26/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5447 - accuracy: 0.7419 - val_loss: 0.5417 - val_accuracy: 0.7435\n",
      "Epoch 27/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5438 - accuracy: 0.7423 - val_loss: 0.5407 - val_accuracy: 0.7433\n",
      "Epoch 28/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5424 - accuracy: 0.7419 - val_loss: 0.5392 - val_accuracy: 0.7433\n",
      "Epoch 29/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5415 - accuracy: 0.7421 - val_loss: 0.5382 - val_accuracy: 0.7439\n",
      "Epoch 30/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5405 - accuracy: 0.7430 - val_loss: 0.5379 - val_accuracy: 0.7429\n",
      "Epoch 31/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5395 - accuracy: 0.7418 - val_loss: 0.5379 - val_accuracy: 0.7421\n",
      "Epoch 32/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5389 - accuracy: 0.7428 - val_loss: 0.5363 - val_accuracy: 0.7436\n",
      "Epoch 33/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5381 - accuracy: 0.7420 - val_loss: 0.5350 - val_accuracy: 0.7436\n",
      "Epoch 34/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5372 - accuracy: 0.7422 - val_loss: 0.5346 - val_accuracy: 0.7431\n",
      "Epoch 35/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5365 - accuracy: 0.7424 - val_loss: 0.5337 - val_accuracy: 0.7441\n",
      "Epoch 36/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5360 - accuracy: 0.7422 - val_loss: 0.5337 - val_accuracy: 0.7427\n",
      "Epoch 37/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5355 - accuracy: 0.7428 - val_loss: 0.5329 - val_accuracy: 0.7449\n",
      "Epoch 38/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5348 - accuracy: 0.7424 - val_loss: 0.5324 - val_accuracy: 0.7431\n",
      "Epoch 39/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5344 - accuracy: 0.7420 - val_loss: 0.5322 - val_accuracy: 0.7452\n",
      "Epoch 40/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7418 - val_loss: 0.5311 - val_accuracy: 0.7434\n",
      "Epoch 41/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7426 - val_loss: 0.5311 - val_accuracy: 0.7422\n",
      "Epoch 42/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7426 - val_loss: 0.5305 - val_accuracy: 0.7430\n",
      "Epoch 43/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5328 - accuracy: 0.7424 - val_loss: 0.5308 - val_accuracy: 0.7432\n",
      "Epoch 44/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5322 - accuracy: 0.7427 - val_loss: 0.5296 - val_accuracy: 0.7430\n",
      "Epoch 45/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5319 - accuracy: 0.7424 - val_loss: 0.5300 - val_accuracy: 0.7428\n",
      "Epoch 46/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5316 - accuracy: 0.7431 - val_loss: 0.5293 - val_accuracy: 0.7430\n",
      "Epoch 47/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5313 - accuracy: 0.7428 - val_loss: 0.5289 - val_accuracy: 0.7441\n",
      "Epoch 48/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5310 - accuracy: 0.7428 - val_loss: 0.5287 - val_accuracy: 0.7445\n",
      "Epoch 49/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5308 - accuracy: 0.7430 - val_loss: 0.5289 - val_accuracy: 0.7425\n",
      "Epoch 50/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5305 - accuracy: 0.7425 - val_loss: 0.5278 - val_accuracy: 0.7433\n",
      "Epoch 51/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5302 - accuracy: 0.7429 - val_loss: 0.5278 - val_accuracy: 0.7438\n",
      "Epoch 52/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5299 - accuracy: 0.7427 - val_loss: 0.5276 - val_accuracy: 0.7433\n",
      "Epoch 53/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5297 - accuracy: 0.7424 - val_loss: 0.5295 - val_accuracy: 0.7429\n",
      "Epoch 54/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5295 - accuracy: 0.7434 - val_loss: 0.5277 - val_accuracy: 0.7436\n",
      "Epoch 55/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5292 - accuracy: 0.7432 - val_loss: 0.5284 - val_accuracy: 0.7433\n",
      "Epoch 56/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5291 - accuracy: 0.7433 - val_loss: 0.5267 - val_accuracy: 0.7435\n",
      "Epoch 57/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5288 - accuracy: 0.7427 - val_loss: 0.5270 - val_accuracy: 0.7440\n",
      "Epoch 58/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5285 - accuracy: 0.7436 - val_loss: 0.5305 - val_accuracy: 0.7410\n",
      "Epoch 59/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5284 - accuracy: 0.7428 - val_loss: 0.5258 - val_accuracy: 0.7442\n",
      "Epoch 60/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5282 - accuracy: 0.7433 - val_loss: 0.5259 - val_accuracy: 0.7439\n",
      "Epoch 61/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5280 - accuracy: 0.7435 - val_loss: 0.5258 - val_accuracy: 0.7437\n",
      "Epoch 62/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5279 - accuracy: 0.7438 - val_loss: 0.5255 - val_accuracy: 0.7441\n",
      "Epoch 63/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5276 - accuracy: 0.7438 - val_loss: 0.5260 - val_accuracy: 0.7434\n",
      "Epoch 64/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5275 - accuracy: 0.7431 - val_loss: 0.5251 - val_accuracy: 0.7436\n",
      "Epoch 65/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5272 - accuracy: 0.7440 - val_loss: 0.5253 - val_accuracy: 0.7440\n",
      "Epoch 66/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5271 - accuracy: 0.7429 - val_loss: 0.5266 - val_accuracy: 0.7431\n",
      "Epoch 67/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5271 - accuracy: 0.7434 - val_loss: 0.5247 - val_accuracy: 0.7452\n",
      "Epoch 68/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5266 - accuracy: 0.7436 - val_loss: 0.5246 - val_accuracy: 0.7447\n",
      "Epoch 69/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5265 - accuracy: 0.7444 - val_loss: 0.5256 - val_accuracy: 0.7438\n",
      "Epoch 70/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5265 - accuracy: 0.7440 - val_loss: 0.5243 - val_accuracy: 0.7452\n",
      "Epoch 71/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5263 - accuracy: 0.7429 - val_loss: 0.5241 - val_accuracy: 0.7446\n",
      "Epoch 72/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5262 - accuracy: 0.7440 - val_loss: 0.5241 - val_accuracy: 0.7447\n",
      "Epoch 73/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5261 - accuracy: 0.7450 - val_loss: 0.5238 - val_accuracy: 0.7452\n",
      "Epoch 74/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5259 - accuracy: 0.7437 - val_loss: 0.5237 - val_accuracy: 0.7455\n",
      "Epoch 75/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5257 - accuracy: 0.7440 - val_loss: 0.5237 - val_accuracy: 0.7438\n",
      "Epoch 76/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5255 - accuracy: 0.7444 - val_loss: 0.5246 - val_accuracy: 0.7442\n",
      "Epoch 77/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5255 - accuracy: 0.7441 - val_loss: 0.5236 - val_accuracy: 0.7459\n",
      "Epoch 78/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5253 - accuracy: 0.7443 - val_loss: 0.5231 - val_accuracy: 0.7445\n",
      "Epoch 79/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5250 - accuracy: 0.7445 - val_loss: 0.5230 - val_accuracy: 0.7449\n",
      "Epoch 80/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5249 - accuracy: 0.7445 - val_loss: 0.5234 - val_accuracy: 0.7447\n",
      "Epoch 81/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5249 - accuracy: 0.7446 - val_loss: 0.5228 - val_accuracy: 0.7453\n",
      "Epoch 82/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5248 - accuracy: 0.7453 - val_loss: 0.5240 - val_accuracy: 0.7446\n",
      "Epoch 83/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5247 - accuracy: 0.7453 - val_loss: 0.5225 - val_accuracy: 0.7448\n",
      "Epoch 84/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5245 - accuracy: 0.7450 - val_loss: 0.5237 - val_accuracy: 0.7451\n",
      "Epoch 85/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5246 - accuracy: 0.7448 - val_loss: 0.5229 - val_accuracy: 0.7445\n",
      "Epoch 86/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5242 - accuracy: 0.7452 - val_loss: 0.5224 - val_accuracy: 0.7464\n",
      "Epoch 87/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5243 - accuracy: 0.7450 - val_loss: 0.5222 - val_accuracy: 0.7449\n",
      "Epoch 88/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5240 - accuracy: 0.7450 - val_loss: 0.5225 - val_accuracy: 0.7458\n",
      "Epoch 89/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5239 - accuracy: 0.7448 - val_loss: 0.5219 - val_accuracy: 0.7448\n",
      "Epoch 90/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5238 - accuracy: 0.7457 - val_loss: 0.5220 - val_accuracy: 0.7458\n",
      "Epoch 91/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5238 - accuracy: 0.7457 - val_loss: 0.5217 - val_accuracy: 0.7458\n",
      "Epoch 92/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5235 - accuracy: 0.7455 - val_loss: 0.5215 - val_accuracy: 0.7454\n",
      "Epoch 93/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5234 - accuracy: 0.7449 - val_loss: 0.5220 - val_accuracy: 0.7454\n",
      "Epoch 94/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5233 - accuracy: 0.7453 - val_loss: 0.5215 - val_accuracy: 0.7458\n",
      "Epoch 95/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5232 - accuracy: 0.7456 - val_loss: 0.5215 - val_accuracy: 0.7458\n",
      "Epoch 96/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5231 - accuracy: 0.7460 - val_loss: 0.5214 - val_accuracy: 0.7459\n",
      "Epoch 97/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5229 - accuracy: 0.7458 - val_loss: 0.5218 - val_accuracy: 0.7457\n",
      "Epoch 98/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5229 - accuracy: 0.7458 - val_loss: 0.5214 - val_accuracy: 0.7459\n",
      "Epoch 99/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5228 - accuracy: 0.7457 - val_loss: 0.5216 - val_accuracy: 0.7458\n",
      "Epoch 100/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5228 - accuracy: 0.7455 - val_loss: 0.5208 - val_accuracy: 0.7455\n",
      "Epoch 101/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5226 - accuracy: 0.7458 - val_loss: 0.5208 - val_accuracy: 0.7463\n",
      "Epoch 102/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5225 - accuracy: 0.7462 - val_loss: 0.5206 - val_accuracy: 0.7458\n",
      "Epoch 103/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5223 - accuracy: 0.7462 - val_loss: 0.5207 - val_accuracy: 0.7460\n",
      "Epoch 104/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5223 - accuracy: 0.7461 - val_loss: 0.5204 - val_accuracy: 0.7461\n",
      "Epoch 105/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5221 - accuracy: 0.7460 - val_loss: 0.5204 - val_accuracy: 0.7460\n",
      "Epoch 106/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5221 - accuracy: 0.7467 - val_loss: 0.5204 - val_accuracy: 0.7456\n",
      "Epoch 107/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5219 - accuracy: 0.7456 - val_loss: 0.5203 - val_accuracy: 0.7464\n",
      "Epoch 108/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5219 - accuracy: 0.7456 - val_loss: 0.5208 - val_accuracy: 0.7463\n",
      "Epoch 109/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5218 - accuracy: 0.7462 - val_loss: 0.5204 - val_accuracy: 0.7461\n",
      "Epoch 110/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5216 - accuracy: 0.7464 - val_loss: 0.5204 - val_accuracy: 0.7459\n",
      "Epoch 111/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5216 - accuracy: 0.7460 - val_loss: 0.5198 - val_accuracy: 0.7463\n",
      "Epoch 112/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5216 - accuracy: 0.7463 - val_loss: 0.5200 - val_accuracy: 0.7460\n",
      "Epoch 113/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5214 - accuracy: 0.7458 - val_loss: 0.5202 - val_accuracy: 0.7465\n",
      "Epoch 114/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5213 - accuracy: 0.7462 - val_loss: 0.5199 - val_accuracy: 0.7468\n",
      "Epoch 115/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5213 - accuracy: 0.7462 - val_loss: 0.5204 - val_accuracy: 0.7468\n",
      "Epoch 116/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5211 - accuracy: 0.7466 - val_loss: 0.5196 - val_accuracy: 0.7461\n",
      "Epoch 117/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5211 - accuracy: 0.7472 - val_loss: 0.5195 - val_accuracy: 0.7465\n",
      "Epoch 118/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5210 - accuracy: 0.7468 - val_loss: 0.5194 - val_accuracy: 0.7465\n",
      "Epoch 119/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5208 - accuracy: 0.7474 - val_loss: 0.5198 - val_accuracy: 0.7465\n",
      "Epoch 120/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5208 - accuracy: 0.7466 - val_loss: 0.5195 - val_accuracy: 0.7467\n",
      "Epoch 121/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5207 - accuracy: 0.7466 - val_loss: 0.5202 - val_accuracy: 0.7463\n",
      "Epoch 122/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5208 - accuracy: 0.7468 - val_loss: 0.5191 - val_accuracy: 0.7464\n",
      "Epoch 123/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5206 - accuracy: 0.7478 - val_loss: 0.5190 - val_accuracy: 0.7461\n",
      "Epoch 124/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5205 - accuracy: 0.7473 - val_loss: 0.5190 - val_accuracy: 0.7465\n",
      "Epoch 125/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5204 - accuracy: 0.7465 - val_loss: 0.5188 - val_accuracy: 0.7462\n",
      "Epoch 126/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5203 - accuracy: 0.7471 - val_loss: 0.5188 - val_accuracy: 0.7464\n",
      "Epoch 127/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5202 - accuracy: 0.7469 - val_loss: 0.5189 - val_accuracy: 0.7465\n",
      "Epoch 128/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5201 - accuracy: 0.7475 - val_loss: 0.5191 - val_accuracy: 0.7472\n",
      "Epoch 129/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5200 - accuracy: 0.7469 - val_loss: 0.5187 - val_accuracy: 0.7462\n",
      "Epoch 130/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5200 - accuracy: 0.7472 - val_loss: 0.5187 - val_accuracy: 0.7460\n",
      "Epoch 131/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5199 - accuracy: 0.7474 - val_loss: 0.5188 - val_accuracy: 0.7464\n",
      "Epoch 132/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5199 - accuracy: 0.7471 - val_loss: 0.5192 - val_accuracy: 0.7459\n",
      "Epoch 133/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5198 - accuracy: 0.7476 - val_loss: 0.5186 - val_accuracy: 0.7468\n",
      "Epoch 134/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5197 - accuracy: 0.7478 - val_loss: 0.5193 - val_accuracy: 0.7466\n",
      "Epoch 135/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5197 - accuracy: 0.7480 - val_loss: 0.5185 - val_accuracy: 0.7464\n",
      "Epoch 136/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5196 - accuracy: 0.7475 - val_loss: 0.5185 - val_accuracy: 0.7466\n",
      "Epoch 137/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5196 - accuracy: 0.7479 - val_loss: 0.5189 - val_accuracy: 0.7468\n",
      "Epoch 138/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5194 - accuracy: 0.7478 - val_loss: 0.5182 - val_accuracy: 0.7462\n",
      "Epoch 139/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5194 - accuracy: 0.7468 - val_loss: 0.5182 - val_accuracy: 0.7467\n",
      "Epoch 140/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5193 - accuracy: 0.7477 - val_loss: 0.5179 - val_accuracy: 0.7466\n",
      "Epoch 141/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5192 - accuracy: 0.7474 - val_loss: 0.5195 - val_accuracy: 0.7467\n",
      "Epoch 142/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5192 - accuracy: 0.7474 - val_loss: 0.5181 - val_accuracy: 0.7464\n",
      "Epoch 143/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5192 - accuracy: 0.7473 - val_loss: 0.5179 - val_accuracy: 0.7469\n",
      "Epoch 144/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5190 - accuracy: 0.7482 - val_loss: 0.5179 - val_accuracy: 0.7464\n",
      "Epoch 145/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5189 - accuracy: 0.7474 - val_loss: 0.5195 - val_accuracy: 0.7475\n",
      "Epoch 146/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5190 - accuracy: 0.7479 - val_loss: 0.5178 - val_accuracy: 0.7466\n",
      "Epoch 147/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5188 - accuracy: 0.7476 - val_loss: 0.5177 - val_accuracy: 0.7473\n",
      "Epoch 148/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5188 - accuracy: 0.7480 - val_loss: 0.5176 - val_accuracy: 0.7465\n",
      "Epoch 149/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5186 - accuracy: 0.7481 - val_loss: 0.5180 - val_accuracy: 0.7464\n",
      "Epoch 150/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5186 - accuracy: 0.7482 - val_loss: 0.5177 - val_accuracy: 0.7473\n",
      "Epoch 151/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5186 - accuracy: 0.7474 - val_loss: 0.5176 - val_accuracy: 0.7467\n",
      "Epoch 152/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5184 - accuracy: 0.7481 - val_loss: 0.5191 - val_accuracy: 0.7463\n",
      "Epoch 153/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5185 - accuracy: 0.7480 - val_loss: 0.5182 - val_accuracy: 0.7474\n",
      "Epoch 154/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5184 - accuracy: 0.7482 - val_loss: 0.5174 - val_accuracy: 0.7469\n",
      "Epoch 155/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5184 - accuracy: 0.7482 - val_loss: 0.5173 - val_accuracy: 0.7462\n",
      "Epoch 156/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5183 - accuracy: 0.7476 - val_loss: 0.5173 - val_accuracy: 0.7468\n",
      "Epoch 157/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5183 - accuracy: 0.7485 - val_loss: 0.5173 - val_accuracy: 0.7462\n",
      "Epoch 158/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5182 - accuracy: 0.7486 - val_loss: 0.5172 - val_accuracy: 0.7468\n",
      "Epoch 159/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5182 - accuracy: 0.7484 - val_loss: 0.5172 - val_accuracy: 0.7467\n",
      "Epoch 160/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5180 - accuracy: 0.7478 - val_loss: 0.5175 - val_accuracy: 0.7463\n",
      "Epoch 161/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5181 - accuracy: 0.7483 - val_loss: 0.5170 - val_accuracy: 0.7467\n",
      "Epoch 162/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5179 - accuracy: 0.7479 - val_loss: 0.5177 - val_accuracy: 0.7472\n",
      "Epoch 163/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5179 - accuracy: 0.7483 - val_loss: 0.5178 - val_accuracy: 0.7464\n",
      "Epoch 164/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5179 - accuracy: 0.7483 - val_loss: 0.5171 - val_accuracy: 0.7475\n",
      "Epoch 165/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5178 - accuracy: 0.7483 - val_loss: 0.5172 - val_accuracy: 0.7466\n",
      "Epoch 166/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5179 - accuracy: 0.7485 - val_loss: 0.5167 - val_accuracy: 0.7468\n",
      "Epoch 167/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5177 - accuracy: 0.7486 - val_loss: 0.5171 - val_accuracy: 0.7468\n",
      "Epoch 168/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5175 - accuracy: 0.7483 - val_loss: 0.5172 - val_accuracy: 0.7474\n",
      "Epoch 169/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5177 - accuracy: 0.7487 - val_loss: 0.5166 - val_accuracy: 0.7465\n",
      "Epoch 170/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5176 - accuracy: 0.7483 - val_loss: 0.5170 - val_accuracy: 0.7466\n",
      "Epoch 171/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5174 - accuracy: 0.7483 - val_loss: 0.5170 - val_accuracy: 0.7468\n",
      "Epoch 172/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5175 - accuracy: 0.7483 - val_loss: 0.5172 - val_accuracy: 0.7464\n",
      "Epoch 173/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5174 - accuracy: 0.7481 - val_loss: 0.5163 - val_accuracy: 0.7463\n",
      "Epoch 174/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5174 - accuracy: 0.7485 - val_loss: 0.5165 - val_accuracy: 0.7475\n",
      "Epoch 175/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5173 - accuracy: 0.7489 - val_loss: 0.5167 - val_accuracy: 0.7467\n",
      "Epoch 176/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5171 - accuracy: 0.7482 - val_loss: 0.5163 - val_accuracy: 0.7467\n",
      "Epoch 177/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5172 - accuracy: 0.7483 - val_loss: 0.5163 - val_accuracy: 0.7471\n",
      "Epoch 178/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5172 - accuracy: 0.7482 - val_loss: 0.5166 - val_accuracy: 0.7468\n",
      "Epoch 179/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5171 - accuracy: 0.7484 - val_loss: 0.5163 - val_accuracy: 0.7461\n",
      "Epoch 180/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5172 - accuracy: 0.7481 - val_loss: 0.5162 - val_accuracy: 0.7468\n",
      "Epoch 181/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5168 - accuracy: 0.7490 - val_loss: 0.5162 - val_accuracy: 0.7473\n",
      "Epoch 182/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5169 - accuracy: 0.7490 - val_loss: 0.5164 - val_accuracy: 0.7471\n",
      "Epoch 183/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5170 - accuracy: 0.7485 - val_loss: 0.5162 - val_accuracy: 0.7460\n",
      "Epoch 184/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5169 - accuracy: 0.7484 - val_loss: 0.5165 - val_accuracy: 0.7467\n",
      "Epoch 185/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5169 - accuracy: 0.7480 - val_loss: 0.5164 - val_accuracy: 0.7464\n",
      "Epoch 186/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5168 - accuracy: 0.7483 - val_loss: 0.5162 - val_accuracy: 0.7469\n",
      "Epoch 187/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5167 - accuracy: 0.7485 - val_loss: 0.5160 - val_accuracy: 0.7464\n",
      "Epoch 188/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5167 - accuracy: 0.7487 - val_loss: 0.5172 - val_accuracy: 0.7469\n",
      "Epoch 189/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5167 - accuracy: 0.7482 - val_loss: 0.5160 - val_accuracy: 0.7466\n",
      "Epoch 190/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5166 - accuracy: 0.7490 - val_loss: 0.5157 - val_accuracy: 0.7464\n",
      "Epoch 191/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5166 - accuracy: 0.7490 - val_loss: 0.5159 - val_accuracy: 0.7473\n",
      "Epoch 192/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5165 - accuracy: 0.7489 - val_loss: 0.5158 - val_accuracy: 0.7463\n",
      "Epoch 193/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5166 - accuracy: 0.7487 - val_loss: 0.5161 - val_accuracy: 0.7479\n",
      "Epoch 194/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5165 - accuracy: 0.7484 - val_loss: 0.5157 - val_accuracy: 0.7465\n",
      "Epoch 195/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5165 - accuracy: 0.7490 - val_loss: 0.5157 - val_accuracy: 0.7458\n",
      "Epoch 196/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5164 - accuracy: 0.7486 - val_loss: 0.5162 - val_accuracy: 0.7474\n",
      "Epoch 197/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5163 - accuracy: 0.7487 - val_loss: 0.5157 - val_accuracy: 0.7470\n",
      "Epoch 198/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5163 - accuracy: 0.7495 - val_loss: 0.5155 - val_accuracy: 0.7469\n",
      "Epoch 199/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5163 - accuracy: 0.7489 - val_loss: 0.5158 - val_accuracy: 0.7472\n",
      "Epoch 200/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5163 - accuracy: 0.7488 - val_loss: 0.5156 - val_accuracy: 0.7466\n",
      "Epoch 201/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5163 - accuracy: 0.7495 - val_loss: 0.5155 - val_accuracy: 0.7464\n",
      "Epoch 202/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5162 - accuracy: 0.7482 - val_loss: 0.5158 - val_accuracy: 0.7462\n",
      "Epoch 203/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5162 - accuracy: 0.7486 - val_loss: 0.5155 - val_accuracy: 0.7463\n",
      "Epoch 204/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5160 - accuracy: 0.7494 - val_loss: 0.5156 - val_accuracy: 0.7463\n",
      "Epoch 205/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5160 - accuracy: 0.7498 - val_loss: 0.5154 - val_accuracy: 0.7464\n",
      "Epoch 206/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5161 - accuracy: 0.7488 - val_loss: 0.5156 - val_accuracy: 0.7475\n",
      "Epoch 207/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5159 - accuracy: 0.7491 - val_loss: 0.5154 - val_accuracy: 0.7463\n",
      "Epoch 208/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5160 - accuracy: 0.7492 - val_loss: 0.5152 - val_accuracy: 0.7468\n",
      "Epoch 209/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5159 - accuracy: 0.7489 - val_loss: 0.5156 - val_accuracy: 0.7475\n",
      "Epoch 210/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5159 - accuracy: 0.7491 - val_loss: 0.5152 - val_accuracy: 0.7466\n",
      "Epoch 211/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5159 - accuracy: 0.7491 - val_loss: 0.5154 - val_accuracy: 0.7471\n",
      "Epoch 212/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5158 - accuracy: 0.7488 - val_loss: 0.5158 - val_accuracy: 0.7459\n",
      "Epoch 213/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5158 - accuracy: 0.7485 - val_loss: 0.5151 - val_accuracy: 0.7466\n",
      "Epoch 214/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5157 - accuracy: 0.7493 - val_loss: 0.5152 - val_accuracy: 0.7464\n",
      "Epoch 215/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5157 - accuracy: 0.7486 - val_loss: 0.5151 - val_accuracy: 0.7471\n",
      "Epoch 216/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5157 - accuracy: 0.7487 - val_loss: 0.5154 - val_accuracy: 0.7465\n",
      "Epoch 217/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5157 - accuracy: 0.7491 - val_loss: 0.5149 - val_accuracy: 0.7469\n",
      "Epoch 218/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5156 - accuracy: 0.7486 - val_loss: 0.5153 - val_accuracy: 0.7476\n",
      "Epoch 219/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5156 - accuracy: 0.7488 - val_loss: 0.5154 - val_accuracy: 0.7481\n",
      "Epoch 220/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5155 - accuracy: 0.7491 - val_loss: 0.5151 - val_accuracy: 0.7463\n",
      "Epoch 221/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5155 - accuracy: 0.7492 - val_loss: 0.5158 - val_accuracy: 0.7485\n",
      "Epoch 222/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5155 - accuracy: 0.7490 - val_loss: 0.5154 - val_accuracy: 0.7474\n",
      "Epoch 223/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5154 - accuracy: 0.7491 - val_loss: 0.5148 - val_accuracy: 0.7465\n",
      "Epoch 224/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5155 - accuracy: 0.7488 - val_loss: 0.5157 - val_accuracy: 0.7480\n",
      "Epoch 225/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5154 - accuracy: 0.7491 - val_loss: 0.5164 - val_accuracy: 0.7484\n",
      "Epoch 226/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5155 - accuracy: 0.7493 - val_loss: 0.5149 - val_accuracy: 0.7469\n",
      "Epoch 227/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5153 - accuracy: 0.7491 - val_loss: 0.5152 - val_accuracy: 0.7465\n",
      "Epoch 228/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5153 - accuracy: 0.7489 - val_loss: 0.5148 - val_accuracy: 0.7473\n",
      "Epoch 229/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5153 - accuracy: 0.7495 - val_loss: 0.5149 - val_accuracy: 0.7473\n",
      "Epoch 230/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5153 - accuracy: 0.7492 - val_loss: 0.5150 - val_accuracy: 0.7472\n",
      "Epoch 231/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5152 - accuracy: 0.7499 - val_loss: 0.5147 - val_accuracy: 0.7465\n",
      "Epoch 232/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5153 - accuracy: 0.7489 - val_loss: 0.5149 - val_accuracy: 0.7471\n",
      "Epoch 233/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5152 - accuracy: 0.7491 - val_loss: 0.5147 - val_accuracy: 0.7463\n",
      "Epoch 234/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5151 - accuracy: 0.7497 - val_loss: 0.5147 - val_accuracy: 0.7469\n",
      "Epoch 235/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5152 - accuracy: 0.7493 - val_loss: 0.5146 - val_accuracy: 0.7466\n",
      "Epoch 236/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5150 - accuracy: 0.7491 - val_loss: 0.5149 - val_accuracy: 0.7466\n",
      "Epoch 237/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5151 - accuracy: 0.7494 - val_loss: 0.5148 - val_accuracy: 0.7463\n",
      "Epoch 238/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5150 - accuracy: 0.7495 - val_loss: 0.5145 - val_accuracy: 0.7475\n",
      "Epoch 239/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5150 - accuracy: 0.7494 - val_loss: 0.5148 - val_accuracy: 0.7477\n",
      "Epoch 240/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5150 - accuracy: 0.7497 - val_loss: 0.5151 - val_accuracy: 0.7485\n",
      "Epoch 241/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5149 - accuracy: 0.7496 - val_loss: 0.5144 - val_accuracy: 0.7469\n",
      "Epoch 242/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5149 - accuracy: 0.7491 - val_loss: 0.5148 - val_accuracy: 0.7466\n",
      "Epoch 243/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5150 - accuracy: 0.7491 - val_loss: 0.5149 - val_accuracy: 0.7478\n",
      "Epoch 244/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5149 - accuracy: 0.7495 - val_loss: 0.5145 - val_accuracy: 0.7471\n",
      "Epoch 245/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5148 - accuracy: 0.7488 - val_loss: 0.5144 - val_accuracy: 0.7469\n",
      "Epoch 246/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5149 - accuracy: 0.7495 - val_loss: 0.5144 - val_accuracy: 0.7465\n",
      "Epoch 247/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5148 - accuracy: 0.7492 - val_loss: 0.5145 - val_accuracy: 0.7457\n",
      "Epoch 248/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5148 - accuracy: 0.7490 - val_loss: 0.5143 - val_accuracy: 0.7466\n",
      "Epoch 249/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5148 - accuracy: 0.7494 - val_loss: 0.5146 - val_accuracy: 0.7458\n",
      "Epoch 250/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5147 - accuracy: 0.7491 - val_loss: 0.5152 - val_accuracy: 0.7483\n",
      "Epoch 251/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5147 - accuracy: 0.7490 - val_loss: 0.5146 - val_accuracy: 0.7468\n",
      "Epoch 252/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5147 - accuracy: 0.7489 - val_loss: 0.5152 - val_accuracy: 0.7475\n",
      "Epoch 253/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5146 - accuracy: 0.7497 - val_loss: 0.5144 - val_accuracy: 0.7469\n",
      "Epoch 254/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5146 - accuracy: 0.7490 - val_loss: 0.5154 - val_accuracy: 0.7474\n",
      "Epoch 255/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5146 - accuracy: 0.7495 - val_loss: 0.5144 - val_accuracy: 0.7471\n",
      "Epoch 256/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5146 - accuracy: 0.7493 - val_loss: 0.5144 - val_accuracy: 0.7477\n",
      "Epoch 257/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5145 - accuracy: 0.7493 - val_loss: 0.5145 - val_accuracy: 0.7477\n",
      "Epoch 258/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5145 - accuracy: 0.7495 - val_loss: 0.5143 - val_accuracy: 0.7477\n",
      "Epoch 259/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5145 - accuracy: 0.7491 - val_loss: 0.5143 - val_accuracy: 0.7469\n",
      "Epoch 260/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5145 - accuracy: 0.7493 - val_loss: 0.5146 - val_accuracy: 0.7484\n",
      "Epoch 261/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5145 - accuracy: 0.7495 - val_loss: 0.5148 - val_accuracy: 0.7481\n",
      "Epoch 262/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5145 - accuracy: 0.7491 - val_loss: 0.5141 - val_accuracy: 0.7472\n",
      "Epoch 263/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5145 - accuracy: 0.7493 - val_loss: 0.5140 - val_accuracy: 0.7468\n",
      "Epoch 264/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5144 - accuracy: 0.7491 - val_loss: 0.5141 - val_accuracy: 0.7471\n",
      "Epoch 265/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5145 - accuracy: 0.7495 - val_loss: 0.5141 - val_accuracy: 0.7470\n",
      "Epoch 266/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5143 - accuracy: 0.7488 - val_loss: 0.5142 - val_accuracy: 0.7473\n",
      "Epoch 267/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5143 - accuracy: 0.7490 - val_loss: 0.5141 - val_accuracy: 0.7473\n",
      "Epoch 268/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5144 - accuracy: 0.7494 - val_loss: 0.5142 - val_accuracy: 0.7479\n",
      "Epoch 269/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5143 - accuracy: 0.7495 - val_loss: 0.5140 - val_accuracy: 0.7464\n",
      "Epoch 270/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5142 - accuracy: 0.7492 - val_loss: 0.5139 - val_accuracy: 0.7468\n",
      "Epoch 271/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5142 - accuracy: 0.7492 - val_loss: 0.5140 - val_accuracy: 0.7467\n",
      "Epoch 272/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5142 - accuracy: 0.7492 - val_loss: 0.5154 - val_accuracy: 0.7484\n",
      "Epoch 273/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5141 - accuracy: 0.7493 - val_loss: 0.5143 - val_accuracy: 0.7481\n",
      "Epoch 274/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5143 - accuracy: 0.7493 - val_loss: 0.5139 - val_accuracy: 0.7471\n",
      "Epoch 275/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5142 - accuracy: 0.7492 - val_loss: 0.5140 - val_accuracy: 0.7475\n",
      "Epoch 276/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5141 - accuracy: 0.7492 - val_loss: 0.5140 - val_accuracy: 0.7464\n",
      "Epoch 277/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5140 - accuracy: 0.7495 - val_loss: 0.5139 - val_accuracy: 0.7467\n",
      "Epoch 278/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5141 - accuracy: 0.7496 - val_loss: 0.5139 - val_accuracy: 0.7473\n",
      "Epoch 279/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5140 - accuracy: 0.7490 - val_loss: 0.5147 - val_accuracy: 0.7486\n",
      "Epoch 280/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5141 - accuracy: 0.7496 - val_loss: 0.5137 - val_accuracy: 0.7468\n",
      "Epoch 281/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5141 - accuracy: 0.7494 - val_loss: 0.5139 - val_accuracy: 0.7468\n",
      "Epoch 282/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5140 - accuracy: 0.7494 - val_loss: 0.5138 - val_accuracy: 0.7467\n",
      "Epoch 283/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5140 - accuracy: 0.7492 - val_loss: 0.5141 - val_accuracy: 0.7458\n",
      "Epoch 284/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5139 - accuracy: 0.7500 - val_loss: 0.5139 - val_accuracy: 0.7476\n",
      "Epoch 285/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5138 - accuracy: 0.7491 - val_loss: 0.5143 - val_accuracy: 0.7479\n",
      "Epoch 286/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5139 - accuracy: 0.7495 - val_loss: 0.5140 - val_accuracy: 0.7473\n",
      "Epoch 287/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5138 - accuracy: 0.7494 - val_loss: 0.5139 - val_accuracy: 0.7473\n",
      "Epoch 288/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5138 - accuracy: 0.7491 - val_loss: 0.5144 - val_accuracy: 0.7480\n",
      "Epoch 289/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5138 - accuracy: 0.7493 - val_loss: 0.5139 - val_accuracy: 0.7471\n",
      "Epoch 290/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5138 - accuracy: 0.7494 - val_loss: 0.5138 - val_accuracy: 0.7484\n",
      "Epoch 291/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5137 - accuracy: 0.7498 - val_loss: 0.5147 - val_accuracy: 0.7483\n",
      "Epoch 292/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5138 - accuracy: 0.7492 - val_loss: 0.5139 - val_accuracy: 0.7459\n",
      "Epoch 293/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5137 - accuracy: 0.7497 - val_loss: 0.5139 - val_accuracy: 0.7482\n",
      "Epoch 294/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5137 - accuracy: 0.7495 - val_loss: 0.5137 - val_accuracy: 0.7469\n",
      "Epoch 295/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5137 - accuracy: 0.7495 - val_loss: 0.5137 - val_accuracy: 0.7467\n",
      "Epoch 296/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5136 - accuracy: 0.7492 - val_loss: 0.5145 - val_accuracy: 0.7485\n",
      "Epoch 297/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5136 - accuracy: 0.7492 - val_loss: 0.5137 - val_accuracy: 0.7466\n",
      "Epoch 298/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5137 - accuracy: 0.7494 - val_loss: 0.5136 - val_accuracy: 0.7476\n",
      "Epoch 299/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5136 - accuracy: 0.7494 - val_loss: 0.5136 - val_accuracy: 0.7470\n",
      "Epoch 300/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5135 - accuracy: 0.7495 - val_loss: 0.5137 - val_accuracy: 0.7468\n",
      "Epoch 301/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5135 - accuracy: 0.7500 - val_loss: 0.5137 - val_accuracy: 0.7470\n",
      "Epoch 302/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5136 - accuracy: 0.7495 - val_loss: 0.5145 - val_accuracy: 0.7484\n",
      "Epoch 303/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5136 - accuracy: 0.7495 - val_loss: 0.5135 - val_accuracy: 0.7472\n",
      "Epoch 304/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5135 - accuracy: 0.7495 - val_loss: 0.5141 - val_accuracy: 0.7479\n",
      "Epoch 305/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5134 - accuracy: 0.7495 - val_loss: 0.5139 - val_accuracy: 0.7477\n",
      "Epoch 306/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5134 - accuracy: 0.7492 - val_loss: 0.5138 - val_accuracy: 0.7468\n",
      "Epoch 307/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5135 - accuracy: 0.7491 - val_loss: 0.5137 - val_accuracy: 0.7476\n",
      "Epoch 308/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5133 - accuracy: 0.7490 - val_loss: 0.5136 - val_accuracy: 0.7478\n",
      "Epoch 309/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5134 - accuracy: 0.7498 - val_loss: 0.5135 - val_accuracy: 0.7474\n",
      "Epoch 310/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5133 - accuracy: 0.7498 - val_loss: 0.5135 - val_accuracy: 0.7470\n",
      "Epoch 311/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5134 - accuracy: 0.7494 - val_loss: 0.5135 - val_accuracy: 0.7466\n",
      "Epoch 312/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5133 - accuracy: 0.7493 - val_loss: 0.5134 - val_accuracy: 0.7471\n",
      "Epoch 313/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5133 - accuracy: 0.7490 - val_loss: 0.5135 - val_accuracy: 0.7466\n",
      "Epoch 314/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5133 - accuracy: 0.7497 - val_loss: 0.5136 - val_accuracy: 0.7484\n",
      "Epoch 315/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5133 - accuracy: 0.7500 - val_loss: 0.5133 - val_accuracy: 0.7462\n",
      "Epoch 316/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5133 - accuracy: 0.7495 - val_loss: 0.5138 - val_accuracy: 0.7486\n",
      "Epoch 317/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5132 - accuracy: 0.7495 - val_loss: 0.5134 - val_accuracy: 0.7472\n",
      "Epoch 318/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5133 - accuracy: 0.7495 - val_loss: 0.5136 - val_accuracy: 0.7471\n",
      "Epoch 319/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5132 - accuracy: 0.7494 - val_loss: 0.5137 - val_accuracy: 0.7484\n",
      "Epoch 320/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5132 - accuracy: 0.7491 - val_loss: 0.5141 - val_accuracy: 0.7486\n",
      "Epoch 321/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5131 - accuracy: 0.7498 - val_loss: 0.5136 - val_accuracy: 0.7465\n",
      "Epoch 322/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5132 - accuracy: 0.7497 - val_loss: 0.5135 - val_accuracy: 0.7479\n",
      "Epoch 323/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5130 - accuracy: 0.7498 - val_loss: 0.5158 - val_accuracy: 0.7479\n",
      "Epoch 324/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5132 - accuracy: 0.7499 - val_loss: 0.5133 - val_accuracy: 0.7474\n",
      "Epoch 325/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5132 - accuracy: 0.7495 - val_loss: 0.5136 - val_accuracy: 0.7461\n",
      "Epoch 326/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5132 - accuracy: 0.7498 - val_loss: 0.5137 - val_accuracy: 0.7481\n",
      "Epoch 327/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5131 - accuracy: 0.7496 - val_loss: 0.5132 - val_accuracy: 0.7474\n",
      "Epoch 328/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5130 - accuracy: 0.7495 - val_loss: 0.5149 - val_accuracy: 0.7474\n",
      "Epoch 329/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5131 - accuracy: 0.7496 - val_loss: 0.5132 - val_accuracy: 0.7469\n",
      "Epoch 330/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5130 - accuracy: 0.7493 - val_loss: 0.5133 - val_accuracy: 0.7480\n",
      "Epoch 331/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5130 - accuracy: 0.7493 - val_loss: 0.5134 - val_accuracy: 0.7473\n",
      "Epoch 332/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5130 - accuracy: 0.7497 - val_loss: 0.5134 - val_accuracy: 0.7479\n",
      "Epoch 333/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5130 - accuracy: 0.7495 - val_loss: 0.5132 - val_accuracy: 0.7471\n",
      "Epoch 334/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5130 - accuracy: 0.7492 - val_loss: 0.5133 - val_accuracy: 0.7476\n",
      "Epoch 335/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5131 - accuracy: 0.7498 - val_loss: 0.5133 - val_accuracy: 0.7475\n",
      "Epoch 336/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5130 - accuracy: 0.7495 - val_loss: 0.5132 - val_accuracy: 0.7471\n",
      "Epoch 337/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5130 - accuracy: 0.7495 - val_loss: 0.5131 - val_accuracy: 0.7482\n",
      "Epoch 338/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5128 - accuracy: 0.7495 - val_loss: 0.5131 - val_accuracy: 0.7480\n",
      "Epoch 339/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5128 - accuracy: 0.7496 - val_loss: 0.5132 - val_accuracy: 0.7476\n",
      "Epoch 340/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5128 - accuracy: 0.7498 - val_loss: 0.5131 - val_accuracy: 0.7481\n",
      "Epoch 341/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5128 - accuracy: 0.7498 - val_loss: 0.5133 - val_accuracy: 0.7474\n",
      "Epoch 342/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5127 - accuracy: 0.7495 - val_loss: 0.5132 - val_accuracy: 0.7475\n",
      "Epoch 343/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5128 - accuracy: 0.7498 - val_loss: 0.5135 - val_accuracy: 0.7481\n",
      "Epoch 344/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5128 - accuracy: 0.7490 - val_loss: 0.5131 - val_accuracy: 0.7470\n",
      "Epoch 345/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5128 - accuracy: 0.7494 - val_loss: 0.5132 - val_accuracy: 0.7474\n",
      "Epoch 346/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5128 - accuracy: 0.7490 - val_loss: 0.5133 - val_accuracy: 0.7486\n",
      "Epoch 347/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5127 - accuracy: 0.7496 - val_loss: 0.5134 - val_accuracy: 0.7482\n",
      "Epoch 348/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5127 - accuracy: 0.7497 - val_loss: 0.5131 - val_accuracy: 0.7481\n",
      "Epoch 349/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.7496 - val_loss: 0.5133 - val_accuracy: 0.7479\n",
      "Epoch 350/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5128 - accuracy: 0.7495 - val_loss: 0.5129 - val_accuracy: 0.7470\n",
      "Epoch 351/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5127 - accuracy: 0.7498 - val_loss: 0.5129 - val_accuracy: 0.7468\n",
      "Epoch 352/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5127 - accuracy: 0.7488 - val_loss: 0.5133 - val_accuracy: 0.7464\n",
      "Epoch 353/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5127 - accuracy: 0.7497 - val_loss: 0.5131 - val_accuracy: 0.7480\n",
      "Epoch 354/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.7498 - val_loss: 0.5132 - val_accuracy: 0.7481\n",
      "Epoch 355/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.7493 - val_loss: 0.5134 - val_accuracy: 0.7484\n",
      "Epoch 356/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.7499 - val_loss: 0.5135 - val_accuracy: 0.7478\n",
      "Epoch 357/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5125 - accuracy: 0.7495 - val_loss: 0.5131 - val_accuracy: 0.7475\n",
      "Epoch 358/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5127 - accuracy: 0.7495 - val_loss: 0.5130 - val_accuracy: 0.7481\n",
      "Epoch 359/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.7498 - val_loss: 0.5130 - val_accuracy: 0.7474\n",
      "Epoch 360/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.7495 - val_loss: 0.5132 - val_accuracy: 0.7479\n",
      "Epoch 361/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.7495 - val_loss: 0.5131 - val_accuracy: 0.7480\n",
      "Epoch 362/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5125 - accuracy: 0.7493 - val_loss: 0.5132 - val_accuracy: 0.7480\n",
      "Epoch 363/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5125 - accuracy: 0.7496 - val_loss: 0.5144 - val_accuracy: 0.7477\n",
      "Epoch 364/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5124 - accuracy: 0.7497 - val_loss: 0.5133 - val_accuracy: 0.7481\n",
      "Epoch 365/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5125 - accuracy: 0.7496 - val_loss: 0.5129 - val_accuracy: 0.7469\n",
      "Epoch 366/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5124 - accuracy: 0.7496 - val_loss: 0.5133 - val_accuracy: 0.7479\n",
      "Epoch 367/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5124 - accuracy: 0.7496 - val_loss: 0.5128 - val_accuracy: 0.7469\n",
      "Epoch 368/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5124 - accuracy: 0.7500 - val_loss: 0.5131 - val_accuracy: 0.7476\n",
      "Epoch 369/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5124 - accuracy: 0.7498 - val_loss: 0.5129 - val_accuracy: 0.7468\n",
      "Epoch 370/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5124 - accuracy: 0.7501 - val_loss: 0.5130 - val_accuracy: 0.7467\n",
      "Epoch 371/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5123 - accuracy: 0.7492 - val_loss: 0.5136 - val_accuracy: 0.7480\n",
      "Epoch 372/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5124 - accuracy: 0.7496 - val_loss: 0.5127 - val_accuracy: 0.7473\n",
      "Epoch 373/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5124 - accuracy: 0.7493 - val_loss: 0.5128 - val_accuracy: 0.7474\n",
      "Epoch 374/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5123 - accuracy: 0.7499 - val_loss: 0.5129 - val_accuracy: 0.7483\n",
      "Epoch 375/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5123 - accuracy: 0.7497 - val_loss: 0.5128 - val_accuracy: 0.7477\n",
      "Epoch 376/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5123 - accuracy: 0.7497 - val_loss: 0.5128 - val_accuracy: 0.7475\n",
      "Epoch 377/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5123 - accuracy: 0.7498 - val_loss: 0.5128 - val_accuracy: 0.7476\n",
      "Epoch 378/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5122 - accuracy: 0.7495 - val_loss: 0.5130 - val_accuracy: 0.7477\n",
      "Epoch 379/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5121 - accuracy: 0.7502 - val_loss: 0.5133 - val_accuracy: 0.7473\n",
      "Epoch 380/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5123 - accuracy: 0.7497 - val_loss: 0.5130 - val_accuracy: 0.7482\n",
      "Epoch 381/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5122 - accuracy: 0.7497 - val_loss: 0.5133 - val_accuracy: 0.7480\n",
      "Epoch 382/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5122 - accuracy: 0.7495 - val_loss: 0.5130 - val_accuracy: 0.7482\n",
      "Epoch 383/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5122 - accuracy: 0.7498 - val_loss: 0.5126 - val_accuracy: 0.7477\n",
      "Epoch 384/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5121 - accuracy: 0.7499 - val_loss: 0.5128 - val_accuracy: 0.7478\n",
      "Epoch 385/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5121 - accuracy: 0.7495 - val_loss: 0.5128 - val_accuracy: 0.7472\n",
      "Epoch 386/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5121 - accuracy: 0.7498 - val_loss: 0.5126 - val_accuracy: 0.7469\n",
      "Epoch 387/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5121 - accuracy: 0.7495 - val_loss: 0.5129 - val_accuracy: 0.7480\n",
      "Epoch 388/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5121 - accuracy: 0.7494 - val_loss: 0.5130 - val_accuracy: 0.7482\n",
      "Epoch 389/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5121 - accuracy: 0.7496 - val_loss: 0.5126 - val_accuracy: 0.7474\n",
      "Epoch 390/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5121 - accuracy: 0.7498 - val_loss: 0.5126 - val_accuracy: 0.7474\n",
      "Epoch 391/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5121 - accuracy: 0.7497 - val_loss: 0.5127 - val_accuracy: 0.7476\n",
      "Epoch 392/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5121 - accuracy: 0.7497 - val_loss: 0.5135 - val_accuracy: 0.7477\n",
      "Epoch 393/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5120 - accuracy: 0.7502 - val_loss: 0.5125 - val_accuracy: 0.7473\n",
      "Epoch 394/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5120 - accuracy: 0.7499 - val_loss: 0.5127 - val_accuracy: 0.7478\n",
      "Epoch 395/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5119 - accuracy: 0.7493 - val_loss: 0.5130 - val_accuracy: 0.7479\n",
      "Epoch 396/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5120 - accuracy: 0.7501 - val_loss: 0.5129 - val_accuracy: 0.7484\n",
      "Epoch 397/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5119 - accuracy: 0.7501 - val_loss: 0.5127 - val_accuracy: 0.7462\n",
      "Epoch 398/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5119 - accuracy: 0.7498 - val_loss: 0.5126 - val_accuracy: 0.7479\n",
      "Epoch 399/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5120 - accuracy: 0.7498 - val_loss: 0.5125 - val_accuracy: 0.7475\n",
      "Epoch 400/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5119 - accuracy: 0.7497 - val_loss: 0.5136 - val_accuracy: 0.7485\n",
      "Epoch 401/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5119 - accuracy: 0.7499 - val_loss: 0.5125 - val_accuracy: 0.7473\n",
      "Epoch 402/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5119 - accuracy: 0.7495 - val_loss: 0.5126 - val_accuracy: 0.7484\n",
      "Epoch 403/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5118 - accuracy: 0.7497 - val_loss: 0.5126 - val_accuracy: 0.7468\n",
      "Epoch 404/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5119 - accuracy: 0.7502 - val_loss: 0.5125 - val_accuracy: 0.7470\n",
      "Epoch 405/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5118 - accuracy: 0.7493 - val_loss: 0.5129 - val_accuracy: 0.7482\n",
      "Epoch 406/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5119 - accuracy: 0.7496 - val_loss: 0.5126 - val_accuracy: 0.7474\n",
      "Epoch 407/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5118 - accuracy: 0.7500 - val_loss: 0.5127 - val_accuracy: 0.7482\n",
      "Epoch 408/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5118 - accuracy: 0.7502 - val_loss: 0.5126 - val_accuracy: 0.7465\n",
      "Epoch 409/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5118 - accuracy: 0.7503 - val_loss: 0.5124 - val_accuracy: 0.7473\n",
      "Epoch 410/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5118 - accuracy: 0.7495 - val_loss: 0.5127 - val_accuracy: 0.7478\n",
      "Epoch 411/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5118 - accuracy: 0.7498 - val_loss: 0.5124 - val_accuracy: 0.7467\n",
      "Epoch 412/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5117 - accuracy: 0.7498 - val_loss: 0.5124 - val_accuracy: 0.7471\n",
      "Epoch 413/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5118 - accuracy: 0.7499 - val_loss: 0.5126 - val_accuracy: 0.7479\n",
      "Epoch 414/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5117 - accuracy: 0.7495 - val_loss: 0.5129 - val_accuracy: 0.7470\n",
      "Epoch 415/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5117 - accuracy: 0.7500 - val_loss: 0.5125 - val_accuracy: 0.7472\n",
      "Epoch 416/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5117 - accuracy: 0.7496 - val_loss: 0.5126 - val_accuracy: 0.7479\n",
      "Epoch 417/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5117 - accuracy: 0.7499 - val_loss: 0.5125 - val_accuracy: 0.7483\n",
      "Epoch 418/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5116 - accuracy: 0.7499 - val_loss: 0.5129 - val_accuracy: 0.7479\n",
      "Epoch 419/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5117 - accuracy: 0.7497 - val_loss: 0.5124 - val_accuracy: 0.7477\n",
      "Epoch 420/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5116 - accuracy: 0.7499 - val_loss: 0.5133 - val_accuracy: 0.7478\n",
      "Epoch 421/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5117 - accuracy: 0.7497 - val_loss: 0.5122 - val_accuracy: 0.7474\n",
      "Epoch 422/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.7500 - val_loss: 0.5122 - val_accuracy: 0.7473\n",
      "Epoch 423/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5116 - accuracy: 0.7495 - val_loss: 0.5124 - val_accuracy: 0.7474\n",
      "Epoch 424/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5116 - accuracy: 0.7500 - val_loss: 0.5124 - val_accuracy: 0.7476\n",
      "Epoch 425/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5116 - accuracy: 0.7501 - val_loss: 0.5123 - val_accuracy: 0.7471\n",
      "Epoch 426/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.7502 - val_loss: 0.5124 - val_accuracy: 0.7474\n",
      "Epoch 427/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.7499 - val_loss: 0.5123 - val_accuracy: 0.7472\n",
      "Epoch 428/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.7499 - val_loss: 0.5124 - val_accuracy: 0.7468\n",
      "Epoch 429/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.7497 - val_loss: 0.5123 - val_accuracy: 0.7473\n",
      "Epoch 430/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.7500 - val_loss: 0.5122 - val_accuracy: 0.7475\n",
      "Epoch 431/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.7500 - val_loss: 0.5124 - val_accuracy: 0.7468\n",
      "Epoch 432/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5114 - accuracy: 0.7497 - val_loss: 0.5128 - val_accuracy: 0.7476\n",
      "Epoch 433/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.7499 - val_loss: 0.5123 - val_accuracy: 0.7474\n",
      "Epoch 434/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5114 - accuracy: 0.7500 - val_loss: 0.5126 - val_accuracy: 0.7480\n",
      "Epoch 435/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5114 - accuracy: 0.7502 - val_loss: 0.5121 - val_accuracy: 0.7470\n",
      "Epoch 436/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5114 - accuracy: 0.7497 - val_loss: 0.5125 - val_accuracy: 0.7477\n",
      "Epoch 437/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5114 - accuracy: 0.7503 - val_loss: 0.5122 - val_accuracy: 0.7477\n",
      "Epoch 438/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.7500 - val_loss: 0.5121 - val_accuracy: 0.7472\n",
      "Epoch 439/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5114 - accuracy: 0.7501 - val_loss: 0.5124 - val_accuracy: 0.7478\n",
      "Epoch 440/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5113 - accuracy: 0.7498 - val_loss: 0.5122 - val_accuracy: 0.7478\n",
      "Epoch 441/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5114 - accuracy: 0.7496 - val_loss: 0.5121 - val_accuracy: 0.7473\n",
      "Epoch 442/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5112 - accuracy: 0.7496 - val_loss: 0.5125 - val_accuracy: 0.7466\n",
      "Epoch 443/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5113 - accuracy: 0.7506 - val_loss: 0.5125 - val_accuracy: 0.7481\n",
      "Epoch 444/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5113 - accuracy: 0.7505 - val_loss: 0.5123 - val_accuracy: 0.7473\n",
      "Epoch 445/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5112 - accuracy: 0.7496 - val_loss: 0.5121 - val_accuracy: 0.7475\n",
      "Epoch 446/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5112 - accuracy: 0.7499 - val_loss: 0.5134 - val_accuracy: 0.7475\n",
      "Epoch 447/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5113 - accuracy: 0.7500 - val_loss: 0.5122 - val_accuracy: 0.7479\n",
      "Epoch 448/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5113 - accuracy: 0.7502 - val_loss: 0.5124 - val_accuracy: 0.7483\n",
      "Epoch 449/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5112 - accuracy: 0.7501 - val_loss: 0.5128 - val_accuracy: 0.7481\n",
      "Epoch 450/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5112 - accuracy: 0.7500 - val_loss: 0.5122 - val_accuracy: 0.7466\n",
      "Epoch 451/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5112 - accuracy: 0.7498 - val_loss: 0.5128 - val_accuracy: 0.7471\n",
      "Epoch 452/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5112 - accuracy: 0.7501 - val_loss: 0.5122 - val_accuracy: 0.7468\n",
      "Epoch 453/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5112 - accuracy: 0.7501 - val_loss: 0.5121 - val_accuracy: 0.7477\n",
      "Epoch 454/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5112 - accuracy: 0.7496 - val_loss: 0.5121 - val_accuracy: 0.7482\n",
      "Epoch 455/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5112 - accuracy: 0.7504 - val_loss: 0.5122 - val_accuracy: 0.7470\n",
      "Epoch 456/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.7504 - val_loss: 0.5122 - val_accuracy: 0.7479\n",
      "Epoch 457/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.7505 - val_loss: 0.5127 - val_accuracy: 0.7464\n",
      "Epoch 458/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5112 - accuracy: 0.7506 - val_loss: 0.5123 - val_accuracy: 0.7482\n",
      "Epoch 459/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.7499 - val_loss: 0.5122 - val_accuracy: 0.7479\n",
      "Epoch 460/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5112 - accuracy: 0.7504 - val_loss: 0.5121 - val_accuracy: 0.7474\n",
      "Epoch 461/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5110 - accuracy: 0.7501 - val_loss: 0.5121 - val_accuracy: 0.7473\n",
      "Epoch 462/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5112 - accuracy: 0.7500 - val_loss: 0.5120 - val_accuracy: 0.7479\n",
      "Epoch 463/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.7497 - val_loss: 0.5124 - val_accuracy: 0.7485\n",
      "Epoch 464/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.7505 - val_loss: 0.5121 - val_accuracy: 0.7479\n",
      "Epoch 465/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5110 - accuracy: 0.7501 - val_loss: 0.5120 - val_accuracy: 0.7479\n",
      "Epoch 466/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.7498 - val_loss: 0.5121 - val_accuracy: 0.7474\n",
      "Epoch 467/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5110 - accuracy: 0.7503 - val_loss: 0.5121 - val_accuracy: 0.7472\n",
      "Epoch 468/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.7505 - val_loss: 0.5121 - val_accuracy: 0.7467\n",
      "Epoch 469/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5110 - accuracy: 0.7502 - val_loss: 0.5120 - val_accuracy: 0.7472\n",
      "Epoch 470/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5110 - accuracy: 0.7503 - val_loss: 0.5122 - val_accuracy: 0.7471\n",
      "Epoch 471/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.7500 - val_loss: 0.5120 - val_accuracy: 0.7467\n",
      "Epoch 472/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5109 - accuracy: 0.7503 - val_loss: 0.5124 - val_accuracy: 0.7480\n",
      "Epoch 473/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5110 - accuracy: 0.7500 - val_loss: 0.5120 - val_accuracy: 0.7474\n",
      "Epoch 474/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5109 - accuracy: 0.7503 - val_loss: 0.5121 - val_accuracy: 0.7473\n",
      "Epoch 475/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5109 - accuracy: 0.7507 - val_loss: 0.5119 - val_accuracy: 0.7472\n",
      "Epoch 476/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5109 - accuracy: 0.7503 - val_loss: 0.5121 - val_accuracy: 0.7478\n",
      "Epoch 477/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5110 - accuracy: 0.7498 - val_loss: 0.5124 - val_accuracy: 0.7481\n",
      "Epoch 478/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5109 - accuracy: 0.7497 - val_loss: 0.5125 - val_accuracy: 0.7478\n",
      "Epoch 479/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7502 - val_loss: 0.5119 - val_accuracy: 0.7476\n",
      "Epoch 480/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5109 - accuracy: 0.7497 - val_loss: 0.5123 - val_accuracy: 0.7478\n",
      "Epoch 481/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5109 - accuracy: 0.7501 - val_loss: 0.5119 - val_accuracy: 0.7480\n",
      "Epoch 482/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5110 - accuracy: 0.7502 - val_loss: 0.5120 - val_accuracy: 0.7468\n",
      "Epoch 483/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7502 - val_loss: 0.5119 - val_accuracy: 0.7466\n",
      "Epoch 484/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7499 - val_loss: 0.5124 - val_accuracy: 0.7483\n",
      "Epoch 485/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7502 - val_loss: 0.5120 - val_accuracy: 0.7474\n",
      "Epoch 486/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7504 - val_loss: 0.5121 - val_accuracy: 0.7468\n",
      "Epoch 487/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7501 - val_loss: 0.5121 - val_accuracy: 0.7475\n",
      "Epoch 488/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7500 - val_loss: 0.5121 - val_accuracy: 0.7478\n",
      "Epoch 489/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7499 - val_loss: 0.5120 - val_accuracy: 0.7475\n",
      "Epoch 490/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5109 - accuracy: 0.7499 - val_loss: 0.5119 - val_accuracy: 0.7474\n",
      "Epoch 491/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5107 - accuracy: 0.7498 - val_loss: 0.5119 - val_accuracy: 0.7475\n",
      "Epoch 492/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7504 - val_loss: 0.5120 - val_accuracy: 0.7468\n",
      "Epoch 493/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5107 - accuracy: 0.7499 - val_loss: 0.5122 - val_accuracy: 0.7479\n",
      "Epoch 494/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5107 - accuracy: 0.7498 - val_loss: 0.5118 - val_accuracy: 0.7472\n",
      "Epoch 495/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5107 - accuracy: 0.7500 - val_loss: 0.5119 - val_accuracy: 0.7471\n",
      "Epoch 496/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7501 - val_loss: 0.5120 - val_accuracy: 0.7471\n",
      "Epoch 497/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5107 - accuracy: 0.7501 - val_loss: 0.5118 - val_accuracy: 0.7476\n",
      "Epoch 498/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5106 - accuracy: 0.7504 - val_loss: 0.5117 - val_accuracy: 0.7477\n",
      "Epoch 499/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7505 - val_loss: 0.5119 - val_accuracy: 0.7480\n",
      "Epoch 500/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5107 - accuracy: 0.7501 - val_loss: 0.5119 - val_accuracy: 0.7470\n",
      "Epoch 501/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5107 - accuracy: 0.7501 - val_loss: 0.5119 - val_accuracy: 0.7480\n",
      "Epoch 502/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5106 - accuracy: 0.7500 - val_loss: 0.5120 - val_accuracy: 0.7482\n",
      "Epoch 503/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5107 - accuracy: 0.7497 - val_loss: 0.5117 - val_accuracy: 0.7479\n",
      "Epoch 504/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5106 - accuracy: 0.7505 - val_loss: 0.5123 - val_accuracy: 0.7476\n",
      "Epoch 505/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5106 - accuracy: 0.7503 - val_loss: 0.5117 - val_accuracy: 0.7476\n",
      "Epoch 506/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5106 - accuracy: 0.7499 - val_loss: 0.5123 - val_accuracy: 0.7478\n",
      "Epoch 507/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5106 - accuracy: 0.7504 - val_loss: 0.5117 - val_accuracy: 0.7480\n",
      "Epoch 508/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5105 - accuracy: 0.7504 - val_loss: 0.5118 - val_accuracy: 0.7466\n",
      "Epoch 509/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5106 - accuracy: 0.7507 - val_loss: 0.5120 - val_accuracy: 0.7475\n",
      "Epoch 510/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5106 - accuracy: 0.7504 - val_loss: 0.5123 - val_accuracy: 0.7481\n",
      "Epoch 511/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5105 - accuracy: 0.7506 - val_loss: 0.5121 - val_accuracy: 0.7482\n",
      "Epoch 512/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5106 - accuracy: 0.7506 - val_loss: 0.5117 - val_accuracy: 0.7471\n",
      "Epoch 513/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5105 - accuracy: 0.7503 - val_loss: 0.5117 - val_accuracy: 0.7480\n",
      "Epoch 514/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5106 - accuracy: 0.7501 - val_loss: 0.5117 - val_accuracy: 0.7478\n",
      "Epoch 515/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5105 - accuracy: 0.7504 - val_loss: 0.5126 - val_accuracy: 0.7480\n",
      "Epoch 516/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5106 - accuracy: 0.7504 - val_loss: 0.5116 - val_accuracy: 0.7472\n",
      "Epoch 517/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5105 - accuracy: 0.7504 - val_loss: 0.5120 - val_accuracy: 0.7466\n",
      "Epoch 518/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5105 - accuracy: 0.7502 - val_loss: 0.5117 - val_accuracy: 0.7475\n",
      "Epoch 519/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5105 - accuracy: 0.7505 - val_loss: 0.5120 - val_accuracy: 0.7475\n",
      "Epoch 520/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5105 - accuracy: 0.7497 - val_loss: 0.5117 - val_accuracy: 0.7479\n",
      "Epoch 521/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5105 - accuracy: 0.7503 - val_loss: 0.5121 - val_accuracy: 0.7480\n",
      "Epoch 522/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5105 - accuracy: 0.7506 - val_loss: 0.5117 - val_accuracy: 0.7483\n",
      "Epoch 523/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5104 - accuracy: 0.7498 - val_loss: 0.5119 - val_accuracy: 0.7478\n",
      "Epoch 524/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5105 - accuracy: 0.7502 - val_loss: 0.5117 - val_accuracy: 0.7475\n",
      "Epoch 525/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5104 - accuracy: 0.7506 - val_loss: 0.5123 - val_accuracy: 0.7474\n",
      "Epoch 526/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5104 - accuracy: 0.7499 - val_loss: 0.5118 - val_accuracy: 0.7480\n",
      "Epoch 527/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5103 - accuracy: 0.7505 - val_loss: 0.5116 - val_accuracy: 0.7477\n",
      "Epoch 528/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5103 - accuracy: 0.7502 - val_loss: 0.5120 - val_accuracy: 0.7465\n",
      "Epoch 529/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5104 - accuracy: 0.7507 - val_loss: 0.5116 - val_accuracy: 0.7474\n",
      "Epoch 530/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5104 - accuracy: 0.7501 - val_loss: 0.5116 - val_accuracy: 0.7474\n",
      "Epoch 531/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5104 - accuracy: 0.7501 - val_loss: 0.5117 - val_accuracy: 0.7466\n",
      "Epoch 532/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5103 - accuracy: 0.7506 - val_loss: 0.5117 - val_accuracy: 0.7474\n",
      "Epoch 533/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5104 - accuracy: 0.7508 - val_loss: 0.5117 - val_accuracy: 0.7476\n",
      "Epoch 534/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5104 - accuracy: 0.7501 - val_loss: 0.5117 - val_accuracy: 0.7467\n",
      "Epoch 535/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5103 - accuracy: 0.7507 - val_loss: 0.5118 - val_accuracy: 0.7476\n",
      "Epoch 536/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5104 - accuracy: 0.7502 - val_loss: 0.5121 - val_accuracy: 0.7479\n",
      "Epoch 537/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5103 - accuracy: 0.7508 - val_loss: 0.5115 - val_accuracy: 0.7477\n",
      "Epoch 538/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5104 - accuracy: 0.7500 - val_loss: 0.5116 - val_accuracy: 0.7479\n",
      "Epoch 539/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5103 - accuracy: 0.7502 - val_loss: 0.5120 - val_accuracy: 0.7472\n",
      "Epoch 540/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5103 - accuracy: 0.7500 - val_loss: 0.5115 - val_accuracy: 0.7473\n",
      "Epoch 541/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5104 - accuracy: 0.7501 - val_loss: 0.5116 - val_accuracy: 0.7472\n",
      "Epoch 542/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7507 - val_loss: 0.5116 - val_accuracy: 0.7480\n",
      "Epoch 543/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5103 - accuracy: 0.7502 - val_loss: 0.5120 - val_accuracy: 0.7480\n",
      "Epoch 544/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5103 - accuracy: 0.7505 - val_loss: 0.5120 - val_accuracy: 0.7464\n",
      "Epoch 545/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7505 - val_loss: 0.5117 - val_accuracy: 0.7477\n",
      "Epoch 546/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5103 - accuracy: 0.7503 - val_loss: 0.5118 - val_accuracy: 0.7475\n",
      "Epoch 547/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5103 - accuracy: 0.7504 - val_loss: 0.5116 - val_accuracy: 0.7468\n",
      "Epoch 548/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7501 - val_loss: 0.5114 - val_accuracy: 0.7467\n",
      "Epoch 549/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7509 - val_loss: 0.5125 - val_accuracy: 0.7482\n",
      "Epoch 550/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5103 - accuracy: 0.7508 - val_loss: 0.5119 - val_accuracy: 0.7476\n",
      "Epoch 551/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5103 - accuracy: 0.7508 - val_loss: 0.5116 - val_accuracy: 0.7480\n",
      "Epoch 552/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7503 - val_loss: 0.5117 - val_accuracy: 0.7477\n",
      "Epoch 553/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7506 - val_loss: 0.5118 - val_accuracy: 0.7482\n",
      "Epoch 554/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5103 - accuracy: 0.7506 - val_loss: 0.5118 - val_accuracy: 0.7468\n",
      "Epoch 555/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7502 - val_loss: 0.5117 - val_accuracy: 0.7475\n",
      "Epoch 556/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7503 - val_loss: 0.5116 - val_accuracy: 0.7475\n",
      "Epoch 557/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7504 - val_loss: 0.5119 - val_accuracy: 0.7479\n",
      "Epoch 558/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7503 - val_loss: 0.5117 - val_accuracy: 0.7474\n",
      "Epoch 559/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7503 - val_loss: 0.5119 - val_accuracy: 0.7470\n",
      "Epoch 560/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7505 - val_loss: 0.5115 - val_accuracy: 0.7471\n",
      "Epoch 561/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7508 - val_loss: 0.5118 - val_accuracy: 0.7473\n",
      "Epoch 562/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7509 - val_loss: 0.5119 - val_accuracy: 0.7477\n",
      "Epoch 563/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7500 - val_loss: 0.5116 - val_accuracy: 0.7477\n",
      "Epoch 564/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7507 - val_loss: 0.5116 - val_accuracy: 0.7476\n",
      "Epoch 565/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7502 - val_loss: 0.5120 - val_accuracy: 0.7482\n",
      "Epoch 566/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7503 - val_loss: 0.5116 - val_accuracy: 0.7473\n",
      "Epoch 567/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7506 - val_loss: 0.5125 - val_accuracy: 0.7480\n",
      "Epoch 568/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7504 - val_loss: 0.5114 - val_accuracy: 0.7479\n",
      "Epoch 569/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7509 - val_loss: 0.5115 - val_accuracy: 0.7480\n",
      "Epoch 570/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7505 - val_loss: 0.5120 - val_accuracy: 0.7478\n",
      "Epoch 571/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.7504 - val_loss: 0.5119 - val_accuracy: 0.7469\n",
      "Epoch 572/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7507 - val_loss: 0.5115 - val_accuracy: 0.7478\n",
      "Epoch 573/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7507 - val_loss: 0.5114 - val_accuracy: 0.7477\n",
      "Epoch 574/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7505 - val_loss: 0.5115 - val_accuracy: 0.7475\n",
      "Epoch 575/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7503 - val_loss: 0.5115 - val_accuracy: 0.7469\n",
      "Epoch 576/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7507 - val_loss: 0.5115 - val_accuracy: 0.7474\n",
      "Epoch 577/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7505 - val_loss: 0.5114 - val_accuracy: 0.7476\n",
      "Epoch 578/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7504 - val_loss: 0.5117 - val_accuracy: 0.7473\n",
      "Epoch 579/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.7503 - val_loss: 0.5115 - val_accuracy: 0.7469\n",
      "Epoch 580/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.7505 - val_loss: 0.5114 - val_accuracy: 0.7482\n",
      "Epoch 581/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7503 - val_loss: 0.5114 - val_accuracy: 0.7475\n",
      "Epoch 582/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7508 - val_loss: 0.5116 - val_accuracy: 0.7479\n",
      "Epoch 583/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7505 - val_loss: 0.5113 - val_accuracy: 0.7476\n",
      "Epoch 584/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.7501 - val_loss: 0.5119 - val_accuracy: 0.7469\n",
      "Epoch 585/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7500 - val_loss: 0.5114 - val_accuracy: 0.7481\n",
      "Epoch 586/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7503 - val_loss: 0.5116 - val_accuracy: 0.7479\n",
      "Epoch 587/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.7503 - val_loss: 0.5112 - val_accuracy: 0.7481\n",
      "Epoch 588/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.7503 - val_loss: 0.5116 - val_accuracy: 0.7477\n",
      "Epoch 589/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.7503 - val_loss: 0.5118 - val_accuracy: 0.7477\n",
      "Epoch 590/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.7505 - val_loss: 0.5117 - val_accuracy: 0.7477\n",
      "Epoch 591/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.7511 - val_loss: 0.5116 - val_accuracy: 0.7480\n",
      "Epoch 592/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.7504 - val_loss: 0.5114 - val_accuracy: 0.7481\n",
      "Epoch 593/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.7502 - val_loss: 0.5113 - val_accuracy: 0.7469\n",
      "Epoch 594/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7510 - val_loss: 0.5115 - val_accuracy: 0.7484\n",
      "Epoch 595/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.7503 - val_loss: 0.5113 - val_accuracy: 0.7477\n",
      "Epoch 596/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.7503 - val_loss: 0.5114 - val_accuracy: 0.7475\n",
      "Epoch 597/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.7501 - val_loss: 0.5117 - val_accuracy: 0.7474\n",
      "Epoch 598/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7505 - val_loss: 0.5112 - val_accuracy: 0.7467\n",
      "Epoch 599/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7505 - val_loss: 0.5114 - val_accuracy: 0.7474\n",
      "Epoch 600/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7499 - val_loss: 0.5119 - val_accuracy: 0.7473\n",
      "Epoch 601/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7509 - val_loss: 0.5114 - val_accuracy: 0.7477\n",
      "Epoch 602/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.7500 - val_loss: 0.5115 - val_accuracy: 0.7473\n",
      "Epoch 603/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7510 - val_loss: 0.5113 - val_accuracy: 0.7476\n",
      "Epoch 604/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7511 - val_loss: 0.5114 - val_accuracy: 0.7478\n",
      "Epoch 605/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7510 - val_loss: 0.5114 - val_accuracy: 0.7475\n",
      "Epoch 606/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7502 - val_loss: 0.5113 - val_accuracy: 0.7476\n",
      "Epoch 607/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7506 - val_loss: 0.5113 - val_accuracy: 0.7478\n",
      "Epoch 608/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7504 - val_loss: 0.5114 - val_accuracy: 0.7475\n",
      "Epoch 609/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7503 - val_loss: 0.5113 - val_accuracy: 0.7479\n",
      "Epoch 610/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7504 - val_loss: 0.5112 - val_accuracy: 0.7477\n",
      "Epoch 611/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7510 - val_loss: 0.5116 - val_accuracy: 0.7478\n",
      "Epoch 612/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7501 - val_loss: 0.5115 - val_accuracy: 0.7477\n",
      "Epoch 613/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7508 - val_loss: 0.5113 - val_accuracy: 0.7476\n",
      "Epoch 614/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7502 - val_loss: 0.5113 - val_accuracy: 0.7476\n",
      "Epoch 615/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7508 - val_loss: 0.5112 - val_accuracy: 0.7475\n",
      "Epoch 616/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7507 - val_loss: 0.5112 - val_accuracy: 0.7479\n",
      "Epoch 617/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5099 - accuracy: 0.7509 - val_loss: 0.5112 - val_accuracy: 0.7480\n",
      "Epoch 618/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7506 - val_loss: 0.5114 - val_accuracy: 0.7476\n",
      "Epoch 619/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7505 - val_loss: 0.5115 - val_accuracy: 0.7477\n",
      "Epoch 620/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7506 - val_loss: 0.5113 - val_accuracy: 0.7478\n",
      "Epoch 621/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7507 - val_loss: 0.5112 - val_accuracy: 0.7471\n",
      "Epoch 622/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7502 - val_loss: 0.5121 - val_accuracy: 0.7480\n",
      "Epoch 623/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7505 - val_loss: 0.5114 - val_accuracy: 0.7475\n",
      "Epoch 624/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7504 - val_loss: 0.5119 - val_accuracy: 0.7485\n",
      "Epoch 625/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5099 - accuracy: 0.7505 - val_loss: 0.5117 - val_accuracy: 0.7474\n",
      "Epoch 626/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7506 - val_loss: 0.5111 - val_accuracy: 0.7480\n",
      "Epoch 627/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7506 - val_loss: 0.5114 - val_accuracy: 0.7473\n",
      "Epoch 628/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7506 - val_loss: 0.5111 - val_accuracy: 0.7475\n",
      "Epoch 629/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7510 - val_loss: 0.5123 - val_accuracy: 0.7488\n",
      "Epoch 630/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7503 - val_loss: 0.5114 - val_accuracy: 0.7475\n",
      "Epoch 631/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7505 - val_loss: 0.5112 - val_accuracy: 0.7480\n",
      "Epoch 632/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7508 - val_loss: 0.5114 - val_accuracy: 0.7478\n",
      "Epoch 633/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7507 - val_loss: 0.5112 - val_accuracy: 0.7479\n",
      "Epoch 634/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7503 - val_loss: 0.5117 - val_accuracy: 0.7477\n",
      "Epoch 635/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7503 - val_loss: 0.5111 - val_accuracy: 0.7479\n",
      "Epoch 636/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7506 - val_loss: 0.5113 - val_accuracy: 0.7477\n",
      "Epoch 637/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7505 - val_loss: 0.5111 - val_accuracy: 0.7477\n",
      "Epoch 638/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7509 - val_loss: 0.5110 - val_accuracy: 0.7477\n",
      "Epoch 639/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7508 - val_loss: 0.5115 - val_accuracy: 0.7475\n",
      "Epoch 640/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7509 - val_loss: 0.5111 - val_accuracy: 0.7482\n",
      "Epoch 641/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7509 - val_loss: 0.5112 - val_accuracy: 0.7472\n",
      "Epoch 642/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7509 - val_loss: 0.5121 - val_accuracy: 0.7485\n",
      "Epoch 643/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7505 - val_loss: 0.5117 - val_accuracy: 0.7485\n",
      "Epoch 644/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7511 - val_loss: 0.5112 - val_accuracy: 0.7473\n",
      "Epoch 645/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7503 - val_loss: 0.5113 - val_accuracy: 0.7474\n",
      "Epoch 646/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7506 - val_loss: 0.5113 - val_accuracy: 0.7480\n",
      "Epoch 647/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7504 - val_loss: 0.5113 - val_accuracy: 0.7479\n",
      "Epoch 648/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7505 - val_loss: 0.5112 - val_accuracy: 0.7473\n",
      "Epoch 649/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7506 - val_loss: 0.5114 - val_accuracy: 0.7472\n",
      "Epoch 650/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7503 - val_loss: 0.5112 - val_accuracy: 0.7476\n",
      "Epoch 651/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7504 - val_loss: 0.5111 - val_accuracy: 0.7477\n",
      "Epoch 652/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7507 - val_loss: 0.5119 - val_accuracy: 0.7486\n",
      "Epoch 653/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7505 - val_loss: 0.5114 - val_accuracy: 0.7479\n",
      "Epoch 654/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7502 - val_loss: 0.5111 - val_accuracy: 0.7479\n",
      "Epoch 655/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7504 - val_loss: 0.5116 - val_accuracy: 0.7476\n",
      "Epoch 656/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7505 - val_loss: 0.5111 - val_accuracy: 0.7476\n",
      "Epoch 657/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7505 - val_loss: 0.5118 - val_accuracy: 0.7485\n",
      "Epoch 658/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7508 - val_loss: 0.5111 - val_accuracy: 0.7474\n",
      "Epoch 659/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7504 - val_loss: 0.5111 - val_accuracy: 0.7467\n",
      "Epoch 660/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7510 - val_loss: 0.5111 - val_accuracy: 0.7477\n",
      "Epoch 661/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7508 - val_loss: 0.5112 - val_accuracy: 0.7480\n",
      "Epoch 662/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7505 - val_loss: 0.5114 - val_accuracy: 0.7481\n",
      "Epoch 663/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7504 - val_loss: 0.5112 - val_accuracy: 0.7475\n",
      "Epoch 664/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.7505 - val_loss: 0.5112 - val_accuracy: 0.7472\n",
      "Epoch 665/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7502 - val_loss: 0.5110 - val_accuracy: 0.7474\n",
      "Epoch 666/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7509 - val_loss: 0.5110 - val_accuracy: 0.7475\n",
      "Epoch 667/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7514 - val_loss: 0.5112 - val_accuracy: 0.7478\n",
      "Epoch 668/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7511 - val_loss: 0.5113 - val_accuracy: 0.7475\n",
      "Epoch 669/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7506 - val_loss: 0.5113 - val_accuracy: 0.7472\n",
      "Epoch 670/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7507 - val_loss: 0.5110 - val_accuracy: 0.7473\n",
      "Epoch 671/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7507 - val_loss: 0.5110 - val_accuracy: 0.7472\n",
      "Epoch 672/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7508 - val_loss: 0.5110 - val_accuracy: 0.7475\n",
      "Epoch 673/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7506 - val_loss: 0.5117 - val_accuracy: 0.7481\n",
      "Epoch 674/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7503 - val_loss: 0.5110 - val_accuracy: 0.7476\n",
      "Epoch 675/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7507 - val_loss: 0.5111 - val_accuracy: 0.7478\n",
      "Epoch 676/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7505 - val_loss: 0.5112 - val_accuracy: 0.7479\n",
      "Epoch 677/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7508 - val_loss: 0.5111 - val_accuracy: 0.7476\n",
      "Epoch 678/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7509 - val_loss: 0.5113 - val_accuracy: 0.7475\n",
      "Epoch 679/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7503 - val_loss: 0.5112 - val_accuracy: 0.7477\n",
      "Epoch 680/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7506 - val_loss: 0.5118 - val_accuracy: 0.7478\n",
      "Epoch 681/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7507 - val_loss: 0.5113 - val_accuracy: 0.7479\n",
      "Epoch 682/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7508 - val_loss: 0.5108 - val_accuracy: 0.7476\n",
      "Epoch 683/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7511 - val_loss: 0.5110 - val_accuracy: 0.7479\n",
      "Epoch 684/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7510 - val_loss: 0.5109 - val_accuracy: 0.7474\n",
      "Epoch 685/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7517 - val_loss: 0.5110 - val_accuracy: 0.7479\n",
      "Epoch 686/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7504 - val_loss: 0.5114 - val_accuracy: 0.7479\n",
      "Epoch 687/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7504 - val_loss: 0.5111 - val_accuracy: 0.7478\n",
      "Epoch 688/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7506 - val_loss: 0.5112 - val_accuracy: 0.7477\n",
      "Epoch 689/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7508 - val_loss: 0.5110 - val_accuracy: 0.7470\n",
      "Epoch 690/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7508 - val_loss: 0.5110 - val_accuracy: 0.7477\n",
      "Epoch 691/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7506 - val_loss: 0.5110 - val_accuracy: 0.7469\n",
      "Epoch 692/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7507 - val_loss: 0.5111 - val_accuracy: 0.7474\n",
      "Epoch 693/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7506 - val_loss: 0.5119 - val_accuracy: 0.7480\n",
      "Epoch 694/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7509 - val_loss: 0.5109 - val_accuracy: 0.7476\n",
      "Epoch 695/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7506 - val_loss: 0.5110 - val_accuracy: 0.7477\n",
      "Epoch 696/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7505 - val_loss: 0.5111 - val_accuracy: 0.7474\n",
      "Epoch 697/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7506 - val_loss: 0.5109 - val_accuracy: 0.7481\n",
      "Epoch 698/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7505 - val_loss: 0.5110 - val_accuracy: 0.7470\n",
      "Epoch 699/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7508 - val_loss: 0.5109 - val_accuracy: 0.7478\n",
      "Epoch 700/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7511 - val_loss: 0.5110 - val_accuracy: 0.7478\n",
      "Epoch 701/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7503 - val_loss: 0.5112 - val_accuracy: 0.7479\n",
      "Epoch 702/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7515 - val_loss: 0.5110 - val_accuracy: 0.7474\n",
      "Epoch 703/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7505 - val_loss: 0.5113 - val_accuracy: 0.7478\n",
      "Epoch 704/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.7508 - val_loss: 0.5110 - val_accuracy: 0.7472\n",
      "Epoch 705/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7505 - val_loss: 0.5116 - val_accuracy: 0.7484\n",
      "Epoch 706/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7506 - val_loss: 0.5113 - val_accuracy: 0.7477\n",
      "Epoch 707/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7505 - val_loss: 0.5113 - val_accuracy: 0.7474\n",
      "Epoch 708/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7506 - val_loss: 0.5113 - val_accuracy: 0.7477\n",
      "Epoch 709/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7504 - val_loss: 0.5110 - val_accuracy: 0.7480\n",
      "Epoch 710/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7503 - val_loss: 0.5109 - val_accuracy: 0.7481\n",
      "Epoch 711/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7507 - val_loss: 0.5109 - val_accuracy: 0.7479\n",
      "Epoch 712/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7509 - val_loss: 0.5109 - val_accuracy: 0.7477\n",
      "Epoch 713/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7510 - val_loss: 0.5112 - val_accuracy: 0.7474\n",
      "Epoch 714/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7509 - val_loss: 0.5111 - val_accuracy: 0.7473\n",
      "Epoch 715/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5093 - accuracy: 0.7509 - val_loss: 0.5111 - val_accuracy: 0.7477\n",
      "Epoch 716/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7509 - val_loss: 0.5114 - val_accuracy: 0.7476\n",
      "Epoch 717/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7512 - val_loss: 0.5112 - val_accuracy: 0.7479\n",
      "Epoch 718/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7506 - val_loss: 0.5113 - val_accuracy: 0.7477\n",
      "Epoch 719/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7506 - val_loss: 0.5115 - val_accuracy: 0.7482\n",
      "Epoch 720/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7513 - val_loss: 0.5116 - val_accuracy: 0.7478\n",
      "Epoch 721/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7509 - val_loss: 0.5109 - val_accuracy: 0.7479\n",
      "Epoch 722/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7507 - val_loss: 0.5109 - val_accuracy: 0.7474\n",
      "Epoch 723/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7510 - val_loss: 0.5117 - val_accuracy: 0.7484\n",
      "Epoch 724/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7513 - val_loss: 0.5110 - val_accuracy: 0.7476\n",
      "Epoch 725/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7507 - val_loss: 0.5109 - val_accuracy: 0.7476\n",
      "Epoch 726/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7512 - val_loss: 0.5110 - val_accuracy: 0.7475\n",
      "Epoch 727/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7507 - val_loss: 0.5109 - val_accuracy: 0.7479\n",
      "Epoch 728/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7508 - val_loss: 0.5112 - val_accuracy: 0.7473\n",
      "Epoch 729/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7507 - val_loss: 0.5109 - val_accuracy: 0.7475\n",
      "Epoch 730/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7506 - val_loss: 0.5109 - val_accuracy: 0.7477\n",
      "Epoch 731/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7504 - val_loss: 0.5108 - val_accuracy: 0.7478\n",
      "Epoch 732/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7507 - val_loss: 0.5108 - val_accuracy: 0.7480\n",
      "Epoch 733/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7514 - val_loss: 0.5110 - val_accuracy: 0.7473\n",
      "Epoch 734/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7510 - val_loss: 0.5116 - val_accuracy: 0.7485\n",
      "Epoch 735/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7507 - val_loss: 0.5110 - val_accuracy: 0.7479\n",
      "Epoch 736/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7505 - val_loss: 0.5110 - val_accuracy: 0.7480\n",
      "Epoch 737/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7507 - val_loss: 0.5107 - val_accuracy: 0.7473\n",
      "Epoch 738/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7505 - val_loss: 0.5108 - val_accuracy: 0.7477\n",
      "Epoch 739/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5093 - accuracy: 0.7510 - val_loss: 0.5109 - val_accuracy: 0.7478\n",
      "Epoch 740/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7506 - val_loss: 0.5110 - val_accuracy: 0.7474\n",
      "Epoch 741/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7505 - val_loss: 0.5109 - val_accuracy: 0.7475\n",
      "Epoch 742/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7510 - val_loss: 0.5111 - val_accuracy: 0.7474\n",
      "Epoch 743/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7508 - val_loss: 0.5110 - val_accuracy: 0.7473\n",
      "Epoch 744/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7508 - val_loss: 0.5110 - val_accuracy: 0.7481\n",
      "Epoch 745/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7508 - val_loss: 0.5113 - val_accuracy: 0.7479\n",
      "Epoch 746/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5091 - accuracy: 0.7510 - val_loss: 0.5116 - val_accuracy: 0.7482\n",
      "Epoch 747/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7509 - val_loss: 0.5110 - val_accuracy: 0.7476\n",
      "Epoch 748/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7507 - val_loss: 0.5108 - val_accuracy: 0.7476\n",
      "Epoch 749/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7510 - val_loss: 0.5108 - val_accuracy: 0.7475\n",
      "Epoch 750/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7508 - val_loss: 0.5111 - val_accuracy: 0.7470\n",
      "Epoch 751/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7508 - val_loss: 0.5111 - val_accuracy: 0.7477\n",
      "Epoch 752/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7509 - val_loss: 0.5108 - val_accuracy: 0.7479\n",
      "Epoch 753/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7505 - val_loss: 0.5108 - val_accuracy: 0.7474\n",
      "Epoch 754/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7514 - val_loss: 0.5109 - val_accuracy: 0.7475\n",
      "Epoch 755/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7510 - val_loss: 0.5109 - val_accuracy: 0.7476\n",
      "Epoch 756/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7510 - val_loss: 0.5108 - val_accuracy: 0.7480\n",
      "Epoch 757/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7507 - val_loss: 0.5110 - val_accuracy: 0.7477\n",
      "Epoch 758/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7510 - val_loss: 0.5108 - val_accuracy: 0.7479\n",
      "Epoch 759/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7507 - val_loss: 0.5108 - val_accuracy: 0.7478\n",
      "Epoch 760/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7509 - val_loss: 0.5110 - val_accuracy: 0.7476\n",
      "Epoch 761/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7511 - val_loss: 0.5108 - val_accuracy: 0.7482\n",
      "Epoch 762/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7504 - val_loss: 0.5110 - val_accuracy: 0.7480\n",
      "Epoch 763/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7509 - val_loss: 0.5107 - val_accuracy: 0.7480\n",
      "Epoch 764/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7509 - val_loss: 0.5112 - val_accuracy: 0.7480\n",
      "Epoch 765/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7506 - val_loss: 0.5109 - val_accuracy: 0.7478\n",
      "Epoch 766/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7507 - val_loss: 0.5108 - val_accuracy: 0.7474\n",
      "Epoch 767/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7508 - val_loss: 0.5107 - val_accuracy: 0.7473\n",
      "Epoch 768/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7512 - val_loss: 0.5110 - val_accuracy: 0.7481\n",
      "Epoch 769/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7505 - val_loss: 0.5108 - val_accuracy: 0.7478\n",
      "Epoch 770/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7508 - val_loss: 0.5108 - val_accuracy: 0.7480\n",
      "Epoch 771/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7511 - val_loss: 0.5108 - val_accuracy: 0.7475\n",
      "Epoch 772/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7509 - val_loss: 0.5111 - val_accuracy: 0.7480\n",
      "Epoch 773/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7509 - val_loss: 0.5113 - val_accuracy: 0.7475\n",
      "Epoch 774/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7512 - val_loss: 0.5111 - val_accuracy: 0.7477\n",
      "Epoch 775/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7510 - val_loss: 0.5107 - val_accuracy: 0.7477\n",
      "Epoch 776/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7507 - val_loss: 0.5111 - val_accuracy: 0.7473\n",
      "Epoch 777/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7507 - val_loss: 0.5111 - val_accuracy: 0.7479\n",
      "Epoch 778/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7508 - val_loss: 0.5108 - val_accuracy: 0.7480\n",
      "Epoch 779/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7506 - val_loss: 0.5108 - val_accuracy: 0.7484\n",
      "Epoch 780/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5092 - accuracy: 0.7510 - val_loss: 0.5108 - val_accuracy: 0.7476\n",
      "Epoch 781/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7508 - val_loss: 0.5114 - val_accuracy: 0.7475\n",
      "Epoch 782/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7509 - val_loss: 0.5108 - val_accuracy: 0.7479\n",
      "Epoch 783/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7511 - val_loss: 0.5108 - val_accuracy: 0.7482\n",
      "Epoch 784/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7511 - val_loss: 0.5108 - val_accuracy: 0.7479\n",
      "Epoch 785/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7514 - val_loss: 0.5110 - val_accuracy: 0.7474\n",
      "Epoch 786/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7507 - val_loss: 0.5107 - val_accuracy: 0.7480\n",
      "Epoch 787/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5091 - accuracy: 0.7507 - val_loss: 0.5120 - val_accuracy: 0.7478\n",
      "Epoch 788/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7506 - val_loss: 0.5120 - val_accuracy: 0.7485\n",
      "Epoch 789/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.7511 - val_loss: 0.5109 - val_accuracy: 0.7483\n",
      "Epoch 790/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7509 - val_loss: 0.5109 - val_accuracy: 0.7480\n",
      "Epoch 791/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7514 - val_loss: 0.5109 - val_accuracy: 0.7477\n",
      "Epoch 792/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7516 - val_loss: 0.5108 - val_accuracy: 0.7477\n",
      "Epoch 793/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7515 - val_loss: 0.5109 - val_accuracy: 0.7478\n",
      "Epoch 794/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7507 - val_loss: 0.5108 - val_accuracy: 0.7481\n",
      "Epoch 795/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7510 - val_loss: 0.5107 - val_accuracy: 0.7481\n",
      "Epoch 796/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5091 - accuracy: 0.7508 - val_loss: 0.5107 - val_accuracy: 0.7480\n",
      "Epoch 797/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5090 - accuracy: 0.7509 - val_loss: 0.5108 - val_accuracy: 0.7473\n",
      "Epoch 798/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7513 - val_loss: 0.5107 - val_accuracy: 0.7482\n",
      "Epoch 799/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7506 - val_loss: 0.5109 - val_accuracy: 0.7473\n",
      "Epoch 800/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7507 - val_loss: 0.5108 - val_accuracy: 0.7471\n",
      "Epoch 801/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7511 - val_loss: 0.5113 - val_accuracy: 0.7476\n",
      "Epoch 802/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7508 - val_loss: 0.5107 - val_accuracy: 0.7477\n",
      "Epoch 803/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7513 - val_loss: 0.5109 - val_accuracy: 0.7476\n",
      "Epoch 804/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7509 - val_loss: 0.5121 - val_accuracy: 0.7481\n",
      "Epoch 805/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7512 - val_loss: 0.5106 - val_accuracy: 0.7477\n",
      "Epoch 806/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7508 - val_loss: 0.5107 - val_accuracy: 0.7474\n",
      "Epoch 807/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7511 - val_loss: 0.5107 - val_accuracy: 0.7480\n",
      "Epoch 808/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7507 - val_loss: 0.5106 - val_accuracy: 0.7482\n",
      "Epoch 809/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7511 - val_loss: 0.5107 - val_accuracy: 0.7474\n",
      "Epoch 810/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7514 - val_loss: 0.5114 - val_accuracy: 0.7476\n",
      "Epoch 811/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7509 - val_loss: 0.5107 - val_accuracy: 0.7478\n",
      "Epoch 812/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7512 - val_loss: 0.5107 - val_accuracy: 0.7482\n",
      "Epoch 813/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7502 - val_loss: 0.5114 - val_accuracy: 0.7475\n",
      "Epoch 814/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7514 - val_loss: 0.5115 - val_accuracy: 0.7471\n",
      "Epoch 815/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7510 - val_loss: 0.5109 - val_accuracy: 0.7482\n",
      "Epoch 816/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7506 - val_loss: 0.5110 - val_accuracy: 0.7475\n",
      "Epoch 817/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7516 - val_loss: 0.5106 - val_accuracy: 0.7480\n",
      "Epoch 818/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7509 - val_loss: 0.5107 - val_accuracy: 0.7476\n",
      "Epoch 819/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7510 - val_loss: 0.5106 - val_accuracy: 0.7475\n",
      "Epoch 820/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7510 - val_loss: 0.5109 - val_accuracy: 0.7470\n",
      "Epoch 821/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7512 - val_loss: 0.5108 - val_accuracy: 0.7477\n",
      "Epoch 822/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7516 - val_loss: 0.5107 - val_accuracy: 0.7473\n",
      "Epoch 823/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7509 - val_loss: 0.5105 - val_accuracy: 0.7478\n",
      "Epoch 824/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7507 - val_loss: 0.5107 - val_accuracy: 0.7476\n",
      "Epoch 825/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7515 - val_loss: 0.5112 - val_accuracy: 0.7478\n",
      "Epoch 826/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7511 - val_loss: 0.5107 - val_accuracy: 0.7476\n",
      "Epoch 827/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7509 - val_loss: 0.5107 - val_accuracy: 0.7478\n",
      "Epoch 828/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7510 - val_loss: 0.5111 - val_accuracy: 0.7475\n",
      "Epoch 829/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7515 - val_loss: 0.5108 - val_accuracy: 0.7475\n",
      "Epoch 830/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7514 - val_loss: 0.5107 - val_accuracy: 0.7481\n",
      "Epoch 831/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7509 - val_loss: 0.5110 - val_accuracy: 0.7484\n",
      "Epoch 832/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7513 - val_loss: 0.5106 - val_accuracy: 0.7479\n",
      "Epoch 833/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7507 - val_loss: 0.5106 - val_accuracy: 0.7477\n",
      "Epoch 834/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7513 - val_loss: 0.5106 - val_accuracy: 0.7480\n",
      "Epoch 835/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7508 - val_loss: 0.5106 - val_accuracy: 0.7477\n",
      "Epoch 836/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7512 - val_loss: 0.5110 - val_accuracy: 0.7473\n",
      "Epoch 837/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7510 - val_loss: 0.5110 - val_accuracy: 0.7474\n",
      "Epoch 838/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7512 - val_loss: 0.5109 - val_accuracy: 0.7477\n",
      "Epoch 839/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7512 - val_loss: 0.5107 - val_accuracy: 0.7476\n",
      "Epoch 840/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7511 - val_loss: 0.5106 - val_accuracy: 0.7478\n",
      "Epoch 841/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7513 - val_loss: 0.5108 - val_accuracy: 0.7476\n",
      "Epoch 842/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7508 - val_loss: 0.5106 - val_accuracy: 0.7478\n",
      "Epoch 843/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7509 - val_loss: 0.5113 - val_accuracy: 0.7469\n",
      "Epoch 844/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7509 - val_loss: 0.5106 - val_accuracy: 0.7478\n",
      "Epoch 845/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7513 - val_loss: 0.5106 - val_accuracy: 0.7474\n",
      "Epoch 846/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7510 - val_loss: 0.5106 - val_accuracy: 0.7479\n",
      "Epoch 847/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7513 - val_loss: 0.5113 - val_accuracy: 0.7477\n",
      "Epoch 848/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7505 - val_loss: 0.5108 - val_accuracy: 0.7477\n",
      "Epoch 849/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7510 - val_loss: 0.5108 - val_accuracy: 0.7477\n",
      "Epoch 850/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7510 - val_loss: 0.5112 - val_accuracy: 0.7469\n",
      "Epoch 851/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7511 - val_loss: 0.5107 - val_accuracy: 0.7479\n",
      "Epoch 852/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7517 - val_loss: 0.5108 - val_accuracy: 0.7477\n",
      "Epoch 853/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7510 - val_loss: 0.5106 - val_accuracy: 0.7480\n",
      "Epoch 854/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7508 - val_loss: 0.5106 - val_accuracy: 0.7485\n",
      "Epoch 855/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7511 - val_loss: 0.5108 - val_accuracy: 0.7475\n",
      "Epoch 856/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7511 - val_loss: 0.5111 - val_accuracy: 0.7477\n",
      "Epoch 857/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7512 - val_loss: 0.5105 - val_accuracy: 0.7479\n",
      "Epoch 858/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7512 - val_loss: 0.5109 - val_accuracy: 0.7474\n",
      "Epoch 859/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7513 - val_loss: 0.5107 - val_accuracy: 0.7479\n",
      "Epoch 860/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7506 - val_loss: 0.5118 - val_accuracy: 0.7471\n",
      "Epoch 861/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7511 - val_loss: 0.5114 - val_accuracy: 0.7478\n",
      "Epoch 862/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7508 - val_loss: 0.5108 - val_accuracy: 0.7473\n",
      "Epoch 863/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7511 - val_loss: 0.5106 - val_accuracy: 0.7481\n",
      "Epoch 864/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7512 - val_loss: 0.5117 - val_accuracy: 0.7476\n",
      "Epoch 865/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7509 - val_loss: 0.5107 - val_accuracy: 0.7478\n",
      "Epoch 866/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7512 - val_loss: 0.5105 - val_accuracy: 0.7478\n",
      "Epoch 867/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5087 - accuracy: 0.7513 - val_loss: 0.5106 - val_accuracy: 0.7482\n",
      "Epoch 868/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5087 - accuracy: 0.7503 - val_loss: 0.5105 - val_accuracy: 0.7480\n",
      "Epoch 869/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7510 - val_loss: 0.5107 - val_accuracy: 0.7475\n",
      "Epoch 870/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5087 - accuracy: 0.7512 - val_loss: 0.5105 - val_accuracy: 0.7477\n",
      "Epoch 871/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7508 - val_loss: 0.5116 - val_accuracy: 0.7482\n",
      "Epoch 872/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7509 - val_loss: 0.5107 - val_accuracy: 0.7484\n",
      "Epoch 873/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7513 - val_loss: 0.5109 - val_accuracy: 0.7482\n",
      "Epoch 874/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7513 - val_loss: 0.5110 - val_accuracy: 0.7475\n",
      "Epoch 875/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7513 - val_loss: 0.5113 - val_accuracy: 0.7483\n",
      "Epoch 876/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7514 - val_loss: 0.5108 - val_accuracy: 0.7475\n",
      "Epoch 877/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7513 - val_loss: 0.5107 - val_accuracy: 0.7477\n",
      "Epoch 878/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7514 - val_loss: 0.5106 - val_accuracy: 0.7474\n",
      "Epoch 879/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7510 - val_loss: 0.5105 - val_accuracy: 0.7482\n",
      "Epoch 880/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7514 - val_loss: 0.5107 - val_accuracy: 0.7476\n",
      "Epoch 881/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7505 - val_loss: 0.5106 - val_accuracy: 0.7477\n",
      "Epoch 882/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5087 - accuracy: 0.7509 - val_loss: 0.5105 - val_accuracy: 0.7473\n",
      "Epoch 883/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5087 - accuracy: 0.7511 - val_loss: 0.5109 - val_accuracy: 0.7475\n",
      "Epoch 884/1000\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.5087 - accuracy: 0.7508 - val_loss: 0.5105 - val_accuracy: 0.7475\n",
      "Epoch 885/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7511 - val_loss: 0.5109 - val_accuracy: 0.7475\n",
      "Epoch 886/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7512 - val_loss: 0.5110 - val_accuracy: 0.7474\n",
      "Epoch 887/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7512 - val_loss: 0.5106 - val_accuracy: 0.7476\n",
      "Epoch 888/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7510 - val_loss: 0.5107 - val_accuracy: 0.7477\n",
      "Epoch 889/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7512 - val_loss: 0.5104 - val_accuracy: 0.7482\n",
      "Epoch 890/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7514 - val_loss: 0.5105 - val_accuracy: 0.7474\n",
      "Epoch 891/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7509 - val_loss: 0.5105 - val_accuracy: 0.7478\n",
      "Epoch 892/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7511 - val_loss: 0.5105 - val_accuracy: 0.7476\n",
      "Epoch 893/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7508 - val_loss: 0.5110 - val_accuracy: 0.7479\n",
      "Epoch 894/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7508 - val_loss: 0.5105 - val_accuracy: 0.7475\n",
      "Epoch 895/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7510 - val_loss: 0.5105 - val_accuracy: 0.7478\n",
      "Epoch 896/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7512 - val_loss: 0.5105 - val_accuracy: 0.7478\n",
      "Epoch 897/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7513 - val_loss: 0.5119 - val_accuracy: 0.7485\n",
      "Epoch 898/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7508 - val_loss: 0.5109 - val_accuracy: 0.7471\n",
      "Epoch 899/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7513 - val_loss: 0.5105 - val_accuracy: 0.7481\n",
      "Epoch 900/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7511 - val_loss: 0.5105 - val_accuracy: 0.7474\n",
      "Epoch 901/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7509 - val_loss: 0.5110 - val_accuracy: 0.7477\n",
      "Epoch 902/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7517 - val_loss: 0.5106 - val_accuracy: 0.7477\n",
      "Epoch 903/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7515 - val_loss: 0.5105 - val_accuracy: 0.7482\n",
      "Epoch 904/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7510 - val_loss: 0.5105 - val_accuracy: 0.7478\n",
      "Epoch 905/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7517 - val_loss: 0.5108 - val_accuracy: 0.7475\n",
      "Epoch 906/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7514 - val_loss: 0.5108 - val_accuracy: 0.7474\n",
      "Epoch 907/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7512 - val_loss: 0.5106 - val_accuracy: 0.7472\n",
      "Epoch 908/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7513 - val_loss: 0.5108 - val_accuracy: 0.7474\n",
      "Epoch 909/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7507 - val_loss: 0.5106 - val_accuracy: 0.7477\n",
      "Epoch 910/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7511 - val_loss: 0.5105 - val_accuracy: 0.7475\n",
      "Epoch 911/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7512 - val_loss: 0.5106 - val_accuracy: 0.7478\n",
      "Epoch 912/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7514 - val_loss: 0.5104 - val_accuracy: 0.7476\n",
      "Epoch 913/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7512 - val_loss: 0.5110 - val_accuracy: 0.7477\n",
      "Epoch 914/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7514 - val_loss: 0.5105 - val_accuracy: 0.7475\n",
      "Epoch 915/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7516 - val_loss: 0.5107 - val_accuracy: 0.7474\n",
      "Epoch 916/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7512 - val_loss: 0.5107 - val_accuracy: 0.7475\n",
      "Epoch 917/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7512 - val_loss: 0.5105 - val_accuracy: 0.7477\n",
      "Epoch 918/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7511 - val_loss: 0.5104 - val_accuracy: 0.7483\n",
      "Epoch 919/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7515 - val_loss: 0.5104 - val_accuracy: 0.7474\n",
      "Epoch 920/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7510 - val_loss: 0.5106 - val_accuracy: 0.7479\n",
      "Epoch 921/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7508 - val_loss: 0.5105 - val_accuracy: 0.7474\n",
      "Epoch 922/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7509 - val_loss: 0.5104 - val_accuracy: 0.7479\n",
      "Epoch 923/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7511 - val_loss: 0.5110 - val_accuracy: 0.7478\n",
      "Epoch 924/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7511 - val_loss: 0.5107 - val_accuracy: 0.7478\n",
      "Epoch 925/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7512 - val_loss: 0.5107 - val_accuracy: 0.7474\n",
      "Epoch 926/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7509 - val_loss: 0.5104 - val_accuracy: 0.7476\n",
      "Epoch 927/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7513 - val_loss: 0.5105 - val_accuracy: 0.7476\n",
      "Epoch 928/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7510 - val_loss: 0.5104 - val_accuracy: 0.7476\n",
      "Epoch 929/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7511 - val_loss: 0.5106 - val_accuracy: 0.7478\n",
      "Epoch 930/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7513 - val_loss: 0.5107 - val_accuracy: 0.7475\n",
      "Epoch 931/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7508 - val_loss: 0.5109 - val_accuracy: 0.7477\n",
      "Epoch 932/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7513 - val_loss: 0.5105 - val_accuracy: 0.7477\n",
      "Epoch 933/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7513 - val_loss: 0.5105 - val_accuracy: 0.7479\n",
      "Epoch 934/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7512 - val_loss: 0.5106 - val_accuracy: 0.7474\n",
      "Epoch 935/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7512 - val_loss: 0.5106 - val_accuracy: 0.7480\n",
      "Epoch 936/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7512 - val_loss: 0.5106 - val_accuracy: 0.7469\n",
      "Epoch 937/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7508 - val_loss: 0.5104 - val_accuracy: 0.7475\n",
      "Epoch 938/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7516 - val_loss: 0.5106 - val_accuracy: 0.7473\n",
      "Epoch 939/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5086 - accuracy: 0.7514 - val_loss: 0.5105 - val_accuracy: 0.7476\n",
      "Epoch 940/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7511 - val_loss: 0.5106 - val_accuracy: 0.7473\n",
      "Epoch 941/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7514 - val_loss: 0.5108 - val_accuracy: 0.7475\n",
      "Epoch 942/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7508 - val_loss: 0.5104 - val_accuracy: 0.7480\n",
      "Epoch 943/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7511 - val_loss: 0.5104 - val_accuracy: 0.7476\n",
      "Epoch 944/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7510 - val_loss: 0.5106 - val_accuracy: 0.7476\n",
      "Epoch 945/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7510 - val_loss: 0.5106 - val_accuracy: 0.7477\n",
      "Epoch 946/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7510 - val_loss: 0.5106 - val_accuracy: 0.7476\n",
      "Epoch 947/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7511 - val_loss: 0.5105 - val_accuracy: 0.7480\n",
      "Epoch 948/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7512 - val_loss: 0.5104 - val_accuracy: 0.7481\n",
      "Epoch 949/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7514 - val_loss: 0.5105 - val_accuracy: 0.7482\n",
      "Epoch 950/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7513 - val_loss: 0.5106 - val_accuracy: 0.7472\n",
      "Epoch 951/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7512 - val_loss: 0.5103 - val_accuracy: 0.7481\n",
      "Epoch 952/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7513 - val_loss: 0.5105 - val_accuracy: 0.7478\n",
      "Epoch 953/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7512 - val_loss: 0.5111 - val_accuracy: 0.7486\n",
      "Epoch 954/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7515 - val_loss: 0.5110 - val_accuracy: 0.7485\n",
      "Epoch 955/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7513 - val_loss: 0.5105 - val_accuracy: 0.7479\n",
      "Epoch 956/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7516 - val_loss: 0.5105 - val_accuracy: 0.7478\n",
      "Epoch 957/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7515 - val_loss: 0.5107 - val_accuracy: 0.7479\n",
      "Epoch 958/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7514 - val_loss: 0.5105 - val_accuracy: 0.7475\n",
      "Epoch 959/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7515 - val_loss: 0.5105 - val_accuracy: 0.7474\n",
      "Epoch 960/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7509 - val_loss: 0.5111 - val_accuracy: 0.7483\n",
      "Epoch 961/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7513 - val_loss: 0.5105 - val_accuracy: 0.7483\n",
      "Epoch 962/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7507 - val_loss: 0.5106 - val_accuracy: 0.7480\n",
      "Epoch 963/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7508 - val_loss: 0.5105 - val_accuracy: 0.7480\n",
      "Epoch 964/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7511 - val_loss: 0.5105 - val_accuracy: 0.7482\n",
      "Epoch 965/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7514 - val_loss: 0.5104 - val_accuracy: 0.7480\n",
      "Epoch 966/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7513 - val_loss: 0.5104 - val_accuracy: 0.7476\n",
      "Epoch 967/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.7514 - val_loss: 0.5106 - val_accuracy: 0.7474\n",
      "Epoch 968/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7511 - val_loss: 0.5104 - val_accuracy: 0.7479\n",
      "Epoch 969/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7507 - val_loss: 0.5105 - val_accuracy: 0.7478\n",
      "Epoch 970/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7513 - val_loss: 0.5105 - val_accuracy: 0.7480\n",
      "Epoch 971/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7512 - val_loss: 0.5109 - val_accuracy: 0.7475\n",
      "Epoch 972/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7510 - val_loss: 0.5104 - val_accuracy: 0.7477\n",
      "Epoch 973/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.7517 - val_loss: 0.5107 - val_accuracy: 0.7480\n",
      "Epoch 974/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7513 - val_loss: 0.5104 - val_accuracy: 0.7480\n",
      "Epoch 975/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7513 - val_loss: 0.5107 - val_accuracy: 0.7474\n",
      "Epoch 976/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7512 - val_loss: 0.5104 - val_accuracy: 0.7479\n",
      "Epoch 977/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7508 - val_loss: 0.5105 - val_accuracy: 0.7484\n",
      "Epoch 978/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7511 - val_loss: 0.5108 - val_accuracy: 0.7480\n",
      "Epoch 979/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7519 - val_loss: 0.5103 - val_accuracy: 0.7476\n",
      "Epoch 980/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.7514 - val_loss: 0.5107 - val_accuracy: 0.7481\n",
      "Epoch 981/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.7514 - val_loss: 0.5118 - val_accuracy: 0.7486\n",
      "Epoch 982/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7507 - val_loss: 0.5104 - val_accuracy: 0.7479\n",
      "Epoch 983/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7516 - val_loss: 0.5104 - val_accuracy: 0.7476\n",
      "Epoch 984/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.7512 - val_loss: 0.5104 - val_accuracy: 0.7479\n",
      "Epoch 985/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.7510 - val_loss: 0.5107 - val_accuracy: 0.7479\n",
      "Epoch 986/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7511 - val_loss: 0.5109 - val_accuracy: 0.7480\n",
      "Epoch 987/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7511 - val_loss: 0.5103 - val_accuracy: 0.7481\n",
      "Epoch 988/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7511 - val_loss: 0.5105 - val_accuracy: 0.7477\n",
      "Epoch 989/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7512 - val_loss: 0.5104 - val_accuracy: 0.7479\n",
      "Epoch 990/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5082 - accuracy: 0.7515 - val_loss: 0.5107 - val_accuracy: 0.7474\n",
      "Epoch 991/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.7510 - val_loss: 0.5103 - val_accuracy: 0.7478\n",
      "Epoch 992/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.7512 - val_loss: 0.5105 - val_accuracy: 0.7479\n",
      "Epoch 993/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.7512 - val_loss: 0.5104 - val_accuracy: 0.7481\n",
      "Epoch 994/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.7516 - val_loss: 0.5105 - val_accuracy: 0.7477\n",
      "Epoch 995/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.7510 - val_loss: 0.5106 - val_accuracy: 0.7473\n",
      "Epoch 996/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7511 - val_loss: 0.5103 - val_accuracy: 0.7480\n",
      "Epoch 997/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.7513 - val_loss: 0.5104 - val_accuracy: 0.7481\n",
      "Epoch 998/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.7510 - val_loss: 0.5105 - val_accuracy: 0.7481\n",
      "Epoch 999/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7512 - val_loss: 0.5109 - val_accuracy: 0.7483\n",
      "Epoch 1000/1000\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.7513 - val_loss: 0.5103 - val_accuracy: 0.7479\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3931/3931 [==============================] - 2s 497us/step\n",
      "2622/2622 [==============================] - 1s 489us/step\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Threshold: 0.5\n",
       "```\n",
       "              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.64      0.31      0.42     37388\n",
       "           1       0.76      0.93      0.84     88396\n",
       "\n",
       "    accuracy                           0.74    125784\n",
       "   macro avg       0.70      0.62      0.63    125784\n",
       "weighted avg       0.73      0.74      0.71    125784\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Optimal Threshold: 0.64\n",
       "```\n",
       "              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.53      0.55      0.54     37388\n",
       "           1       0.81      0.80      0.80     88396\n",
       "\n",
       "    accuracy                           0.72    125784\n",
       "   macro avg       0.67      0.67      0.67    125784\n",
       "weighted avg       0.72      0.72      0.72    125784\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/22 19:27:01 WARNING mlflow.tensorflow: Failed to infer model signature: could not sample data to infer model signature: tuple index out of range\n",
      "2023/03/22 19:27:01 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpl96uka3k/model/data/model/assets\n",
      "2023-03-22 19:27:01 INFO     Assets written to: /tmp/tmpl96uka3k/model/data/model/assets\n",
      "2023-03-22 19:27:04 INFO     creating /workspaces/dsm150-2022-oct/cw02/phase_03/mlruns/317142480596307992/ce86a96ed02646feb12b64235b9792ea/artifacts/model/data\n",
      "2023-03-22 19:27:04 INFO     creating /workspaces/dsm150-2022-oct/cw02/phase_03/mlruns/317142480596307992/ce86a96ed02646feb12b64235b9792ea/artifacts/model/data/model\n",
      "2023-03-22 19:27:04 INFO     creating /workspaces/dsm150-2022-oct/cw02/phase_03/mlruns/317142480596307992/ce86a96ed02646feb12b64235b9792ea/artifacts/model/data/model/variables\n",
      "2023-03-22 19:27:04 INFO     creating /workspaces/dsm150-2022-oct/cw02/phase_03/mlruns/317142480596307992/ce86a96ed02646feb12b64235b9792ea/artifacts/model/data/model/assets\n",
      "2023-03-22 19:27:04 INFO     creating /workspaces/dsm150-2022-oct/cw02/phase_03/mlruns/317142480596307992/ce86a96ed02646feb12b64235b9792ea/artifacts/tensorboard_logs/train\n",
      "2023-03-22 19:27:04 INFO     creating /workspaces/dsm150-2022-oct/cw02/phase_03/mlruns/317142480596307992/ce86a96ed02646feb12b64235b9792ea/artifacts/tensorboard_logs/validation\n"
     ]
    }
   ],
   "source": [
    "# log the parameters\n",
    "mlflow.set_tag('model', 'simple-dense-model')\n",
    "mlflow.set_tag('notebook', '03-101_mlflow.ipynb')\n",
    "mt.log_params(simple_model_dataset, feature_list, random_state)\n",
    "\n",
    "# train the model\n",
    "model = mm.train_simple_dense(\n",
    "    dataset=simple_model_dataset,\n",
    "    input_shape=simple_model_shape,\n",
    "    output_shape=simple_model_output_shape,\n",
    "    dense_layer_count=2,\n",
    "    dense_units=1024,\n",
    "    dense_activation='relu',\n",
    "    dense_l1_regulization=1e-5,\n",
    "    dense_l2_regulization=0.0,\n",
    "    dense_dropout=0.0,\n",
    "    train_epochs=1000,\n",
    "    train_batch_size=1000,\n",
    "    train_optimizer=k.optimizers.RMSprop(learning_rate=1e-4),\n",
    "    train_loss='binary_crossentropy',\n",
    "    train_metrics=['accuracy'],\n",
    "    train_class_weight=None)\n",
    "\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycaret",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ccd5d0ac8155e415534e4a3fa63dae6febf91ec88901d75be48b34bb32be8ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
