{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03-202 : Question 5 Model\n",
    "\n",
    "Train a model specifically for question 5, and then use it in conjunction with `simple_monkey` to predict the entire dataset and get a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "import keras as k\n",
    "from keras import optimizers\n",
    "import keras_tuner\n",
    "import keras_tuner as kt\n",
    "\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from competition import data_preparation as dp\n",
    "from competition import feature_engineering as fe\n",
    "from competition import model_data as md\n",
    "from competition import source_data as sd\n",
    "import competition.models.simple_dense as sd_model\n",
    "from competition.models.heatmap_covnet import HeatmapCovnetModel\n",
    "\n",
    "from competition.model_training import mprint, mflush, mclear\n",
    "from competition.predict import PredictionBase, Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-10 07:07:39 INFO     Started\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        handlers=[\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ])\n",
    "\n",
    "logging.info(\"Started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13174211, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>index</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>event_name</th>\n",
       "      <th>name</th>\n",
       "      <th>level</th>\n",
       "      <th>page</th>\n",
       "      <th>room_coor_x</th>\n",
       "      <th>room_coor_y</th>\n",
       "      <th>screen_coor_x</th>\n",
       "      <th>screen_coor_y</th>\n",
       "      <th>hover_duration</th>\n",
       "      <th>text</th>\n",
       "      <th>fqid</th>\n",
       "      <th>room_fqid</th>\n",
       "      <th>text_fqid</th>\n",
       "      <th>fullscreen</th>\n",
       "      <th>hq</th>\n",
       "      <th>music</th>\n",
       "      <th>level_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cutscene_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991394</td>\n",
       "      <td>-159.314682</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>undefined</td>\n",
       "      <td>intro</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.intro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>1</td>\n",
       "      <td>1323</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991394</td>\n",
       "      <td>-159.314682</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Whatcha doing over there, Jo?</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>2</td>\n",
       "      <td>831</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991394</td>\n",
       "      <td>-159.314682</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just talking to Teddy.</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          session_id  index  elapsed_time      event_name   name  level page  \\\n",
       "0  20090312431273200      0             0  cutscene_click  basic      0  NaN   \n",
       "1  20090312431273200      1          1323    person_click  basic      0  NaN   \n",
       "2  20090312431273200      2           831    person_click  basic      0  NaN   \n",
       "\n",
       "   room_coor_x  room_coor_y  screen_coor_x  screen_coor_y  hover_duration  \\\n",
       "0  -413.991394  -159.314682          380.0          494.0             NaN   \n",
       "1  -413.991394  -159.314682          380.0          494.0             NaN   \n",
       "2  -413.991394  -159.314682          380.0          494.0             NaN   \n",
       "\n",
       "                            text    fqid                       room_fqid  \\\n",
       "0                      undefined   intro  tunic.historicalsociety.closet   \n",
       "1  Whatcha doing over there, Jo?  gramps  tunic.historicalsociety.closet   \n",
       "2         Just talking to Teddy.  gramps  tunic.historicalsociety.closet   \n",
       "\n",
       "                                           text_fqid fullscreen   hq music  \\\n",
       "0               tunic.historicalsociety.closet.intro        NaN  NaN   NaN   \n",
       "1  tunic.historicalsociety.closet.gramps.intro_0_...        NaN  NaN   NaN   \n",
       "2  tunic.historicalsociety.closet.gramps.intro_0_...        NaN  NaN   NaN   \n",
       "\n",
       "  level_group  \n",
       "0         0-4  \n",
       "1         0-4  \n",
       "2         0-4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the source training set\n",
    "df_source = sd.read_csv('../data/train.csv.gz',\n",
    "                        compression='gzip',\n",
    "                        dtype=sd.source_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(212022, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200_q1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312433251036_q1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090314121766812_q1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             session_id  correct\n",
       "0  20090312431273200_q1        1\n",
       "1  20090312433251036_q1        0\n",
       "2  20090314121766812_q1        1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the source training labels\n",
    "df_source_labels = sd.read_csv('../data/train_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the main dataset\n",
    "df_source = dp.prepare_main_dataset(df_source,\n",
    "                                    elapsed_time_min_clip=0,\n",
    "                                    elapsed_time_max_clip=3691298)\n",
    "\n",
    "# remove sessions with problems\n",
    "problem_sessions = dp.find_problem_sessions(df_source)\n",
    "df_source = df_source[~df_source['session_id'].isin(problem_sessions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the label dataset\n",
    "df_source_labels = dp.prepare_label_dataset(df_source_labels)\n",
    "\n",
    "# remove sessions with problems\n",
    "df_source_labels = df_source_labels[~df_source_labels['session_id'].isin(problem_sessions)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of duplicating the feature engineering workflow, we will use the same feature dataset created in notebook `03-123`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.read_pickle(\n",
    "    'data/features/03-123.parquet',\n",
    "    compression='gzip')\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_features.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first combine the features with the labels as we will do data selection now based on question number as opposed to to all previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_source_labels.merge(\n",
    "    right=df_features, \n",
    "    on=['session_id', 'level_group'],\n",
    "    how='left')\n",
    "\n",
    "print(df_combined.shape)\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_combined.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that will combine the datasets like we just did above and then return the dataset for the specified question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the function\n",
    "df_question_features, df_question_labels = md.get_question_dataset(features=df_features,\n",
    "                                                                labels=df_source_labels,\n",
    "                                                                question_num=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train, validation and test sets\n",
    "train, val, test = md.select_sessions(\n",
    "    y=df_question_labels,\n",
    "    random_state=random_state,\n",
    "    test_size=0.60,\n",
    "    train_size=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the feature list\n",
    "feature_list = ['elapsed_time_sum', 'elapsed_time_max', 'elapsed_time_min', 'elapsed_time_mean', 'elapsed_time_mode']\n",
    "\n",
    "# create the simple model dataset\n",
    "features_dataset = md.get_feature_dataset(\n",
    "    features=df_question_features,\n",
    "    y=df_question_labels,\n",
    "    feature_list=feature_list,\n",
    "    train=train,\n",
    "    val=val,\n",
    "    test=test,\n",
    "    include_question=True,\n",
    "    expand_question=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the labels for multi-label classification\n",
    "cat_features_dataset = md.labels_to_categorical(features_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the shape of the question only dataset\n",
    "input_data = cat_features_dataset['train']['X']\n",
    "features_dataset_shape = input_data.shape[1]\n",
    "print('features_dataset_shape:', features_dataset_shape)\n",
    "\n",
    "# define the output shape\n",
    "output_data = cat_features_dataset['train']['y']\n",
    "output_shape = output_data.shape[1]\n",
    "print('output_shape', output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flat Features Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure mlflow\n",
    "mlflow.set_experiment(\"question-10-simple\")\n",
    "mlflow.keras.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the hyperparameter object\n",
    "def define_tune_parameters(hp):\n",
    "    hp.Int('dense_layer_count', min_value=1, max_value=6, step=1)\n",
    "    hp.Int('dense_units', min_value=512, max_value=1700, step=32)\n",
    "    hp.Choice('dense_activation', values=['relu', 'tanh', 'LeakyReLU'])\n",
    "    hp.Float('dense_l1_regularization', min_value=0.0, max_value=0.0005, step=0.00001)\n",
    "    hp.Float('dense_l2_regularization', min_value=0.0, max_value=0.001, step=0.0001)\n",
    "    hp.Float('dense_dropout', min_value=0.005, max_value=0.1, step=0.005)\n",
    "    hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4, 1e-5, 1e-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best model\n",
    "for batch_size in [500, 1000, 2000, 3000, 4000]:\n",
    "    for optimizer in [optimizers.Adam, optimizers.RMSprop]:\n",
    "        sd_model.tune_model(\n",
    "            define_tune_parameters=define_tune_parameters,\n",
    "            dataset=cat_features_dataset,\n",
    "            max_trials=50,\n",
    "            input_shape=features_dataset_shape,\n",
    "            output_shape=output_shape,\n",
    "            dense_layer_count='dense_layer_count',\n",
    "            dense_units='dense_units',\n",
    "            dense_activation='dense_activation',\n",
    "            dense_l1_regularization='dense_l1_regularization',\n",
    "            dense_l2_regularization='dense_l2_regularization',\n",
    "            dense_dropout='dense_dropout',\n",
    "            train_epochs=2000,\n",
    "            train_batch_size=batch_size,\n",
    "            train_optimizer=optimizer,\n",
    "            train_learning_rate='learning_rate',\n",
    "            train_loss='categorical_crossentropy',\n",
    "            train_metrics=[tfa.metrics.F1Score(name='f1_score', num_classes=2, threshold=0.5, average='macro')],\n",
    "            train_class_weight=None,\n",
    "            tune_objective='val_f1_score',\n",
    "            tune_direction='max',\n",
    "            tuner_type=kt.tuners.BayesianOptimization,\n",
    "            tune_patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure mlflow\n",
    "mlflow.set_experiment(\"question-10-heatmap\")\n",
    "mlflow.keras.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the heatmap dataset\n",
    "heatmap_dataset = md.get_feature_dataset(\n",
    "    features=df_question_features,\n",
    "    y=df_question_labels,\n",
    "    feature_list=['screen_heatmap_feature'],\n",
    "    train=train,\n",
    "    val=val,\n",
    "    test=test,\n",
    "    include_question=False,\n",
    "    expand_question=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the flat heatmap input shape\n",
    "input_data = heatmap_dataset['train']['X']\n",
    "heatmap_shape = input_data.shape[1], input_data.shape[2], input_data.shape[3]\n",
    "print('heatmap_shape:', heatmap_shape)\n",
    "\n",
    "# get the shape of the question only dataset\n",
    "input_data = cat_features_dataset['train']['X']\n",
    "features_dataset_shape = input_data.shape[1]\n",
    "print('features_dataset_shape:', features_dataset_shape)\n",
    "\n",
    "# define the output shape\n",
    "output_data = cat_features_dataset['train']['y']\n",
    "output_shape = output_data.shape[1]\n",
    "print('output_shape', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the tuner parameters\n",
    "def define_heatmap_tune_parameters(hp):\n",
    "    # add the simple model parameters\n",
    "    define_tune_parameters(hp)\n",
    "\n",
    "    # add the heatmap model parameters\n",
    "    hp.Int('covnet_block_count', min_value=1, max_value=3, step=1)\n",
    "    hp.Choice('covnet_activation', values=['relu', 'tanh', 'LeakyReLU'])\n",
    "    hp.Int('covnet_cov_count', min_value=1, max_value=3, step=1)\n",
    "    hp.Int('covnet_channels', min_value=32, max_value=64, step=16)\n",
    "    hp.Choice('covnet_kernel_size', values=['(3, 3)'])\n",
    "    hp.Choice('covnet_pool_size', values=['(2, 2)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "heatmap_model = HeatmapCovnetModel(\n",
    "    input_shape=features_dataset_shape,\n",
    "    heatmap_shape=heatmap_shape,\n",
    "    output_shape=output_shape,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tfa.metrics.F1Score(name='f1_score', num_classes=2, threshold=0.5, average='macro')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best model\n",
    "for batch_size in [500, 1000, 2000, 3000, 4000]:\n",
    "    for optimizer in [optimizers.Adam, optimizers.RMSprop]:\n",
    "        model = heatmap_model.tune_model(\n",
    "            define_tune_parameters=define_heatmap_tune_parameters,\n",
    "            heatmap_dataset=heatmap_dataset,\n",
    "            feature_dataset=cat_features_dataset,\n",
    "            max_trials=50,\n",
    "            train_epochs=1000,\n",
    "            train_batch_size=batch_size,\n",
    "            train_optimizer=optimizer,\n",
    "            tuner_type=kt.tuners.BayesianOptimization,\n",
    "            tune_objective='val_f1_score',\n",
    "            tune_direction='max',\n",
    "            train_class_weight=None,\n",
    "            tune_patience=10)\n",
    "        \n",
    "        mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model from mlflow\n",
    "model_uri = \"runs:/98db2ce003464d77a6a836c74d6a3b54/model\"\n",
    "q10_model = mlflow.keras.load_model(model_uri)\n",
    "\n",
    "# save the model to disk\n",
    "k.models.save_model(q10_model, \"../data/interim/model_03-203.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 07:08:12.705587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-10 07:08:12.715899: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-10 07:08:12.716013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-10 07:08:12.716501: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-10 07:08:12.717009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-10 07:08:12.717126: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-10 07:08:12.717227: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-10 07:08:13.177792: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-10 07:08:13.178151: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-10 07:08:13.178255: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-10 07:08:13.178471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9694 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:08:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# load the test session ids\n",
    "test = np.load('../data/interim/test_03-202.npy')\n",
    "\n",
    "# load the models\n",
    "q05_model = k.models.load_model('../data/interim/model_03-202.h5')\n",
    "q10_model = k.models.load_model('../data/interim/model_03-203.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all the records in the training set\n",
    "df_test = df_source[df_source.session_id.isin(test)]\n",
    "\n",
    "# select the last record for each session\n",
    "df_test_labels = df_source_labels[df_source_labels.session_id.isin(test)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(Baseline):\n",
    "    \"\"\"\n",
    "    Use the best model for question 5 to predict the correct labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, models:Dict[int, Dict[Any, float]]):\n",
    "        # call the base class constructor\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize the models collection\n",
    "        self.models = {}\n",
    "        self.thresholds = {}\n",
    "\n",
    "        # assign the models\n",
    "        self.models = models\n",
    "\n",
    "    def feature_engineering(self, data:pd.DataFrame, labels:pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        This method is used to perform feature engineering on the data.\n",
    "        \"\"\"\n",
    "        logging.info('Performing feature engineering:')\n",
    "        # create the initial features\n",
    "        df_features = fe.create_initial_features(data, labels)\n",
    "\n",
    "        # add the elapsed time feature to the features dataset\n",
    "        logging.info('\\tAdding elapsed time features...')\n",
    "        df_features = fe.add_elapsed_time_features(\n",
    "            features=df_features,\n",
    "            X=data)\n",
    "\n",
    "        # add the total count features to the features dataset\n",
    "        logging.info('\\tAdding total count features...')\n",
    "        df_features = fe.add_count_total_features(\n",
    "            features=df_features,\n",
    "            X=data)\n",
    "\n",
    "        # add the unique count features to the features dataset\n",
    "        logging.info('\\tAdding unique count features...')\n",
    "        df_features = fe.add_count_unique_features(\n",
    "            features=df_features,\n",
    "            X=data)    \n",
    "\n",
    "        # add the heatmap features to the features dataset\n",
    "        logging.info('\\tAdding heatmap features...')\n",
    "        df_features = fe.add_screen_heatmap_feature(\n",
    "            features=df_features,\n",
    "            X=df_source,\n",
    "            verbose=True)\n",
    "        \n",
    "        return df_features\n",
    "\n",
    "    def create_feature_dataset(self,\n",
    "                               features:pd.DataFrame,\n",
    "                               labels:pd.DataFrame) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        This method is used to create a feature dataset.\n",
    "        \"\"\"   \n",
    "        # create the flat features dataset\n",
    "        features_dataset = md.create_feature_dataset(\n",
    "            df_features=features,\n",
    "            df_source_labels=labels,\n",
    "            session_list=labels.session_id.unique(),\n",
    "            feature_list=['elapsed_time_sum', 'elapsed_time_max', \n",
    "                          'elapsed_time_min', 'elapsed_time_mean', \n",
    "                          'elapsed_time_mode'],\n",
    "            include_question=True,\n",
    "            expand_question=False,\n",
    "            verbose=False)\n",
    "\n",
    "        # create the heatmap features dataset\n",
    "        #logging.info('Creating the heatmap features dataset...')\n",
    "        heatmap_dataset = md.create_feature_dataset(\n",
    "            df_features=features,\n",
    "            df_source_labels=labels,\n",
    "            session_list=labels.session_id.unique(),\n",
    "            feature_list=['screen_heatmap_feature'],\n",
    "            include_question=False,\n",
    "            expand_question=False,\n",
    "            verbose=False)\n",
    "\n",
    "        return [heatmap_dataset, features_dataset]\n",
    "\n",
    "\n",
    "    def predict_question(self, feature_set:List[np.ndarray], question_num:int) -> int:\n",
    "        \"\"\"\n",
    "        Predict the correct answer for the given question.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_set : List[pd.DataFrame]\n",
    "            The list of feature sets for the questions.\n",
    "        question_num : int\n",
    "            The question number to predict.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The predicted answer for the question.\n",
    "        \"\"\"\n",
    "        # if no model is defined for the question, use the base class\n",
    "        model_data = self.models.get(question_num, None)\n",
    "        if model_data is None:\n",
    "            return super().predict_question(feature_set, question_num)\n",
    "\n",
    "        # get the model and threshold\n",
    "        model = model_data['model']\n",
    "        threshold = model_data['threshold']\n",
    "\n",
    "        # use the model for prediction\n",
    "        y_pred_model = model.predict(feature_set, verbose=0)\n",
    "        y_pred_model = (y_pred_model[:, 1] > threshold).astype(int)\n",
    "\n",
    "        return y_pred_model[0]\n",
    "\n",
    "# create the predictor object\n",
    "models = {\n",
    "    5:  {'model': q05_model, 'threshold': 0.52},\n",
    "    10: {'model': q10_model, 'threshold': 0.50}\n",
    "}\n",
    "\n",
    "predictor = Predictor(models)\n",
    "#feature_set = predictor.feature_engineering(df_test, df_test_labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the data labels for the specific question\n",
    "df_q_labels = df_test_labels[df_test_labels.question_num == 10]\n",
    "q_level_groups = df_q_labels.level_group.unique()\n",
    "\n",
    "# select the source data for the question\n",
    "df_q = df_test[df_test.level_group.isin(q_level_groups)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the test data labels\n",
    "y_true = df_q_labels['correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-10 07:09:39 INFO     Predicting the target variable\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2579b8879bac494792e072a17bdd888f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6988 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# perform the predictions\n",
    "base_model:Baseline = Baseline()\n",
    "df_q_baseline = base_model.predict(data=df_q, labels=df_q_labels)\n",
    "y_pred_baseline = df_q_baseline['correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-10 07:24:25 INFO     Performing feature engineering:\n",
      "2023-04-10 07:24:25 INFO     \tAdding elapsed time features...\n",
      "2023-04-10 07:24:26 INFO     \tAdding total count features...\n",
      "2023-04-10 07:24:26 INFO     \tAdding unique count features...\n",
      "2023-04-10 07:24:26 INFO     \tAdding heatmap features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7c84d9b17f45428aa53d6197fbb915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6988 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-10 07:27:55 INFO     Predicting the target variable\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9afda94776e4131a36c810ce00916f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6988 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# perform predictions with the actual models\n",
    "q_predictor:Predictor = Predictor(models={\n",
    "    10: {'model': q10_model, 'threshold': 0.50}\n",
    "})\n",
    "\n",
    "df_q_model = q_predictor.predict(data=df_q, labels=df_q_labels)\n",
    "y_pred_model = df_q_model['correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Baseline\n",
       "```\n",
       "              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.50      1.00      0.67      3512\n",
       "           1       0.00      0.00      0.00      3476\n",
       "\n",
       "    accuracy                           0.50      6988\n",
       "   macro avg       0.25      0.50      0.33      6988\n",
       "weighted avg       0.25      0.50      0.34      6988\n",
       "\n",
       "```\n",
       "#### Model\n",
       "```\n",
       "              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.61      0.59      0.60      3512\n",
       "           1       0.60      0.62      0.61      3476\n",
       "\n",
       "    accuracy                           0.60      6988\n",
       "   macro avg       0.60      0.60      0.60      6988\n",
       "weighted avg       0.60      0.60      0.60      6988\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the classification report\n",
    "mprint('#### Baseline')\n",
    "mprint('```')\n",
    "mprint(classification_report(y_true, y_pred_baseline, zero_division=0))\n",
    "mprint('```')\n",
    "\n",
    "mprint('#### Model')\n",
    "mprint('```')\n",
    "mprint(classification_report(y_true, y_pred_model, zero_division=0))\n",
    "mprint('```')\n",
    "\n",
    "mflush()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the test data labels\n",
    "y_true = df_test_labels['correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-10 08:45:48 INFO     Predicting the target variable\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d22e2c0f0ab44bf8b061fd5254ff634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125784 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# perform the baseline predictions\n",
    "base_model:Baseline = Baseline()\n",
    "df_baseline = base_model.predict(data=df_test, labels=df_test_labels)\n",
    "y_pred_baseline = df_baseline['correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-10 08:53:58 INFO     Performing feature engineering:\n",
      "2023-04-10 08:53:58 INFO     \tAdding elapsed time features...\n",
      "2023-04-10 08:54:00 INFO     \tAdding total count features...\n",
      "2023-04-10 08:54:00 INFO     \tAdding unique count features...\n",
      "2023-04-10 08:54:02 INFO     \tAdding heatmap features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e03a7de75344aa3b110675e456035e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6988 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-10 09:02:36 INFO     Predicting the target variable\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379f1812a5b04e629fd51e459bd51aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125784 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# perform predictions with the model\n",
    "q_predictor:Predictor = Predictor(models={\n",
    "    10: {'model': q10_model, 'threshold': 0.50}\n",
    "})\n",
    "\n",
    "df_model = q_predictor.predict(data=df_test, labels=df_test_labels)\n",
    "y_pred_model = df_model['correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Baseline\n",
       "```\n",
       "              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.52      0.48      0.50     37388\n",
       "           1       0.79      0.81      0.80     88396\n",
       "\n",
       "    accuracy                           0.71    125784\n",
       "   macro avg       0.65      0.65      0.65    125784\n",
       "weighted avg       0.71      0.71      0.71    125784\n",
       "\n",
       "```\n",
       "#### Model\n",
       "```\n",
       "              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.53      0.45      0.48     37388\n",
       "           1       0.78      0.83      0.81     88396\n",
       "\n",
       "    accuracy                           0.72    125784\n",
       "   macro avg       0.66      0.64      0.65    125784\n",
       "weighted avg       0.71      0.72      0.71    125784\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the classification report\n",
    "mprint('#### Baseline')\n",
    "mprint('```')\n",
    "mprint(classification_report(y_true, y_pred_baseline, zero_division=0))\n",
    "mprint('```')\n",
    "\n",
    "mprint('#### Model')\n",
    "mprint('```')\n",
    "mprint(classification_report(y_true, y_pred_model, zero_division=0))\n",
    "mprint('```')\n",
    "\n",
    "mflush()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 and 10 models combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-10 09:21:13 INFO     Performing feature engineering:\n",
      "2023-04-10 09:21:13 INFO     \tAdding elapsed time features...\n",
      "2023-04-10 09:21:15 INFO     \tAdding total count features...\n",
      "2023-04-10 09:21:16 INFO     \tAdding unique count features...\n",
      "2023-04-10 09:21:17 INFO     \tAdding heatmap features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2afe73b5a341e790d4f043f454f706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6988 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-10 09:29:53 INFO     Predicting the target variable\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbee8893d2e74eda92e7de900f614117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125784 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# perform predictions with the model\n",
    "q_predictor:Predictor = Predictor(models={\n",
    "    5:  {'model': q05_model, 'threshold': 0.52},\n",
    "    10: {'model': q10_model, 'threshold': 0.50}\n",
    "})\n",
    "\n",
    "df_model = q_predictor.predict(data=df_test, labels=df_test_labels)\n",
    "y_pred_model = df_model['correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Model\n",
       "```\n",
       "              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.56      0.41      0.47     37388\n",
       "           1       0.77      0.86      0.82     88396\n",
       "\n",
       "    accuracy                           0.73    125784\n",
       "   macro avg       0.67      0.63      0.64    125784\n",
       "weighted avg       0.71      0.73      0.71    125784\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprint('#### Model')\n",
    "mprint('```')\n",
    "mprint(classification_report(y_true, y_pred_model, zero_division=0))\n",
    "mprint('```')\n",
    "\n",
    "mflush()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 macro average now decreased while the accuracy went up bu nearly 0.02. This is a bit frustrating and is due to the fact that the f1-score for the 0 class is now lower. As an experiment we could try an randomly flip some 1s to 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_to_zeros(y_pred:np.ndarray, n):\n",
    "    y_result = y_pred.copy()\n",
    "\n",
    "    # Find the indices of elements with value 1\n",
    "    one_indices = np.where(y_result == 1)[0]\n",
    "\n",
    "    # Check if there are enough ones to flip\n",
    "    if len(one_indices) < n:\n",
    "        raise ValueError(f'There are only {len(one_indices)} ones in the array, but you want to flip {n}.')\n",
    "\n",
    "    # Randomly choose n indices to flip\n",
    "    indices_to_flip = np.random.choice(one_indices, size=n, replace=False)\n",
    "\n",
    "    # Flip the selected indices\n",
    "    y_result[indices_to_flip] = 0\n",
    "\n",
    "    return y_result\n",
    "\n",
    "# # Example usage:\n",
    "# y_example = np.array([0, 1, 1, 0, 1, 1, 0, 0, 1, 1])\n",
    "# n = 3\n",
    "# result = flip_to_zeros(y_example, n)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b43274764647da9c72f7a02241407d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f1 score: 0.643132872241387\n",
      "Best flip sample fraction: 0.0006000000000000001\n"
     ]
    }
   ],
   "source": [
    "df_flips = pd.DataFrame()\n",
    "f1_best = 0\n",
    "best_flip_sample_frac = 0\n",
    "\n",
    "for flip_sample_frac in tqdm(np.arange(0, 0.75, 0.0001)):\n",
    "    # get the flipped predictions\n",
    "    flip_sample_size = int(flip_sample_frac * len(y_pred_model))\n",
    "    y_flipped = flip_to_zeros(y_pred_model.to_numpy(), flip_sample_size)\n",
    "\n",
    "    # calcualte the f1 score\n",
    "    f1 = f1_score(y_true, y_flipped, zero_division=0, average='macro')\n",
    "    if f1 > f1_best:\n",
    "        f1_best = f1\n",
    "        best_flip_sample_frac = flip_sample_frac\n",
    "\n",
    "# display the best f1 score\n",
    "print(f'Best f1 score: {f1_best}')\n",
    "print(f'Best flip sample fraction: {best_flip_sample_frac}')\n",
    "\n",
    "#print(classification_report(y_true, y_flipped, zero_division=0))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This idea was perhaps silly but worth a try. We can attempt in later experiments with class weights and bias some questions to 0s."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycaret",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ccd5d0ac8155e415534e4a3fa63dae6febf91ec88901d75be48b34bb32be8ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
