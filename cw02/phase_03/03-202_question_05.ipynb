{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03-202 : Question 5 Model\n",
    "\n",
    "Train a model specifically for question 5, and then use it in conjunction with `simple_monkey` to predict the entire dataset and get a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "import keras as k\n",
    "from keras import optimizers\n",
    "import keras_tuner\n",
    "import keras_tuner as kt\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from competition import data_preparation as dp\n",
    "from competition import feature_engineering as fe\n",
    "from competition import model_data as md\n",
    "from competition import source_data as sd\n",
    "import competition.models.simple_dense as sd_model\n",
    "from competition.models.heatmap_covnet import HeatmapCovnetModel\n",
    "\n",
    "from competition.model_training import mprint, mflush\n",
    "from competition.predict import PredictionBase, Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-08 21:42:13 INFO     Started\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        handlers=[\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ])\n",
    "\n",
    "logging.info(\"Started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13174211, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>index</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>event_name</th>\n",
       "      <th>name</th>\n",
       "      <th>level</th>\n",
       "      <th>page</th>\n",
       "      <th>room_coor_x</th>\n",
       "      <th>room_coor_y</th>\n",
       "      <th>screen_coor_x</th>\n",
       "      <th>screen_coor_y</th>\n",
       "      <th>hover_duration</th>\n",
       "      <th>text</th>\n",
       "      <th>fqid</th>\n",
       "      <th>room_fqid</th>\n",
       "      <th>text_fqid</th>\n",
       "      <th>fullscreen</th>\n",
       "      <th>hq</th>\n",
       "      <th>music</th>\n",
       "      <th>level_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cutscene_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991394</td>\n",
       "      <td>-159.314682</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>undefined</td>\n",
       "      <td>intro</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.intro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>1</td>\n",
       "      <td>1323</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991394</td>\n",
       "      <td>-159.314682</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Whatcha doing over there, Jo?</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>2</td>\n",
       "      <td>831</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991394</td>\n",
       "      <td>-159.314682</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just talking to Teddy.</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          session_id  index  elapsed_time      event_name   name  level page  \\\n",
       "0  20090312431273200      0             0  cutscene_click  basic      0  NaN   \n",
       "1  20090312431273200      1          1323    person_click  basic      0  NaN   \n",
       "2  20090312431273200      2           831    person_click  basic      0  NaN   \n",
       "\n",
       "   room_coor_x  room_coor_y  screen_coor_x  screen_coor_y  hover_duration  \\\n",
       "0  -413.991394  -159.314682          380.0          494.0             NaN   \n",
       "1  -413.991394  -159.314682          380.0          494.0             NaN   \n",
       "2  -413.991394  -159.314682          380.0          494.0             NaN   \n",
       "\n",
       "                            text    fqid                       room_fqid  \\\n",
       "0                      undefined   intro  tunic.historicalsociety.closet   \n",
       "1  Whatcha doing over there, Jo?  gramps  tunic.historicalsociety.closet   \n",
       "2         Just talking to Teddy.  gramps  tunic.historicalsociety.closet   \n",
       "\n",
       "                                           text_fqid fullscreen   hq music  \\\n",
       "0               tunic.historicalsociety.closet.intro        NaN  NaN   NaN   \n",
       "1  tunic.historicalsociety.closet.gramps.intro_0_...        NaN  NaN   NaN   \n",
       "2  tunic.historicalsociety.closet.gramps.intro_0_...        NaN  NaN   NaN   \n",
       "\n",
       "  level_group  \n",
       "0         0-4  \n",
       "1         0-4  \n",
       "2         0-4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the source training set\n",
    "df_source = sd.read_csv('../data/train.csv.gz',\n",
    "                        compression='gzip',\n",
    "                        dtype=sd.source_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(212022, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200_q1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312433251036_q1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090314121766812_q1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             session_id  correct\n",
       "0  20090312431273200_q1        1\n",
       "1  20090312433251036_q1        0\n",
       "2  20090314121766812_q1        1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the source training labels\n",
    "df_source_labels = sd.read_csv('../data/train_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the main dataset\n",
    "df_source = dp.prepare_main_dataset(df_source,\n",
    "                                    elapsed_time_min_clip=0,\n",
    "                                    elapsed_time_max_clip=3691298)\n",
    "\n",
    "# remove sessions with problems\n",
    "problem_sessions = dp.find_problem_sessions(df_source)\n",
    "df_source = df_source[~df_source['session_id'].isin(problem_sessions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the label dataset\n",
    "df_source_labels = dp.prepare_label_dataset(df_source_labels)\n",
    "\n",
    "# remove sessions with problems\n",
    "df_source_labels = df_source_labels[~df_source_labels['session_id'].isin(problem_sessions)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of duplicating the feature engineering workflow, we will use the same feature dataset created in notebook `03-123`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.read_pickle(\n",
    "    'data/features/03-123.parquet',\n",
    "    compression='gzip')\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_features.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first combine the features with the labels as we will do data selection now based on question number as opposed to to all previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_source_labels.merge(\n",
    "    right=df_features, \n",
    "    on=['session_id', 'level_group'],\n",
    "    how='left')\n",
    "\n",
    "print(df_combined.shape)\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_combined.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that will combine the datasets like we just did above and then return the dataset for the specified question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_dataset(features: pd.DataFrame,\n",
    "                         labels: pd.DataFrame,\n",
    "                         question_num: int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns a dataset containing only the specified question_num.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : pd.DataFrame\n",
    "        The features dataset with prepared and normalized data.\n",
    "    labels : pd.DataFrame\n",
    "        The labels dataset containing the target variable.\n",
    "    question_num : int\n",
    "        The question number to filter on.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[pd.DataFrame, pd.DataFrame]\n",
    "        The filtered features and labels datasets.\n",
    "    \"\"\"\n",
    "    # combine the features and labels datasets\n",
    "    df_combined = labels.merge(\n",
    "        right=features, \n",
    "        on=['session_id', 'level_group'],\n",
    "        how='left')\n",
    "\n",
    "    # filter the combined dataset on the specified question_num\n",
    "    df_question = df_combined[df_combined['question_num'] == question_num]\n",
    "\n",
    "    # convert the \"heatmap\" column to a list\n",
    "    screen_heatmap_feature = pd.DataFrame()\n",
    "    if 'screen_heatmap_feature' in df_question.columns:\n",
    "        logging.info('Temporarily removing the \"screen_heatmap_feature\" column')\n",
    "        screen_heatmap_feature = df_question['screen_heatmap_feature']\n",
    "\n",
    "    # split the combined dataset into features and labels again\n",
    "    df_question_features = df_question \\\n",
    "        .drop(columns=['question_num', 'correct', 'screen_heatmap_feature']) \\\n",
    "        .drop_duplicates()\n",
    "    \n",
    "    # add the heatmap feature to df_question_features\n",
    "    if 'screen_heatmap_feature' in df_question.columns:\n",
    "        logging.info('Adding back the \"screen_heatmap_feature\" column')\n",
    "        df_question_features = df_question_features.join(screen_heatmap_feature, how='left')\n",
    "\n",
    "    df_question_labels = df_question[['session_id', 'question_num', \n",
    "                                     'correct', 'level_group']]        \n",
    "    \n",
    "    # return the filtered features and labels datasets\n",
    "    return df_question_features, df_question_labels\n",
    "\n",
    "# test the function\n",
    "df_question_features, df_question_labels = get_question_dataset(features=df_features,\n",
    "                                                                labels=df_source_labels,\n",
    "                                                                question_num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_question_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train, validation and test sets\n",
    "train, val, test = md.select_sessions(\n",
    "    y=df_question_labels,\n",
    "    random_state=random_state,\n",
    "    test_size=0.60,\n",
    "    train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the test numpy.ndarray to disk\n",
    "np.save('../data/interim/test_03-202.npy', test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the feature list\n",
    "feature_list = ['elapsed_time_sum', 'elapsed_time_max', 'elapsed_time_min', 'elapsed_time_mean', 'elapsed_time_mode']\n",
    "\n",
    "# create the simple model dataset\n",
    "features_dataset = md.get_feature_dataset(\n",
    "    features=df_question_features,\n",
    "    y=df_question_labels,\n",
    "    feature_list=feature_list,\n",
    "    train=train,\n",
    "    val=val,\n",
    "    test=test,\n",
    "    include_question=True,\n",
    "    expand_question=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the labels for multi-label classification\n",
    "cat_features_dataset = md.labels_to_categorical(features_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the shape of the question only dataset\n",
    "input_data = cat_features_dataset['train']['X']\n",
    "features_dataset_shape = input_data.shape[1]\n",
    "print('features_dataset_shape:', features_dataset_shape)\n",
    "\n",
    "# define the output shape\n",
    "output_data = cat_features_dataset['train']['y']\n",
    "output_shape = output_data.shape[1]\n",
    "print('output_shape', output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flat Features Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure mlflow\n",
    "mlflow.set_experiment(\"question-05-simple\")\n",
    "mlflow.keras.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the hyperparameter object\n",
    "def define_tune_parameters(hp):\n",
    "    hp.Int('dense_layer_count', min_value=1, max_value=6, step=1)\n",
    "    hp.Int('dense_units', min_value=512, max_value=1700, step=32)\n",
    "    hp.Choice('dense_activation', values=['relu', 'tanh', 'LeakyReLU'])\n",
    "    hp.Float('dense_l1_regularization', min_value=0.0, max_value=0.0005, step=0.00001)\n",
    "    hp.Float('dense_l2_regularization', min_value=0.0, max_value=0.001, step=0.0001)\n",
    "    hp.Float('dense_dropout', min_value=0.005, max_value=0.1, step=0.005)\n",
    "    hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4, 1e-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best model\n",
    "model = sd_model.tune_model(\n",
    "    define_tune_parameters=define_tune_parameters,\n",
    "    dataset=cat_features_dataset,\n",
    "    max_trials=100,\n",
    "    input_shape=features_dataset_shape,\n",
    "    output_shape=output_shape,\n",
    "    dense_layer_count='dense_layer_count',\n",
    "    dense_units='dense_units',\n",
    "    dense_activation='dense_activation',\n",
    "    dense_l1_regularization='dense_l1_regularization',\n",
    "    dense_l2_regularization='dense_l2_regularization',\n",
    "    dense_dropout='dense_dropout',\n",
    "    train_epochs=2000,\n",
    "    train_batch_size=4000,\n",
    "    train_optimizer=optimizers.Adam,\n",
    "    train_learning_rate='learning_rate',\n",
    "    train_loss='categorical_crossentropy',\n",
    "    train_metrics=[tfa.metrics.F1Score(name='f1_score', num_classes=2, threshold=0.5, average='macro')],\n",
    "    train_class_weight=None,\n",
    "    tune_objective='val_f1_score',\n",
    "    tune_direction='max',\n",
    "    tuner_type=kt.tuners.BayesianOptimization)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure mlflow\n",
    "mlflow.set_experiment(\"question-05-heatmap\")\n",
    "mlflow.keras.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the heatmap dataset\n",
    "heatmap_dataset = md.get_feature_dataset(\n",
    "    features=df_question_features,\n",
    "    y=df_question_labels,\n",
    "    feature_list=['screen_heatmap_feature'],\n",
    "    train=train,\n",
    "    val=val,\n",
    "    test=test,\n",
    "    include_question=False,\n",
    "    expand_question=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the flat heatmap input shape\n",
    "input_data = heatmap_dataset['train']['X']\n",
    "heatmap_shape = input_data.shape[1], input_data.shape[2], input_data.shape[3]\n",
    "print('heatmap_shape:', heatmap_shape)\n",
    "\n",
    "# get the shape of the question only dataset\n",
    "input_data = cat_features_dataset['train']['X']\n",
    "features_dataset_shape = input_data.shape[1]\n",
    "print('features_dataset_shape:', features_dataset_shape)\n",
    "\n",
    "# define the output shape\n",
    "output_data = cat_features_dataset['train']['y']\n",
    "output_shape = output_data.shape[1]\n",
    "print('output_shape', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the tuner parameters\n",
    "def define_heatmap_tune_parameters(hp):\n",
    "    # add the simple model parameters\n",
    "    define_tune_parameters(hp)\n",
    "\n",
    "    # add the heatmap model parameters\n",
    "    hp.Int('covnet_block_count', min_value=1, max_value=3, step=1)\n",
    "    hp.Choice('covnet_activation', values=['relu', 'tanh', 'LeakyReLU'])\n",
    "    hp.Int('covnet_cov_count', min_value=1, max_value=3, step=1)\n",
    "    hp.Int('covnet_channels', min_value=32, max_value=64, step=16)\n",
    "    hp.Choice('covnet_kernel_size', values=['(3, 3)'])\n",
    "    hp.Choice('covnet_pool_size', values=['(2, 2)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "heatmap_model = HeatmapCovnetModel(\n",
    "    input_shape=features_dataset_shape,\n",
    "    heatmap_shape=heatmap_shape,\n",
    "    output_shape=output_shape,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tfa.metrics.F1Score(name='f1_score', num_classes=2, threshold=0.5, average='macro')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for the best model\n",
    "heatmap_model.tune_model(\n",
    "    define_tune_parameters=define_heatmap_tune_parameters,\n",
    "    heatmap_dataset=heatmap_dataset,\n",
    "    feature_dataset=cat_features_dataset,\n",
    "    max_trials=100,\n",
    "    train_epochs=1000,\n",
    "    train_batch_size=4000,\n",
    "    train_optimizer=optimizers.Adam,\n",
    "    tuner_type=kt.tuners.BayesianOptimization,\n",
    "    tune_objective='val_f1_score',\n",
    "    tune_direction='max',\n",
    "    train_class_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model from mlflow\n",
    "model_uri = \"runs:/c17ac7db56ea455e867a93e76e473270/model\"\n",
    "q5_model = mlflow.keras.load_model(model_uri)\n",
    "\n",
    "# save the model to disk\n",
    "k.models.save_model(q5_model, \"../data/interim/model_03-202.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the test session ids\n",
    "test = np.load('../data/interim/test_03-202.npy')\n",
    "\n",
    "# load the model\n",
    "q5_model = k.models.load_model('../data/interim/model_03-202.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all the records in the training set\n",
    "df_test = df_source[df_source.session_id.isin(test)]\n",
    "\n",
    "# select the last record for each session\n",
    "df_test_labels = df_source_labels[df_source_labels.session_id.isin(test)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(Baseline):\n",
    "    \"\"\"\n",
    "    Use the best model for question 5 to predict the correct labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_q5, threshold_q5):\n",
    "        # call the base class constructor\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize the models collection\n",
    "        self.models = {}\n",
    "        self.thresholds = {}\n",
    "\n",
    "        # add the model to the collection\n",
    "        self.models[5] = model_q5\n",
    "        self.thresholds[5] = threshold_q5\n",
    "\n",
    "    def feature_engineering(self, data:pd.DataFrame, labels:pd.DataFrame) -> List[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        This method is used to perform feature engineering on the data.\n",
    "        \"\"\"\n",
    "        # create the initial features\n",
    "        df_features = fe.create_initial_features(data, labels)\n",
    "\n",
    "        # add the elapsed time feature to the features dataset\n",
    "        df_features = fe.add_elapsed_time_features(\n",
    "            features=df_features,\n",
    "            X=data)\n",
    "\n",
    "        # add the total count features to the features dataset\n",
    "        df_features = fe.add_count_total_features(\n",
    "            features=df_features,\n",
    "            X=data)\n",
    "\n",
    "        # add the unique count features to the features dataset\n",
    "        df_features = fe.add_count_unique_features(\n",
    "            features=df_features,\n",
    "            X=data)    \n",
    "\n",
    "        # add the heatmap features to the features dataset\n",
    "        df_features = fe.add_screen_heatmap_feature(\n",
    "            features=df_features,\n",
    "            X=df_source,\n",
    "            verbose=False)\n",
    "\n",
    "        # create the flat features dataset\n",
    "        features_dataset = md.create_feature_dataset(\n",
    "            df_features=df_features,\n",
    "            df_source_labels=labels,\n",
    "            session_list=labels.session_id.unique(),\n",
    "            feature_list=['elapsed_time_sum', 'elapsed_time_max', \n",
    "                          'elapsed_time_min', 'elapsed_time_mean', \n",
    "                          'elapsed_time_mode'],\n",
    "            include_question=True,\n",
    "            expand_question=False,\n",
    "            verbose=False)\n",
    "\n",
    "        # create the heatmap features dataset\n",
    "        heatmap_dataset = md.create_feature_dataset(\n",
    "            df_features=df_features,\n",
    "            df_source_labels=labels,\n",
    "            session_list=labels.session_id.unique(),\n",
    "            feature_list=['screen_heatmap_feature'],\n",
    "            include_question=False,\n",
    "            expand_question=False,\n",
    "            verbose=False)\n",
    "\n",
    "        return [heatmap_dataset, features_dataset]\n",
    "\n",
    "\n",
    "    def predict_question(self, feature_set:List[pd.DataFrame], question_num:int) -> int:\n",
    "        \"\"\"\n",
    "        Predict the correct answer for the given question.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_set : List[pd.DataFrame]\n",
    "            The list of feature sets for the questions.\n",
    "        question_num : int\n",
    "            The question number to predict.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The predicted answer for the question.\n",
    "        \"\"\"\n",
    "        # if no model is defined for the question, use the base class\n",
    "        model = self.models.get(question_num, None)\n",
    "        threshold = self.thresholds.get(question_num, None)\n",
    "\n",
    "        if model is None:\n",
    "            return super().predict_question(feature_set, question_num)\n",
    "\n",
    "        # use the model for prediction\n",
    "        y_pred_model = model.predict(feature_set, verbose=0)\n",
    "        y_pred_model = (y_pred_model[:, 1] > threshold).astype(int)\n",
    "\n",
    "        return y_pred_model[0]\n",
    "\n",
    "# test the class\n",
    "predictor = Predictor(q5_model, 0.52)\n",
    "#feature_set = predictor.feature_engineering(df_test, df_test_labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the data labels for only question 5\n",
    "df_q5_labels = df_test_labels[df_test_labels.question_num == 5]\n",
    "q5_level_groups = df_q5_labels.level_group.unique()\n",
    "\n",
    "# select the source data for question 5\n",
    "df_q5 = df_test[df_test.level_group.isin(q5_level_groups)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the test data labels\n",
    "y_true = df_q5_labels['correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8279fb6d80143f7bd9d346c1c0fb740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6988 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# perform the predictions\n",
    "base_model:Baseline = Baseline()\n",
    "df_q5_baseline = base_model.predict(data=df_q5, labels=df_q5_labels)\n",
    "y_pred_baseline = df_q5_baseline['correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b959537c959e4cdeb4dda585a56fd12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6988 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# perform predictions with the q5 model\n",
    "q5_predictor:Predictor = Predictor(q5_model, 0.52)\n",
    "df_q5_model = q5_predictor.predict(data=df_q5, labels=df_q5_labels)\n",
    "y_pred_model = df_q5_model['correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Baseline\n",
       "```\n",
       "              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.45      1.00      0.62      3157\n",
       "           1       0.00      0.00      0.00      3831\n",
       "\n",
       "    accuracy                           0.45      6988\n",
       "   macro avg       0.23      0.50      0.31      6988\n",
       "weighted avg       0.20      0.45      0.28      6988\n",
       "\n",
       "```\n",
       "#### Model\n",
       "```\n",
       "              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.57      0.55      0.56      3157\n",
       "           1       0.64      0.66      0.65      3831\n",
       "\n",
       "    accuracy                           0.61      6988\n",
       "   macro avg       0.61      0.61      0.61      6988\n",
       "weighted avg       0.61      0.61      0.61      6988\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the classification report\n",
    "mprint('#### Baseline')\n",
    "mprint('```')\n",
    "mprint(classification_report(y_true, y_pred_baseline, zero_division=0))\n",
    "mprint('```')\n",
    "\n",
    "mprint('#### Model')\n",
    "mprint('```')\n",
    "mprint(classification_report(y_true, y_pred_model, zero_division=0))\n",
    "mprint('```')\n",
    "\n",
    "mflush()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the test data labels\n",
    "y_true = df_test_labels['correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9439c54f514b99897dadcec75fe615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125784 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# perform the baseline predictions\n",
    "base_model:Baseline = Baseline()\n",
    "df_baseline = base_model.predict(data=df_test, labels=df_test_labels)\n",
    "y_pred_baseline = df_baseline['correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6d0cfb501b4d24bb8460a345ffa5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125784 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# perform predictions with the model\n",
    "q5_predictor:Predictor = Predictor(q5_model, 0.52)\n",
    "df_model = q5_predictor.predict(data=df_test, labels=df_test_labels)\n",
    "y_pred_model = df_model['correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Baseline\n",
       "```\n",
       "              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.52      0.48      0.50     37388\n",
       "           1       0.79      0.81      0.80     88396\n",
       "\n",
       "    accuracy                           0.71    125784\n",
       "   macro avg       0.65      0.65      0.65    125784\n",
       "weighted avg       0.71      0.71      0.71    125784\n",
       "\n",
       "```\n",
       "#### Model\n",
       "```\n",
       "              precision    recall  f1-score   support\n",
       "\n",
       "           0       0.54      0.45      0.49     37388\n",
       "           1       0.78      0.84      0.81     88396\n",
       "\n",
       "    accuracy                           0.72    125784\n",
       "   macro avg       0.66      0.64      0.65    125784\n",
       "weighted avg       0.71      0.72      0.71    125784\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the classification report\n",
    "mprint('#### Baseline')\n",
    "mprint('```')\n",
    "mprint(classification_report(y_true, y_pred_baseline, zero_division=0))\n",
    "mprint('```')\n",
    "\n",
    "mprint('#### Model')\n",
    "mprint('```')\n",
    "mprint(classification_report(y_true, y_pred_model, zero_division=0))\n",
    "mprint('```')\n",
    "\n",
    "mflush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycaret",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ccd5d0ac8155e415534e4a3fa63dae6febf91ec88901d75be48b34bb32be8ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
