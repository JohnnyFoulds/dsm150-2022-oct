{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Different Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 11:18:39.769651: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-28 11:18:40.252036: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-28 11:18:40.252075: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-28 11:18:40.252081: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import logging\n",
    "import gc\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "from keras import metrics\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13174211, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>event_name</th>\n",
       "      <th>name</th>\n",
       "      <th>level</th>\n",
       "      <th>page</th>\n",
       "      <th>room_coor_x</th>\n",
       "      <th>room_coor_y</th>\n",
       "      <th>screen_coor_x</th>\n",
       "      <th>screen_coor_y</th>\n",
       "      <th>hover_duration</th>\n",
       "      <th>text</th>\n",
       "      <th>fqid</th>\n",
       "      <th>room_fqid</th>\n",
       "      <th>text_fqid</th>\n",
       "      <th>fullscreen</th>\n",
       "      <th>hq</th>\n",
       "      <th>music</th>\n",
       "      <th>level_group</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>0</td>\n",
       "      <td>cutscene_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>undefined</td>\n",
       "      <td>intro</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.intro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>1323</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Whatcha doing over there, Jo?</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>831</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just talking to Teddy.</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              session_id  elapsed_time      event_name   name  level  page  \\\n",
       "index                                                                        \n",
       "0      20090312431273200             0  cutscene_click  basic      0   NaN   \n",
       "1      20090312431273200          1323    person_click  basic      0   NaN   \n",
       "2      20090312431273200           831    person_click  basic      0   NaN   \n",
       "\n",
       "       room_coor_x  room_coor_y  screen_coor_x  screen_coor_y  hover_duration  \\\n",
       "index                                                                           \n",
       "0      -413.991405  -159.314686          380.0          494.0             NaN   \n",
       "1      -413.991405  -159.314686          380.0          494.0             NaN   \n",
       "2      -413.991405  -159.314686          380.0          494.0             NaN   \n",
       "\n",
       "                                text    fqid                       room_fqid  \\\n",
       "index                                                                          \n",
       "0                          undefined   intro  tunic.historicalsociety.closet   \n",
       "1      Whatcha doing over there, Jo?  gramps  tunic.historicalsociety.closet   \n",
       "2             Just talking to Teddy.  gramps  tunic.historicalsociety.closet   \n",
       "\n",
       "                                               text_fqid  fullscreen  hq  \\\n",
       "index                                                                      \n",
       "0                   tunic.historicalsociety.closet.intro         NaN NaN   \n",
       "1      tunic.historicalsociety.closet.gramps.intro_0_...         NaN NaN   \n",
       "2      tunic.historicalsociety.closet.gramps.intro_0_...         NaN NaN   \n",
       "\n",
       "       music level_group  \n",
       "index                     \n",
       "0        NaN         0-4  \n",
       "1        NaN         0-4  \n",
       "2        NaN         0-4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the source training set\n",
    "df_source = pd.read_csv('data/train.csv.gz', compression='gzip', index_col=1)\n",
    "\n",
    "print(df_source.shape)\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_source.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(212022, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200_q1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312433251036_q1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090314121766812_q1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             session_id  correct\n",
       "0  20090312431273200_q1        1\n",
       "1  20090312433251036_q1        0\n",
       "2  20090314121766812_q1        1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the source training labels\n",
    "df_source_labels = pd.read_csv('data/train_labels.csv')\n",
    "\n",
    "print(df_source_labels.shape)\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_source_labels.head(3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_question_to_level_group(question_number):\n",
    "    \"\"\"\n",
    "    Maps the question number to the level group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question_number : int\n",
    "        The question number.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The level group.\n",
    "    \"\"\"\n",
    "    if question_number in [1, 2, 3]:\n",
    "        return '0-4'\n",
    "    elif question_number in [4, 5, 6, 7, 8, 9, 10, 11, 12, 13]:\n",
    "        return '5-12'\n",
    "    elif question_number in [14, 15, 16, 17, 18]:\n",
    "        return '13-22'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def prepare_label_dataset(data : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the label dataset and add columns for the level group \n",
    "    and the question number.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The label dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The prepared label dataset.\n",
    "    \"\"\"\n",
    "    # add the columns to determine the level group\n",
    "    df_labels = data \\\n",
    "        .rename(columns={'session_id': 'id'}) \\\n",
    "        .assign(session_id=lambda df: df['id'].str.split('_').str[0].astype(int)) \\\n",
    "        .assign(question_id=lambda df: df['id'].str.split('_').str[1]) \\\n",
    "        .assign(question_num=lambda df: df['question_id'].str[1:].astype(int)) \\\n",
    "        [['session_id', 'question_num', 'correct']]\n",
    "    \n",
    "    # add the level group column\n",
    "    df_labels['level_group'] = df_labels['question_num'].apply(map_question_to_level_group) \n",
    "batch_size = 64\n",
    "        \n",
    "    return df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_main_dataset(data : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the main dataset by removing duplicates and removing \n",
    "    columns that are not needed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The main dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The prepared main dataset.\n",
    "    \"\"\"\n",
    "    empty_columns = ['fullscreen', 'hq', 'music', 'page', 'hover_duration']\n",
    "\n",
    "    df_main = data \\\n",
    "        .drop_duplicates() \\\n",
    "        .reset_index(drop=True) \\\n",
    "        .drop(empty_columns, axis=1) \\\n",
    "        .drop('text', axis=1)\n",
    "\n",
    "    return df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_dataset(data: pd.DataFrame, standardize_coordinates: bool=True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Vectorizes the dataset for deep learning by one-hot encoding and standardizing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The dataset to prepare.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The vectorized dataset.\n",
    "    \"\"\"\n",
    "    categorical_cols = ['event_name', 'name', 'level', 'fqid', 'room_fqid', 'text_fqid', 'level_group']\n",
    "    numerical_cols = ['elapsed_time']    \n",
    "    coordinates_cols = ['room_coor_x', 'room_coor_y', 'screen_coor_x', 'screen_coor_y']\n",
    "\n",
    "    df_vectorized = data \\\n",
    "        .drop('session_id', axis=1) \\\n",
    "        .fillna(0)\n",
    "\n",
    "    # standardize the numerical variables\n",
    "    df_vectorized[numerical_cols] = (df_vectorized[numerical_cols] - df_vectorized[numerical_cols].mean()) / df_vectorized[numerical_cols].std()\n",
    "\n",
    "    # standardize the coordinates\n",
    "    if standardize_coordinates:\n",
    "        df_vectorized[coordinates_cols] = (df_vectorized[coordinates_cols] - df_vectorized[coordinates_cols].mean()) / df_vectorized[coordinates_cols].std()\n",
    "\n",
    "    # one-hot encode the categorical variables\n",
    "    df_vectorized = pd.get_dummies(df_vectorized, columns=categorical_cols)\n",
    "    \n",
    "    return df_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_matrix(data: pd.DataFrame, vector_columns: list, standardize_coordinates: bool=True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare a sequence matrix from a DataFrame for a specific session and level group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The dataframe containing the data.\n",
    "\n",
    "    vector_columns : list\n",
    "        The columns that should appear in the sequence matrix.\n",
    "\n",
    "    standardize_coordinates : bool, optional\n",
    "        Whether to standardize the coordinates.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The sequence matrix.\n",
    "    \"\"\"\n",
    "    df_sequence_matrix = vectorize_dataset(data, standardize_coordinates=standardize_coordinates)\n",
    "\n",
    "    # add the missing columns\n",
    "    missing_columns = [column for column in vector_columns if column not in df_sequence_matrix.columns]\n",
    "    df_sequence_matrix = pd.concat([\n",
    "        df_sequence_matrix, \n",
    "        pd.DataFrame(columns=missing_columns)], axis=1).fillna(0)\n",
    "\n",
    "    return df_sequence_matrix[vector_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_array(session_data: pd.DataFrame, \n",
    "                        level_group: int,\n",
    "                        event_count:int = 1000) -> np.array:\n",
    "    \"\"\"\n",
    "    Creates a vector array for a specific session and question number.\n",
    "    \"\"\"\n",
    "    # get the data for the session and level group\n",
    "    df_session = session_data.query('level_group == @level_group')\n",
    "\n",
    "    # prepare the sequence matrix\n",
    "    df_sequence_matrix = prepare_sequence_matrix(df_session, vector_columns)\n",
    "\n",
    "    # convert it to a numpy array\n",
    "    vector_array = df_sequence_matrix.to_numpy()\n",
    "\n",
    "    # the array cannot have more events than the event count\n",
    "    vector_array = vector_array[:event_count]\n",
    "\n",
    "    # pad the array with zeros if it has less events than the event count\n",
    "    if vector_array.shape[0] < event_count:\n",
    "        vector_array = np.pad(vector_array, ((0, event_count - vector_array.shape[0]), (0, 0)), 'constant')\n",
    "\n",
    "    del df_session\n",
    "    del df_sequence_matrix\n",
    "\n",
    "    return vector_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X: pd.DataFrame,\n",
    "                   y: pd.DataFrame,\n",
    "                   event_count:int,\n",
    "                   session_list: list) -> Tuple[np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Creates a dataset for a specific set of sessions and question numbers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        The main dataset.\n",
    "\n",
    "    y : pd.DataFrame\n",
    "        The label dataset.\n",
    "\n",
    "    event_count : int\n",
    "        The number of events to include in the dataset.\n",
    "\n",
    "    session_ids : list\n",
    "        The list of session ids.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.array, np.array]\n",
    "        The dataset and the labels.\n",
    "    \"\"\"\n",
    "    X_dataset = []\n",
    "    y_dataset = []\n",
    "\n",
    "    for session_id in session_list:\n",
    "        # get the session labels\n",
    "        df_session_labels = y.query('session_id == @session_id')\n",
    "        df_session = X.query('session_id == @session_id')\n",
    "\n",
    "        # create the level groups\n",
    "        vector_arrays = {\n",
    "            '0-4': create_vector_array(df_session, '0-4', event_count),\n",
    "            '5-12': create_vector_array(df_session, '5-12', event_count),\n",
    "            '13-22': create_vector_array(df_session, '13-22', event_count),\n",
    "        }\n",
    "\n",
    "        # iterate over all the questions answered in the session\n",
    "        for _, row in df_session_labels.iterrows():\n",
    "            question_number = row['question_num']\n",
    "            correct = row['correct']\n",
    "            level_group = map_question_to_level_group(question_number)\n",
    "\n",
    "            # get the vector array\n",
    "            vector_array = vector_arrays[level_group].copy()\n",
    "\n",
    "            # set the question number value\n",
    "            column_index = vector_columns.index(f'question_{question_number}')\n",
    "            #print(f'Question: {question_number}, Column: {column_index}')\n",
    "            vector_array[:, column_index] = 1\n",
    "\n",
    "            # add the vector array to the dataset\n",
    "            X_dataset.append(vector_array)\n",
    "            del vector_array\n",
    "\n",
    "            # add the label to the dataset\n",
    "            y_dataset.append(correct)\n",
    "\n",
    "    X_out = np.array(X_dataset, dtype=np.float64)\n",
    "    y_out = np.array(y_dataset)\n",
    "\n",
    "    return X_out, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_sessions(\n",
    "        y: pd.DataFrame,\n",
    "        sample_size: int,\n",
    "        random_state: int=1337) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Selects a sample of sessions from the dataset.\n",
    "global_train_model(\n",
    "#     model=model,\n",
    "#     epochs=10,\n",
    "#     batch_size=batch_size,\n",
    "#     optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "#     loss=loss,\n",
    "#     metrics=metrics)\n",
    "        The label dataset.\n",
    "    sample_size : int\n",
    "        The number of sessions to select.\n",
    "    random_state : int\n",
    "        The random state to use.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, np.ndarray, np.ndarray]\n",
    "        The selected session ids, the main dataset and the label dataset.\n",
    "    \"\"\"\n",
    "    # select all the unique session ids\n",
    "    all_session_ids = y['session_id'].unique()\n",
    "\n",
    "    # create a sample for testing\n",
    "    session_ids = np.random.choice(all_session_ids, size=sample_size, replace=False)\n",
    "\n",
    "    # split the dataset into train, validation and test sets\n",
    "    train, test = train_test_split(session_ids, test_size=0.3)\n",
    "    test, val = train_test_split(test, test_size=0.5)\n",
    "\n",
    "    # print the number of sessions in each set\n",
    "    print(f'Train: {len(train)}')\n",
    "    print(f'Validation: {len(val)}')\n",
    "    print(f'Test: {len(test)}')\n",
    "\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history: callbacks.History, figsize: Tuple[int, int] = (5, 3)) -> None:\n",
    "    \"\"\"\n",
    "    Plot the loss and validation loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    history : keras.callbacks.History\n",
    "        The history of the model training.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(epochs, history.history['loss'])\n",
    "    \n",
    "    if ('val_loss' in history.history):\n",
    "        plt.plot(epochs, history.history['val_loss'])\n",
    "        plt.legend(['Training loss', 'Validation loss'], loc='upper left')\n",
    "        plt.title('Training and validation loss')\n",
    "    else:\n",
    "        plt.title('Training loss')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(history: callbacks.History, figsize: Tuple[int, int] = (5, 3)) -> None:\n",
    "    \"\"\"\n",
    "    Plot the accuracy and validation accuracy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    history : keras.callbacks.History\n",
    "        The history of the model training.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(epochs, history.history['accuracy'])\n",
    "\n",
    "    if ('val_accuracy' in history.history):\n",
    "        plt.plot(epochs, history.history['val_accuracy'])\n",
    "        plt.legend(['Training acc', 'Validation acc'], loc='upper left')\n",
    "        plt.title('Training and validation accuracy')\n",
    "    else:\n",
    "        plt.title('Training accuracy')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_f1(y_true: np.ndarray, y_score: np.ndarray) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Optimize the F1 score.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        The true labels.\n",
    "    y_score : np.ndarray\n",
    "        The predicted labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[float, float, float]\n",
    "        The optimized threshold, precision, and recall.\n",
    "    \"\"\"\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0\n",
    "    best_precision = 0\n",
    "    best_recall = 0\n",
    "\n",
    "    for threshold in np.arange(0, 1, 0.01):\n",
    "        y_pred = (y_score > threshold).astype(int)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=1)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_precision = precision\n",
    "            best_recall = recall\n",
    "\n",
    "    return best_threshold, best_precision, best_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        model,\n",
    "        X_train: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        X_val : np.ndarray,\n",
    "        y_val: np.ndarray,\n",
    "        epochs: int,\n",
    "        batch_size: int,\n",
    "        optimizer,\n",
    "        loss: str,\n",
    "        metrics: list,\n",
    "        class_weight: dict=None) -> callbacks.History:\n",
    "    \"\"\"\n",
    "    Train the keras model based on the parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : keras.models\n",
    "        The model to train.\n",
    "    X_train : np.ndarray\n",
    "        The training data.\n",
    "    y_train : np.ndarray\n",
    "        The training labels.\n",
    "    X_val : np.ndarray\n",
    "        The validation data.\n",
    "    y_val : np.ndarray\n",
    "        The validation labels.\n",
    "    epochs : int\n",
    "        The number of epochs.\n",
    "    batch_size : int\n",
    "        The batch size.\n",
    "    optimizer : keras.optimizers\n",
    "        The optimizer.\n",
    "    loss : str\n",
    "        The loss function.\n",
    "    metrics : list\n",
    "        The metrics.\n",
    "    class_weight : dict, optional\n",
    "        The class weights, by default None\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    keras.callbacks.History\n",
    "        The history of the training.\n",
    "    \"\"\"\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics)\n",
    "    \n",
    "    # fit the model\n",
    "    history = model.fit(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val),\n",
    "        class_weight=class_weight)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(\n",
    "        model,\n",
    "        history: callbacks.History,\n",
    "        X_test: np.ndarray,\n",
    "        y_test: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Test the model based on the test data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : keras.models\n",
    "        The model to test.\n",
    "    history : keras.callbacks.History\n",
    "        The history of the training.\n",
    "    X_test : np.ndarray\n",
    "        The test data.\n",
    "    y_test : np.ndarray\n",
    "        The test labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The optimized threshold for the best F1 score.\n",
    "    \"\"\"\n",
    "    \n",
    "    plot_loss(history)\n",
    "    plot_accuracy(history)\n",
    "\n",
    "    y_test_score = model.predict(X_test)\n",
    "    threshold, _, _ = optimize_f1(y_test, y_test_score)\n",
    "\n",
    "    print(classification_report(y_test, y_test_score > threshold))\n",
    "    print(f'Optimized threshold for best F1: {threshold:.2f}')\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(\n",
    "        model,\n",
    "        X_train: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        X_val : np.ndarray,\n",
    "        y_val: np.ndarray,\n",
    "        X_test: np.ndarray,\n",
    "        y_test: np.ndarray,\n",
    "        epochs: int,\n",
    "        batch_size: int,\n",
    "        optimizer,\n",
    "        loss: str,\n",
    "        metrics: list,\n",
    "        class_weight: dict=None) -> float:\n",
    "    \"\"\"\n",
    "    Train and test the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : keras.models\n",
    "        The model to train and test.\n",
    "    X_train : np.ndarray\n",
    "        The training data.\n",
    "    y_train : np.ndarray\n",
    "        The training labels.\n",
    "    X_val : np.ndarray\n",
    "        The validation data.\n",
    "    y_val : np.ndarray\n",
    "        The validation labels.\n",
    "    X_test : np.ndarray\n",
    "        The test data.\n",
    "    y_test : np.ndarray\n",
    "        The test labels.\n",
    "    epochs : int\n",
    "        The number of epochs.\n",
    "    batch_size : int\n",
    "        The batch size.\n",
    "    optimizer : keras.optimizers\n",
    "        The optimizer.\n",
    "    loss : str\n",
    "        The loss function.\n",
    "    metrics : list\n",
    "        The metrics.\n",
    "    class_weight : dict, optional\n",
    "        The class weights, by default None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The optimized threshold for the best F1 score.\n",
    "    \"\"\"\n",
    "    history = train_model(\n",
    "        model=model,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,\n",
    "        y_val=y_val,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics,\n",
    "        class_weight=class_weight)\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    return test_model(model, history, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_train_model(model,\n",
    "                       epochs: int,\n",
    "                       batch_size: int,\n",
    "                       optimizer,\n",
    "                       loss: str,\n",
    "                       metrics: list,\n",
    "                       class_weight: dict=None) -> float:\n",
    "        \"\"\"\n",
    "        Train the model using the global data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : keras.models\n",
    "                The model to train.\n",
    "        epochs : int\n",
    "                The number of epochs.\n",
    "        batch_size : int\n",
    "                The batch size.\n",
    "        optimizer : keras.optimizers\n",
    "                The optimizer.\n",
    "        loss : str\n",
    "                The loss function.\n",
    "        metrics : list\n",
    "                The metrics.\n",
    "        class_weight : dict\n",
    "                The class weights.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "                The optimized threshold for the best F1 score.\n",
    "        \"\"\"\n",
    "        global X_train\n",
    "        global y_train\n",
    "        global X_val\n",
    "        global y_val\n",
    "        global X_test\n",
    "        global y_test\n",
    "\n",
    "        return train_and_test_model(\n",
    "                model=model,\n",
    "                X_train=X_train,\n",
    "                y_train=y_train,\n",
    "                X_val=X_val,\n",
    "                y_val=y_val,\n",
    "                X_test=X_test,\n",
    "                y_test=y_test,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                optimizer=optimizer,\n",
    "                loss=loss,\n",
    "                metrics=metrics,\n",
    "                class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-defined Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_columns = ['elapsed_time', 'room_coor_x', 'room_coor_y', 'screen_coor_x', 'screen_coor_y', 'event_name_checkpoint', 'event_name_cutscene_click', 'event_name_map_click', 'event_name_map_hover', 'event_name_navigate_click', 'event_name_notebook_click', 'event_name_notification_click', 'event_name_object_click', 'event_name_object_hover', 'event_name_observation_click', 'event_name_person_click', 'name_basic', 'name_close', 'name_next', 'name_open', 'name_prev', 'name_undefined', 'level_0', 'level_1', 'level_2', 'level_3', 'level_4', 'level_5', 'level_6', 'level_7', 'level_8', 'level_9', 'level_10', 'level_11', 'level_12', 'level_13', 'level_14', 'level_15', 'level_16', 'level_17', 'level_18', 'level_19', 'level_20', 'level_21', 'level_22', 'fqid_0', 'fqid_archivist', 'fqid_archivist_glasses', 'fqid_block', 'fqid_block_0', 'fqid_block_1', 'fqid_block_badge', 'fqid_block_badge_2', 'fqid_block_magnify', 'fqid_block_nelson', 'fqid_block_tocollection', 'fqid_block_tomap1', 'fqid_block_tomap2', 'fqid_boss', 'fqid_businesscards', 'fqid_businesscards.card_0.next', 'fqid_businesscards.card_1.next', 'fqid_businesscards.card_bingo.bingo', 'fqid_businesscards.card_bingo.next', 'fqid_ch3start', 'fqid_chap1_finale', 'fqid_chap1_finale_c', 'fqid_chap2_finale_c', 'fqid_chap4_finale_c', 'fqid_coffee', 'fqid_colorbook', 'fqid_confrontation', 'fqid_crane_ranger', 'fqid_cs', 'fqid_directory', 'fqid_directory.closeup.archivist', 'fqid_door_block_clean', 'fqid_door_block_talk', 'fqid_doorblock', 'fqid_expert', 'fqid_flag_girl', 'fqid_fox', 'fqid_glasses', 'fqid_gramps', 'fqid_groupconvo', 'fqid_groupconvo_flag', 'fqid_intro', 'fqid_janitor', 'fqid_journals', 'fqid_journals.hub.topics', 'fqid_journals.pic_0.next', 'fqid_journals.pic_1.next', 'fqid_journals.pic_2.bingo', 'fqid_journals.pic_2.next', 'fqid_journals_flag', 'fqid_journals_flag.hub.topics', 'fqid_journals_flag.hub.topics_old', 'fqid_journals_flag.pic_0.bingo', 'fqid_journals_flag.pic_0.next', 'fqid_journals_flag.pic_0_old.next', 'fqid_journals_flag.pic_1.bingo', 'fqid_journals_flag.pic_1.next', 'fqid_journals_flag.pic_1_old.next', 'fqid_journals_flag.pic_2.bingo', 'fqid_journals_flag.pic_2.next', 'fqid_journals_flag.pic_2_old.next', 'fqid_key', 'fqid_lockeddoor', 'fqid_logbook', 'fqid_logbook.page.bingo', 'fqid_magnify', 'fqid_need_glasses', 'fqid_notebook', 'fqid_outtolunch', 'fqid_photo', 'fqid_plaque', 'fqid_plaque.face.date', 'fqid_reader', 'fqid_reader.paper0.next', 'fqid_reader.paper0.prev', 'fqid_reader.paper1.next', 'fqid_reader.paper1.prev', 'fqid_reader.paper2.bingo', 'fqid_reader.paper2.next', 'fqid_reader.paper2.prev', 'fqid_reader_flag', 'fqid_reader_flag.paper0.next', 'fqid_reader_flag.paper0.prev', 'fqid_reader_flag.paper1.next', 'fqid_reader_flag.paper1.prev', 'fqid_reader_flag.paper2.bingo', 'fqid_reader_flag.paper2.next', 'fqid_reader_flag.paper2.prev', 'fqid_remove_cup', 'fqid_report', 'fqid_retirement_letter', 'fqid_savedteddy', 'fqid_seescratches', 'fqid_teddy', 'fqid_tobasement', 'fqid_tocage', 'fqid_tocloset', 'fqid_tocloset_dirty', 'fqid_tocollection', 'fqid_tocollectionflag', 'fqid_toentry', 'fqid_tofrontdesk', 'fqid_togrampa', 'fqid_tohallway', 'fqid_tomap', 'fqid_tomicrofiche', 'fqid_tostacks', 'fqid_tracks', 'fqid_tracks.hub.deer', 'fqid_trigger_coffee', 'fqid_trigger_scarf', 'fqid_tunic', 'fqid_tunic.capitol_0', 'fqid_tunic.capitol_1', 'fqid_tunic.capitol_2', 'fqid_tunic.drycleaner', 'fqid_tunic.flaghouse', 'fqid_tunic.historicalsociety', 'fqid_tunic.hub.slip', 'fqid_tunic.humanecology', 'fqid_tunic.kohlcenter', 'fqid_tunic.library', 'fqid_tunic.wildlife', 'fqid_unlockdoor', 'fqid_wells', 'fqid_wellsbadge', 'fqid_what_happened', 'fqid_worker', 'room_fqid_tunic.capitol_0.hall', 'room_fqid_tunic.capitol_1.hall', 'room_fqid_tunic.capitol_2.hall', 'room_fqid_tunic.drycleaner.frontdesk', 'room_fqid_tunic.flaghouse.entry', 'room_fqid_tunic.historicalsociety.basement', 'room_fqid_tunic.historicalsociety.cage', 'room_fqid_tunic.historicalsociety.closet', 'room_fqid_tunic.historicalsociety.closet_dirty', 'room_fqid_tunic.historicalsociety.collection', 'room_fqid_tunic.historicalsociety.collection_flag', 'room_fqid_tunic.historicalsociety.entry', 'room_fqid_tunic.historicalsociety.frontdesk', 'room_fqid_tunic.historicalsociety.stacks', 'room_fqid_tunic.humanecology.frontdesk', 'room_fqid_tunic.kohlcenter.halloffame', 'room_fqid_tunic.library.frontdesk', 'room_fqid_tunic.library.microfiche', 'room_fqid_tunic.wildlife.center', 'text_fqid_0', 'text_fqid_tunic.capitol_0.hall.boss.talktogramps', 'text_fqid_tunic.capitol_0.hall.chap1_finale_c', 'text_fqid_tunic.capitol_1.hall.boss.haveyougotit', 'text_fqid_tunic.capitol_1.hall.boss.writeitup', 'text_fqid_tunic.capitol_1.hall.chap2_finale_c', 'text_fqid_tunic.capitol_2.hall.boss.haveyougotit', 'text_fqid_tunic.capitol_2.hall.chap4_finale_c', 'text_fqid_tunic.drycleaner.frontdesk.block_0', 'text_fqid_tunic.drycleaner.frontdesk.block_1', 'text_fqid_tunic.drycleaner.frontdesk.logbook.page.bingo', 'text_fqid_tunic.drycleaner.frontdesk.worker.done', 'text_fqid_tunic.drycleaner.frontdesk.worker.done2', 'text_fqid_tunic.drycleaner.frontdesk.worker.hub', 'text_fqid_tunic.drycleaner.frontdesk.worker.takealook', 'text_fqid_tunic.flaghouse.entry.colorbook', 'text_fqid_tunic.flaghouse.entry.flag_girl.hello', 'text_fqid_tunic.flaghouse.entry.flag_girl.hello_recap', 'text_fqid_tunic.flaghouse.entry.flag_girl.symbol', 'text_fqid_tunic.flaghouse.entry.flag_girl.symbol_recap', 'text_fqid_tunic.historicalsociety.basement.ch3start', 'text_fqid_tunic.historicalsociety.basement.gramps.seeyalater', 'text_fqid_tunic.historicalsociety.basement.gramps.whatdo', 'text_fqid_tunic.historicalsociety.basement.janitor', 'text_fqid_tunic.historicalsociety.basement.savedteddy', 'text_fqid_tunic.historicalsociety.basement.seescratches', 'text_fqid_tunic.historicalsociety.cage.confrontation', 'text_fqid_tunic.historicalsociety.cage.glasses.afterteddy', 'text_fqid_tunic.historicalsociety.cage.glasses.beforeteddy', 'text_fqid_tunic.historicalsociety.cage.lockeddoor', 'text_fqid_tunic.historicalsociety.cage.need_glasses', 'text_fqid_tunic.historicalsociety.cage.teddy.trapped', 'text_fqid_tunic.historicalsociety.cage.unlockdoor', 'text_fqid_tunic.historicalsociety.closet.doorblock', 'text_fqid_tunic.historicalsociety.closet.gramps.intro_0_cs_0', 'text_fqid_tunic.historicalsociety.closet.intro', 'text_fqid_tunic.historicalsociety.closet.notebook', 'text_fqid_tunic.historicalsociety.closet.photo', 'text_fqid_tunic.historicalsociety.closet.retirement_letter.hub', 'text_fqid_tunic.historicalsociety.closet.teddy.intro_0_cs_0', 'text_fqid_tunic.historicalsociety.closet.teddy.intro_0_cs_5', 'text_fqid_tunic.historicalsociety.closet_dirty.door_block_clean', 'text_fqid_tunic.historicalsociety.closet_dirty.door_block_talk', 'text_fqid_tunic.historicalsociety.closet_dirty.gramps.archivist', 'text_fqid_tunic.historicalsociety.closet_dirty.gramps.helpclean', 'text_fqid_tunic.historicalsociety.closet_dirty.gramps.news', 'text_fqid_tunic.historicalsociety.closet_dirty.gramps.nothing', 'text_fqid_tunic.historicalsociety.closet_dirty.photo', 'text_fqid_tunic.historicalsociety.closet_dirty.trigger_coffee', 'text_fqid_tunic.historicalsociety.closet_dirty.trigger_scarf', 'text_fqid_tunic.historicalsociety.closet_dirty.what_happened', 'text_fqid_tunic.historicalsociety.collection.cs', 'text_fqid_tunic.historicalsociety.collection.gramps.found', 'text_fqid_tunic.historicalsociety.collection.gramps.look_0', 'text_fqid_tunic.historicalsociety.collection.gramps.lost', 'text_fqid_tunic.historicalsociety.collection.tunic', 'text_fqid_tunic.historicalsociety.collection.tunic.slip', 'text_fqid_tunic.historicalsociety.collection_flag.gramps.flag', 'text_fqid_tunic.historicalsociety.collection_flag.gramps.recap', 'text_fqid_tunic.historicalsociety.entry.block_tocollection', 'text_fqid_tunic.historicalsociety.entry.block_tomap1', 'text_fqid_tunic.historicalsociety.entry.block_tomap2', 'text_fqid_tunic.historicalsociety.entry.boss.flag', 'text_fqid_tunic.historicalsociety.entry.boss.flag_recap', 'text_fqid_tunic.historicalsociety.entry.boss.talktogramps', 'text_fqid_tunic.historicalsociety.entry.directory.closeup.archivist', 'text_fqid_tunic.historicalsociety.entry.gramps.hub', 'text_fqid_tunic.historicalsociety.entry.groupconvo', 'text_fqid_tunic.historicalsociety.entry.groupconvo_flag', 'text_fqid_tunic.historicalsociety.entry.wells.flag', 'text_fqid_tunic.historicalsociety.entry.wells.flag_recap', 'text_fqid_tunic.historicalsociety.entry.wells.talktogramps', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.foundtheodora', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.have_glass', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.have_glass_recap', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.hello', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.need_glass_0', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.need_glass_1', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.newspaper', 'text_fqid_tunic.historicalsociety.frontdesk.archivist.newspaper_recap', 'text_fqid_tunic.historicalsociety.frontdesk.archivist_glasses.confrontation', 'text_fqid_tunic.historicalsociety.frontdesk.archivist_glasses.confrontation_recap', 'text_fqid_tunic.historicalsociety.frontdesk.block_magnify', 'text_fqid_tunic.historicalsociety.frontdesk.key', 'text_fqid_tunic.historicalsociety.frontdesk.magnify', 'text_fqid_tunic.historicalsociety.stacks.block', 'text_fqid_tunic.historicalsociety.stacks.journals.pic_2.bingo', 'text_fqid_tunic.historicalsociety.stacks.journals_flag.pic_0.bingo', 'text_fqid_tunic.historicalsociety.stacks.journals_flag.pic_1.bingo', 'text_fqid_tunic.historicalsociety.stacks.journals_flag.pic_2.bingo', 'text_fqid_tunic.historicalsociety.stacks.outtolunch', 'text_fqid_tunic.humanecology.frontdesk.block_0', 'text_fqid_tunic.humanecology.frontdesk.block_1', 'text_fqid_tunic.humanecology.frontdesk.businesscards.card_bingo.bingo', 'text_fqid_tunic.humanecology.frontdesk.worker.badger', 'text_fqid_tunic.humanecology.frontdesk.worker.intro', 'text_fqid_tunic.kohlcenter.halloffame.block_0', 'text_fqid_tunic.kohlcenter.halloffame.plaque.face.date', 'text_fqid_tunic.kohlcenter.halloffame.togrampa', 'text_fqid_tunic.library.frontdesk.block_badge', 'text_fqid_tunic.library.frontdesk.block_badge_2', 'text_fqid_tunic.library.frontdesk.block_nelson', 'text_fqid_tunic.library.frontdesk.wellsbadge.hub', 'text_fqid_tunic.library.frontdesk.worker.droppedbadge', 'text_fqid_tunic.library.frontdesk.worker.flag', 'text_fqid_tunic.library.frontdesk.worker.flag_recap', 'text_fqid_tunic.library.frontdesk.worker.hello', 'text_fqid_tunic.library.frontdesk.worker.hello_short', 'text_fqid_tunic.library.frontdesk.worker.nelson', 'text_fqid_tunic.library.frontdesk.worker.nelson_recap', 'text_fqid_tunic.library.frontdesk.worker.preflag', 'text_fqid_tunic.library.frontdesk.worker.wells', 'text_fqid_tunic.library.frontdesk.worker.wells_recap', 'text_fqid_tunic.library.microfiche.block_0', 'text_fqid_tunic.library.microfiche.reader.paper2.bingo', 'text_fqid_tunic.library.microfiche.reader_flag.paper2.bingo', 'text_fqid_tunic.wildlife.center.coffee', 'text_fqid_tunic.wildlife.center.crane_ranger.crane', 'text_fqid_tunic.wildlife.center.expert.recap', 'text_fqid_tunic.wildlife.center.expert.removed_cup', 'text_fqid_tunic.wildlife.center.fox.concern', 'text_fqid_tunic.wildlife.center.remove_cup', 'text_fqid_tunic.wildlife.center.tracks.hub.deer', 'text_fqid_tunic.wildlife.center.wells.animals', 'text_fqid_tunic.wildlife.center.wells.animals2', 'text_fqid_tunic.wildlife.center.wells.nodeer', 'text_fqid_tunic.wildlife.center.wells.nodeer_recap', 'level_group_0-4', 'level_group_13-22', 'level_group_5-12', \n",
    "'question_1',\n",
    "'question_2',\n",
    "'question_3',\n",
    "'question_4',\n",
    "'question_5',\n",
    "'question_6',\n",
    "'question_7',\n",
    "'question_8',\n",
    "'question_9',\n",
    "'question_10',\n",
    "'question_11',\n",
    "'question_12',\n",
    "'question_13',\n",
    "'question_14',\n",
    "'question_15',\n",
    "'question_16',\n",
    "'question_17',\n",
    "'question_18',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13173445, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>event_name</th>\n",
       "      <th>name</th>\n",
       "      <th>level</th>\n",
       "      <th>room_coor_x</th>\n",
       "      <th>room_coor_y</th>\n",
       "      <th>screen_coor_x</th>\n",
       "      <th>screen_coor_y</th>\n",
       "      <th>fqid</th>\n",
       "      <th>room_fqid</th>\n",
       "      <th>text_fqid</th>\n",
       "      <th>level_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>0</td>\n",
       "      <td>cutscene_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>intro</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.intro</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>1323</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>831</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          session_id  elapsed_time      event_name   name  level  room_coor_x  \\\n",
       "0  20090312431273200             0  cutscene_click  basic      0  -413.991405   \n",
       "1  20090312431273200          1323    person_click  basic      0  -413.991405   \n",
       "2  20090312431273200           831    person_click  basic      0  -413.991405   \n",
       "\n",
       "   room_coor_y  screen_coor_x  screen_coor_y    fqid  \\\n",
       "0  -159.314686          380.0          494.0   intro   \n",
       "1  -159.314686          380.0          494.0  gramps   \n",
       "2  -159.314686          380.0          494.0  gramps   \n",
       "\n",
       "                        room_fqid  \\\n",
       "0  tunic.historicalsociety.closet   \n",
       "1  tunic.historicalsociety.closet   \n",
       "2  tunic.historicalsociety.closet   \n",
       "\n",
       "                                           text_fqid level_group  \n",
       "0               tunic.historicalsociety.closet.intro         0-4  \n",
       "1  tunic.historicalsociety.closet.gramps.intro_0_...         0-4  \n",
       "2  tunic.historicalsociety.closet.gramps.intro_0_...         0-4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare the main dataset\n",
    "df_source = prepare_main_dataset(df_source)\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    print(df_source.shape)\n",
    "    display(df_source.head(3))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>question_num</th>\n",
       "      <th>correct</th>\n",
       "      <th>level_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21476</th>\n",
       "      <td>22010116250792520</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84068</th>\n",
       "      <td>21000111433937450</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171219</th>\n",
       "      <td>21040510125933256</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>13-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               session_id  question_num  correct level_group\n",
       "21476   22010116250792520             2        1         0-4\n",
       "84068   21000111433937450             8        1        5-12\n",
       "171219  21040510125933256            15        0       13-22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare the label dataset\n",
    "df_source_labels = prepare_label_dataset(df_source_labels)\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_source_labels.sample(n=3, random_state=51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "loss = 'binary_crossentropy'\n",
    "metrics = ['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 51\n",
    "\n",
    "sample_size = 1000\n",
    "event_count = 1000\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 700\n",
      "Validation: 150\n",
      "Test: 150\n"
     ]
    }
   ],
   "source": [
    "train, val, test = select_sessions(\n",
    "    y=df_source_labels,\n",
    "    sample_size=sample_size,\n",
    "    random_state=random_state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the training dataset\n",
    "X_train, y_train = create_dataset(\n",
    "    X=df_source, y=df_source_labels, session_list=train, event_count=event_count)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the validation dataset\n",
    "X_val, y_val = create_dataset(\n",
    "    X=df_source, y=df_source_labels, session_list=val, event_count=event_count)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the test dataset\n",
    "X_test, y_test = create_dataset(\n",
    "    X=df_source, y=df_source_labels, session_list=test, event_count=event_count)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(layers.LSTM(1024, input_shape=(event_count, len(vector_columns))))\n",
    "# model.add(layers.Dropout(0.2))\n",
    "# #model.add(layers.Dense(64, activation='relu'))\n",
    "# model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# #model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_train_model(\n",
    "#     model=model,\n",
    "#     epochs=10,\n",
    "#     batch_size=batch_size,\n",
    "#     optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "#     loss=loss,\n",
    "#     metrics=metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smaller Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 11:23:55.606843: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-28 11:23:55.619439: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-28 11:23:55.619609: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-28 11:23:55.620112: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-28 11:23:55.620520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-28 11:23:55.620646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-28 11:23:55.620755: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-28 11:23:55.854077: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-28 11:23:55.854221: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-28 11:23:55.854329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-28 11:23:55.854413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9831 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.LSTM(32, input_shape=(event_count, len(vector_columns))))\n",
    "model.add(layers.Dropout(0.2))\n",
    "#model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "#model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "global_train_model(\n",
    "    model=model,\n",
    "    epochs=10,\n",
    "    batch_size=batch_size,\n",
    "    optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=loss,\n",
    "    metrics=metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
