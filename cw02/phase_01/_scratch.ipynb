{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "b = [1,2,3]\n",
    "c = [4,5,6]\n",
    "\n",
    "a.append(b)\n",
    "a.append(c)\n",
    "\n",
    "del b\n",
    "del c\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty 3D array with 2 layers, 3 rows and 4 columns\n",
    "arr = np.empty((2, 3, 4))\n",
    "\n",
    "# print the array\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#na = np.array([[1, 2, 3]], dtype=np.float64)\n",
    "na = np.array()\n",
    "\n",
    "#na = np.append(na, [[4, 5, 6]], axis=0)\n",
    "\n",
    "\n",
    "na"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import os\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,\n",
    "                          as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE, train_dataset.output_shapes)\n",
    "\n",
    "test_dataset = test_dataset.padded_batch(BATCH_SIZE, test_dataset.output_shapes)\n",
    "encoder = info.features['text'].encoder\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n",
    "    #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset, \n",
    "                    validation_steps=30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_heatmap(screen_coor_x, screen_coor_y):\n",
    "    # Determine the size of the heatmap\n",
    "    max_x = int(np.ceil(max(screen_coor_x)))\n",
    "    max_y = int(np.ceil(max(screen_coor_y)))\n",
    "\n",
    "    # Create a 2D histogram of the coordinates\n",
    "    heatmap, xedges, yedges = np.histogram2d(screen_coor_y, screen_coor_x, bins=(max_y, max_x))\n",
    "\n",
    "    # Create the heatmap image and display it\n",
    "    plt.imshow(heatmap.T, origin='lower')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "session_1_screen_coor_x = [1.2, 2.4, 3.1, 4.5, 2.2]\n",
    "session_1_screen_coor_y = [3.5, 1.7, 4.8, 2.1, 5.0]\n",
    "\n",
    "create_heatmap(session_1_screen_coor_x, session_1_screen_coor_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the source training set\n",
    "df_source = pd.read_csv('data/train.csv.gz', compression='gzip', index_col=1)\n",
    "\n",
    "print(df_source.shape)\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_source.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_screen_coor(data: df_source, session_id: int):\n",
    "    # get the session data\n",
    "    session_data = df_source[df_source['session_id'] == session_id]\n",
    "    session_data = session_data[['screen_coor_x', 'screen_coor_y']]\n",
    "    session_data = session_data.dropna()\n",
    "\n",
    "    # get the screen coordinates\n",
    "    screen_coor_x = session_data['screen_coor_x'].values\n",
    "    screen_coor_y = session_data['screen_coor_y'].values\n",
    "    \n",
    "    return screen_coor_x, screen_coor_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_1_screen_coor_x, session_1_screen_coor_y =  \\\n",
    "    get_session_screen_coor(df_source, 20090312431273200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_2_screen_coor_x, session_2_screen_coor_y =  \\\n",
    "    get_session_screen_coor(df_source, 21040510125933256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#20090312433251036\n",
    "#22100221145014656\n",
    "\n",
    "# this one has a bigger max x value\n",
    "session_3_screen_coor_x, session_3_screen_coor_y =  \\\n",
    "    get_session_screen_coor(df_source, 21040510125933256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_heatmap(screen_coor_x, screen_coor_y):\n",
    "    # Determine the size of the heatmap\n",
    "    max_x = int(np.ceil(max(screen_coor_x)))\n",
    "    max_y = int(np.ceil(max(screen_coor_y)))\n",
    "\n",
    "    # Create a 2D histogram of the coordinates\n",
    "    heatmap, xedges, yedges = np.histogram2d(screen_coor_y, screen_coor_x, bins=50)\n",
    "\n",
    "    # Create the heatmap image and display it\n",
    "    # plt.imshow(heatmap.T, origin='lower')\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "    plt.imshow(heatmap.T, cmap='hot', extent=extent, origin='lower')\n",
    "\n",
    "# Example usage\n",
    "#create_heatmap(screen_coor_x, screen_coor_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap(screen_coor_x, screen_coor_y):\n",
    "    \"\"\"\n",
    "    Make the points stand out more by applying a logarithmic transformation\n",
    "    \"\"\"\n",
    "    # Create the 2D histogram\n",
    "    heatmap, xedges, yedges = np.histogram2d(screen_coor_y, screen_coor_x, bins=50)\n",
    "\n",
    "    # Apply logarithmic transformation\n",
    "    heatmap = np.log(heatmap + 1)\n",
    "\n",
    "    # Plot the heatmap\n",
    "    extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "    plt.imshow(heatmap.T, cmap='hot', extent=extent, origin='lower')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "#create_heatmap(screen_coor_x, screen_coor_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_1_heatmap = create_heatmap(session_1_screen_coor_x, session_1_screen_coor_y)\n",
    "session_2_heatmap = create_heatmap(session_2_screen_coor_x, session_2_screen_coor_y)\n",
    "session_3_heatmap = create_heatmap(session_3_screen_coor_x, session_3_screen_coor_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session_1_heatmap.shape)\n",
    "print(session_2_heatmap.shape)\n",
    "print(session_3_heatmap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def save_heatmap_to_image(heatmap):\n",
    "    # Normalize the heatmap\n",
    "    normalized_heatmap = (heatmap - np.min(heatmap)) / (np.max(heatmap) - np.min(heatmap))\n",
    "\n",
    "    # Scale the heatmap to the range 0-255\n",
    "    scaled_heatmap = (normalized_heatmap * 255).astype(np.uint8)\n",
    "\n",
    "    # Create an image from the heatmap array\n",
    "    img = Image.fromarray(scaled_heatmap)\n",
    "\n",
    "    # Save the image to a file\n",
    "    img.save('heatmap.png')\n",
    "\n",
    "    return img\n",
    "\n",
    "# test the function\n",
    "simple_screen_coor_x = [1.2, 2.4, 3.1, 4.5, 2.2]\n",
    "simple_screen_coor_y = [3.5, 1.7, 4.8, 2.1, 5.0]\n",
    "\n",
    "#heatmap = create_heatmap(simple_screen_coor_x, simple_screen_coor_y)\n",
    "heatmap = create_heatmap(session_1_screen_coor_x, session_1_screen_coor_y)\n",
    "heatmap_image = save_heatmap_to_image(heatmap)\n",
    "heatmap_array = np.array(heatmap_image)\n",
    "heatmap_reshape = heatmap.reshape((heatmap.shape[0], heatmap.shape[1], 1))\n",
    "\n",
    "#img = Image.fromarray(np.uint8(heatmap_reshape*255))\n",
    "img = Image.fromarray(heatmap)\n",
    "img = img.convert('RGB')\n",
    "img.save('heatmap_reshape.png')\n",
    "\n",
    "print(heatmap)\n",
    "\n",
    "print(heatmap_array.shape)\n",
    "print(heatmap_reshape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_x = np.max(session_1_screen_coor_x)\n",
    "df_source[df_source['screen_coor_x'] > max_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def create_heatmap(screen_coor_x, screen_coor_y):\n",
    "#     # Determine the size of the heatmap\n",
    "#     max_x = int(np.ceil(max(screen_coor_x)))\n",
    "#     max_y = int(np.ceil(max(screen_coor_y)))\n",
    "\n",
    "#     # Create a 2D histogram of the coordinates\n",
    "#     heatmap, xedges, yedges = np.histogram2d(screen_coor_y, screen_coor_x, bins=50)\n",
    "\n",
    "#     # Create the heatmap image and display it\n",
    "#     # plt.imshow(heatmap.T, origin='lower')\n",
    "#     # plt.show()\n",
    "\n",
    "\n",
    "#     extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "#     plt.imshow(heatmap.T, cmap='hot', extent=extent, origin='lower')\n",
    "\n",
    "# # Example usage\n",
    "# create_heatmap(screen_coor_x, screen_coor_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 18:50:46.345489: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 18:50:47.465206: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-03 18:50:47.466765: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-03 18:50:47.466879: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11490434/11490434 [==============================] - 2s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 18:50:49.879026: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-03 18:50:49.879495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-03 18:50:49.879630: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-03 18:50:49.879734: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-03 18:50:50.166994: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-03 18:50:50.167124: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-03 18:50:50.167215: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-03 18:50:50.167292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9702 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 18:50:51.113966: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-03-03 18:50:51.235922: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f953bfc8e80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-03-03 18:50:51.235941: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3080 Ti, Compute Capability 8.6\n",
      "2023-03-03 18:50:51.238423: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-03-03 18:50:51.311060: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 3s 867us/step - loss: 0.2591 - accuracy: 0.9257\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 2s 890us/step - loss: 0.1172 - accuracy: 0.9653\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 2s 880us/step - loss: 0.0800 - accuracy: 0.9757\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 2s 873us/step - loss: 0.0613 - accuracy: 0.9815\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 2s 874us/step - loss: 0.0470 - accuracy: 0.9855\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 2s 872us/step - loss: 0.0370 - accuracy: 0.9883\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 2s 870us/step - loss: 0.0294 - accuracy: 0.9908\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 2s 876us/step - loss: 0.0243 - accuracy: 0.9926\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 2s 877us/step - loss: 0.0205 - accuracy: 0.9940\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 2s 870us/step - loss: 0.0160 - accuracy: 0.9951\n",
      "313/313 - 0s - loss: 0.0816 - accuracy: 0.9777 - 277ms/epoch - 885us/step\n",
      "\n",
      "Test loss: 0.08158182352781296\n",
      "Test accuracy: 0.9776999950408936\n"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Set the GPU memory from growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Preprocess the images by scaling them to the range [0, 1]\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Define the model architecture using a sequential API\n",
    "model = keras.Sequential([\n",
    "  keras.layers.Flatten(input_shape=(28, 28)), # Flatten the input images to a vector of 784 pixels\n",
    "  keras.layers.Dense(128, activation='relu'), # Add a hidden layer with 128 neurons and ReLU activation\n",
    "  keras.layers.Dense(10) # Add an output layer with 10 neurons for each class (0-9)\n",
    "])\n",
    "\n",
    "# Compile the model with an optimizer, a loss function and a metric\n",
    "model.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data for 10 epochs\n",
    "model.fit(train_images, train_labels, epochs=10)\n",
    "\n",
    "# Evaluate the model on the test data and print the results\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print('\\nTest loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
