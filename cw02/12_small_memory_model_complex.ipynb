{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Small and Fast Training\n",
    "\n",
    "Find a way to do fast training based on previous methods. The main speed comes from not using a generator and using smaller datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 11:56:23.601527: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-27 11:56:23.766249: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-27 11:56:23.770224: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/hadoop/current/lib/native:/opt/hadoop/current/lib/native\n",
      "2023-02-27 11:56:23.770241: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-27 11:56:24.303663: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/hadoop/current/lib/native:/opt/hadoop/current/lib/native\n",
      "2023-02-27 11:56:24.303744: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/hadoop/current/lib/native:/opt/hadoop/current/lib/native\n",
      "2023-02-27 11:56:24.303752: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import logging\n",
    "import gc\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13174211, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>event_name</th>\n",
       "      <th>name</th>\n",
       "      <th>level</th>\n",
       "      <th>page</th>\n",
       "      <th>room_coor_x</th>\n",
       "      <th>room_coor_y</th>\n",
       "      <th>screen_coor_x</th>\n",
       "      <th>screen_coor_y</th>\n",
       "      <th>hover_duration</th>\n",
       "      <th>text</th>\n",
       "      <th>fqid</th>\n",
       "      <th>room_fqid</th>\n",
       "      <th>text_fqid</th>\n",
       "      <th>fullscreen</th>\n",
       "      <th>hq</th>\n",
       "      <th>music</th>\n",
       "      <th>level_group</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>0</td>\n",
       "      <td>cutscene_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>undefined</td>\n",
       "      <td>intro</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.intro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>1323</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Whatcha doing over there, Jo?</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>831</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just talking to Teddy.</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              session_id  elapsed_time      event_name   name  level  page  \\\n",
       "index                                                                        \n",
       "0      20090312431273200             0  cutscene_click  basic      0   NaN   \n",
       "1      20090312431273200          1323    person_click  basic      0   NaN   \n",
       "2      20090312431273200           831    person_click  basic      0   NaN   \n",
       "\n",
       "       room_coor_x  room_coor_y  screen_coor_x  screen_coor_y  hover_duration  \\\n",
       "index                                                                           \n",
       "0      -413.991405  -159.314686          380.0          494.0             NaN   \n",
       "1      -413.991405  -159.314686          380.0          494.0             NaN   \n",
       "2      -413.991405  -159.314686          380.0          494.0             NaN   \n",
       "\n",
       "                                text    fqid                       room_fqid  \\\n",
       "index                                                                          \n",
       "0                          undefined   intro  tunic.historicalsociety.closet   \n",
       "1      Whatcha doing over there, Jo?  gramps  tunic.historicalsociety.closet   \n",
       "2             Just talking to Teddy.  gramps  tunic.historicalsociety.closet   \n",
       "\n",
       "                                               text_fqid  fullscreen  hq  \\\n",
       "index                                                                      \n",
       "0                   tunic.historicalsociety.closet.intro         NaN NaN   \n",
       "1      tunic.historicalsociety.closet.gramps.intro_0_...         NaN NaN   \n",
       "2      tunic.historicalsociety.closet.gramps.intro_0_...         NaN NaN   \n",
       "\n",
       "       music level_group  \n",
       "index                     \n",
       "0        NaN         0-4  \n",
       "1        NaN         0-4  \n",
       "2        NaN         0-4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the source training set\n",
    "df_source = pd.read_csv('data/train.csv.gz', compression='gzip', index_col=1)\n",
    "\n",
    "print(df_source.shape)\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_source.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(212022, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200_q1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312433251036_q1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090314121766812_q1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             session_id  correct\n",
       "0  20090312431273200_q1        1\n",
       "1  20090312433251036_q1        0\n",
       "2  20090314121766812_q1        1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the source training labels\n",
    "df_source_labels = pd.read_csv('data/train_labels.csv')\n",
    "\n",
    "print(df_source_labels.shape)\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_source_labels.head(3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_question_to_level_group(question_number):\n",
    "    \"\"\"\n",
    "    Maps the question number to the level group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question_number : int\n",
    "        The question number.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The level group.\n",
    "    \"\"\"\n",
    "    if question_number in [1, 2, 3]:\n",
    "        return '0-4'\n",
    "    elif question_number in [4, 5, 6, 7, 8, 9, 10, 11, 12, 13]:\n",
    "        return '5-12'\n",
    "    elif question_number in [14, 15, 16, 17, 18]:\n",
    "        return '13-22'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def prepare_label_dataset(data : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the label dataset and add columns for the level group \n",
    "    and the question number.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The label dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The prepared label dataset.\n",
    "    \"\"\"\n",
    "    # add the columns to determine the level group\n",
    "    df_labels = data \\\n",
    "        .rename(columns={'session_id': 'id'}) \\\n",
    "        .assign(session_id=lambda df: df['id'].str.split('_').str[0].astype(int)) \\\n",
    "        .assign(question_id=lambda df: df['id'].str.split('_').str[1]) \\\n",
    "        .assign(question_num=lambda df: df['question_id'].str[1:].astype(int)) \\\n",
    "        [['session_id', 'question_num', 'correct']]\n",
    "    \n",
    "    # add the level group column\n",
    "    df_labels['level_group'] = df_labels['question_num'].apply(map_question_to_level_group) \n",
    "\n",
    "        \n",
    "    return df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_main_dataset(data : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the main dataset by removing duplicates and removing \n",
    "    columns that are not needed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The main dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The prepared main dataset.\n",
    "    \"\"\"\n",
    "    empty_columns = ['fullscreen', 'hq', 'music', 'page', 'hover_duration']\n",
    "\n",
    "    df_main = data \\\n",
    "        .drop_duplicates() \\\n",
    "        .reset_index(drop=True) \\\n",
    "        .drop(empty_columns, axis=1) \\\n",
    "        .drop('text', axis=1)\n",
    "\n",
    "    return df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_dataset(data: pd.DataFrame, standardize_coordinates: bool=True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Vectorizes the dataset for deep learning by one-hot encoding and standardizing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The dataset to prepare.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The vectorized dataset.\n",
    "    \"\"\"\n",
    "    categorical_cols = ['event_name', 'name', 'level', 'fqid', 'room_fqid', 'text_fqid', 'level_group']\n",
    "    numerical_cols = ['elapsed_time']    \n",
    "    coordinates_cols = ['room_coor_x', 'room_coor_y', 'screen_coor_x', 'screen_coor_y']\n",
    "\n",
    "    df_vectorized = data \\\n",
    "        .drop('session_id', axis=1) \\\n",
    "        .fillna(0)\n",
    "\n",
    "    # standardize the numerical variables\n",
    "    df_vectorized[numerical_cols] = (df_vectorized[numerical_cols] - df_vectorized[numerical_cols].mean()) / df_vectorized[numerical_cols].std()\n",
    "\n",
    "    # standardize the coordinates\n",
    "    if standardize_coordinates:\n",
    "        df_vectorized[coordinates_cols] = (df_vectorized[coordinates_cols] - df_vectorized[coordinates_cols].mean()) / df_vectorized[coordinates_cols].std()\n",
    "\n",
    "    # one-hot encode the categorical variables\n",
    "    df_vectorized = pd.get_dummies(df_vectorized, columns=categorical_cols)\n",
    "    \n",
    "    return df_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_matrix(data: pd.DataFrame, vector_columns: list, standardize_coordinates: bool=True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare a sequence matrix from a DataFrame for a specific session and level group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The dataframe containing the data.\n",
    "\n",
    "    vector_columns : list\n",
    "        The columns that should appear in the sequence matrix.\n",
    "\n",
    "    standardize_coordinates : bool, optional\n",
    "        Whether to standardize the coordinates.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The sequence matrix.\n",
    "    \"\"\"\n",
    "    df_sequence_matrix = vectorize_dataset(data, standardize_coordinates=standardize_coordinates)\n",
    "\n",
    "    # add the missing columns\n",
    "    missing_columns = [column for column in vector_columns if column not in df_sequence_matrix.columns]\n",
    "    df_sequence_matrix = pd.concat([\n",
    "        df_sequence_matrix, \n",
    "        pd.DataFrame(columns=missing_columns)], axis=1).fillna(0)\n",
    "\n",
    "    return df_sequence_matrix[vector_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_array(session_data: pd.DataFrame, \n",
    "                        level_group: int,\n",
    "                        event_count:int = 1000) -> np.array:\n",
    "    \"\"\"\n",
    "    Creates a vector array for a specific session and question number.\n",
    "    \"\"\"\n",
    "    # get the data for the session and level group\n",
    "    df_session = session_data.query('level_group == @level_group')\n",
    "\n",
    "    # prepare the sequence matrix\n",
    "    df_sequence_matrix = prepare_sequence_matrix(df_session, vector_columns)\n",
    "\n",
    "    # convert it to a numpy array\n",
    "    vector_array = df_sequence_matrix.to_numpy()\n",
    "\n",
    "    # the array cannot have more events than the event count\n",
    "    vector_array = vector_array[:event_count]\n",
    "\n",
    "    # pad the array with zeros if it has less events than the event count\n",
    "    if vector_array.shape[0] < event_count:\n",
    "        vector_array = np.pad(vector_array, ((0, event_count - vector_array.shape[0]), (0, 0)), 'constant')\n",
    "\n",
    "    del df_session\n",
    "    del df_sequence_matrix\n",
    "\n",
    "    return vector_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X: pd.DataFrame,\n",
    "                   y: pd.DataFrame,\n",
    "                   event_count:int,\n",
    "                   session_list: list) -> Tuple[np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Creates a dataset for a specific set of sessions and question numbers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        The main dataset.\n",
    "\n",
    "    y : pd.DataFrame\n",
    "        The label dataset.\n",
    "\n",
    "    event_count : int\n",
    "        The number of events to include in the dataset.\n",
    "\n",
    "    session_ids : list\n",
    "        The list of session ids.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.array, np.array]\n",
    "        The dataset and the labels.\n",
    "    \"\"\"\n",
    "    X_dataset = []\n",
    "    y_dataset = []\n",
    "\n",
    "    for session_id in session_list:\n",
    "        # get the session labels\n",
    "        df_session_labels = y.query('session_id == @session_id')\n",
    "        df_session = X.query('session_id == @session_id')\n",
    "\n",
    "        # create the level groups\n",
    "        vector_arrays = {\n",
    "            '0-4': create_vector_array(df_session, '0-4', event_count),\n",
    "            '5-12': create_vector_array(df_session, '5-12', event_count),\n",
    "            '13-22': create_vector_array(df_session, '13-22', event_count),\n",
    "        }\n",
    "\n",
    "        # iterate over all the questions answered in the session\n",
    "        for _, row in df_session_labels.iterrows():\n",
    "            question_number = row['question_num']\n",
    "            correct = row['correct']\n",
    "            level_group = map_question_to_level_group(question_number)\n",
    "\n",
    "            # get the vector array\n",
    "            vector_array = vector_arrays[level_group].copy()\n",
    "\n",
    "            # set the question number value\n",
    "            column_index = vector_columns.index(f'question_{question_number}')\n",
    "            #print(f'Question: {question_number}, Column: {column_index}')\n",
    "            vector_array[:, column_index] = 1\n",
    "\n",
    "            # add the vector array to the dataset\n",
    "            X_dataset.append(vector_array)\n",
    "            del vector_array\n",
    "\n",
    "            # add the label to the dataset\n",
    "            y_dataset.append(correct)\n",
    "\n",
    "    X_out = np.array(X_dataset, dtype=np.float64)\n",
    "    y_out = np.array(y_dataset)\n",
    "\n",
    "    return X_out, y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history: callbacks.History, figsize: Tuple[int, int] = (5, 3)) -> None:\n",
    "    \"\"\"\n",
    "    Plot the loss and validation loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    history : keras.callbacks.History\n",
    "        The history of the model training.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(epochs, history.history['loss'])\n",
    "    \n",
    "    if ('val_loss' in history.history):\n",
    "        plt.plot(epochs, history.history['val_loss'])\n",
    "        plt.legend(['Training loss', 'Validation loss'], loc='upper left')\n",
    "        plt.title('Training and validation loss')\n",
    "    else:\n",
    "        plt.title('Training loss')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(history: callbacks.History, figsize: Tuple[int, int] = (5, 3)) -> None:\n",
    "    \"\"\"\n",
    "    Plot the accuracy and validation accuracy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    history : keras.callbacks.History\n",
    "        The history of the model training.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(epochs, history.history['accuracy'])\n",
    "\n",
    "    if ('val_accuracy' in history.history):\n",
    "        plt.plot(epochs, history.history['val_accuracy'])\n",
    "        plt.legend(['Training acc', 'Validation acc'], loc='upper left')\n",
    "        plt.title('Training and validation accuracy')\n",
    "    else:\n",
    "        plt.title('Training accuracy')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_f1(y_true: np.ndarray, y_score: np.ndarray) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Optimize the F1 score.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        The true labels.\n",
    "    y_score : np.ndarray\n",
    "        The predicted labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[float, float, float]\n",
    "        The optimized threshold, precision, and recall.\n",
    "    \"\"\"\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0\n",
    "    best_precision = 0\n",
    "    best_recall = 0\n",
    "\n",
    "    for threshold in np.arange(0, 1, 0.01):\n",
    "        y_pred = (y_score > threshold).astype(int)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=1)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_precision = precision\n",
    "            best_recall = recall\n",
    "\n",
    "    return best_threshold, best_precision, best_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        model,\n",
    "        X_train: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        X_val : np.ndarray,\n",
    "        y_val: np.ndarray,\n",
    "        epochs: int,\n",
    "        batch_size: int,\n",
    "        optimizer,\n",
    "        loss: str,\n",
    "        metrics: list) -> callbacks.History:\n",
    "    \"\"\"\n",
    "    Train the keras model based on the parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : keras.models\n",
    "        The model to train.\n",
    "    X_train : np.ndarray\n",
    "        The training data.\n",
    "    y_train : np.ndarray\n",
    "        The training labels.\n",
    "    X_val : np.ndarray\n",
    "        The validation data.\n",
    "    y_val : np.ndarray\n",
    "        The validation labels.\n",
    "    epochs : int\n",
    "        The number of epochs.\n",
    "    batch_size : int\n",
    "        The batch size.\n",
    "    optimizer : keras.optimizers\n",
    "        The optimizer.\n",
    "    loss : str\n",
    "        The loss function.\n",
    "    metrics : list\n",
    "        The metrics.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    keras.callbacks.History\n",
    "        The history of the training.\n",
    "    \"\"\"\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics)\n",
    "    \n",
    "    # fit the model\n",
    "    history = model.fit(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val))\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(\n",
    "        model,\n",
    "        history: callbacks.History,\n",
    "        X_test: np.ndarray,\n",
    "        y_test: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Test the model based on the test data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : keras.models\n",
    "        The model to test.\n",
    "    history : keras.callbacks.History\n",
    "        The history of the training.\n",
    "    X_test : np.ndarray\n",
    "        The test data.\n",
    "    y_test : np.ndarray\n",
    "        The test labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The optimized threshold for the best F1 score.\n",
    "    \"\"\"\n",
    "    \n",
    "    plot_loss(history)\n",
    "    plot_accuracy(history)\n",
    "\n",
    "    y_test_score = model.predict(X_test)\n",
    "    threshold, _, _ = optimize_f1(y_test, y_test_score)\n",
    "\n",
    "    print(classification_report(y_test, y_test_score > threshold))\n",
    "    print(f'Optimized threshold for best F1: {threshold:.2f}')\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-defined Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_columns = [\n",
    "    'elapsed_time', \n",
    "    'room_coor_x', \n",
    "    'room_coor_y', \n",
    "    'screen_coor_x', \n",
    "    'screen_coor_y', \n",
    "    'event_name_checkpoint', \n",
    "    'event_name_cutscene_click', \n",
    "    'event_name_map_click', \n",
    "    'event_name_map_hover', \n",
    "    'event_name_navigate_click', \n",
    "    'event_name_notebook_click', \n",
    "    'event_name_notification_click', \n",
    "    'event_name_object_click', \n",
    "    'event_name_object_hover', \n",
    "    'event_name_observation_click', \n",
    "    'event_name_person_click', \n",
    "    'name_basic', \n",
    "    'name_close', \n",
    "    'name_next', \n",
    "    'name_open', \n",
    "    'name_prev', \n",
    "    'name_undefined', \n",
    "    'level_0', \n",
    "    'level_1', \n",
    "    'level_2', \n",
    "    'level_3', \n",
    "    'level_4', \n",
    "    'level_5', \n",
    "    'level_6', \n",
    "    'level_7', \n",
    "    'level_8', \n",
    "    'level_9', \n",
    "    'level_10', \n",
    "    'level_11', \n",
    "    'level_12', \n",
    "    'level_13', \n",
    "    'level_14', \n",
    "    'level_15', \n",
    "    'level_16', \n",
    "    'level_17', \n",
    "    'level_18', \n",
    "    'level_19', \n",
    "    'level_20', \n",
    "    'level_21', \n",
    "    'level_22', \n",
    "    'level_group_0-4', \n",
    "    'level_group_13-22', \n",
    "    'level_group_5-12', \n",
    "    'question_1',\n",
    "    'question_2',\n",
    "    'question_3',\n",
    "    'question_4',\n",
    "    'question_5',\n",
    "    'question_6',\n",
    "    'question_7',\n",
    "    'question_8',\n",
    "    'question_9',\n",
    "    'question_10',\n",
    "    'question_11',\n",
    "    'question_12',\n",
    "    'question_13',\n",
    "    'question_14',\n",
    "    'question_15',\n",
    "    'question_16',\n",
    "    'question_17',\n",
    "    'question_18',    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13173445, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>event_name</th>\n",
       "      <th>name</th>\n",
       "      <th>level</th>\n",
       "      <th>room_coor_x</th>\n",
       "      <th>room_coor_y</th>\n",
       "      <th>screen_coor_x</th>\n",
       "      <th>screen_coor_y</th>\n",
       "      <th>fqid</th>\n",
       "      <th>room_fqid</th>\n",
       "      <th>text_fqid</th>\n",
       "      <th>level_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>0</td>\n",
       "      <td>cutscene_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>intro</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.intro</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>1323</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>831</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          session_id  elapsed_time      event_name   name  level  room_coor_x  \\\n",
       "0  20090312431273200             0  cutscene_click  basic      0  -413.991405   \n",
       "1  20090312431273200          1323    person_click  basic      0  -413.991405   \n",
       "2  20090312431273200           831    person_click  basic      0  -413.991405   \n",
       "\n",
       "   room_coor_y  screen_coor_x  screen_coor_y    fqid  \\\n",
       "0  -159.314686          380.0          494.0   intro   \n",
       "1  -159.314686          380.0          494.0  gramps   \n",
       "2  -159.314686          380.0          494.0  gramps   \n",
       "\n",
       "                        room_fqid  \\\n",
       "0  tunic.historicalsociety.closet   \n",
       "1  tunic.historicalsociety.closet   \n",
       "2  tunic.historicalsociety.closet   \n",
       "\n",
       "                                           text_fqid level_group  \n",
       "0               tunic.historicalsociety.closet.intro         0-4  \n",
       "1  tunic.historicalsociety.closet.gramps.intro_0_...         0-4  \n",
       "2  tunic.historicalsociety.closet.gramps.intro_0_...         0-4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare the main dataset\n",
    "df_source = prepare_main_dataset(df_source)\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    print(df_source.shape)\n",
    "    display(df_source.head(3))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>question_num</th>\n",
       "      <th>correct</th>\n",
       "      <th>level_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21476</th>\n",
       "      <td>22010116250792520</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84068</th>\n",
       "      <td>21000111433937450</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171219</th>\n",
       "      <td>21040510125933256</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>13-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               session_id  question_num  correct level_group\n",
       "21476   22010116250792520             2        1         0-4\n",
       "84068   21000111433937450             8        1        5-12\n",
       "171219  21040510125933256            15        0       13-22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare the label dataset\n",
    "df_source_labels = prepare_label_dataset(df_source_labels)\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_source_labels.sample(n=3, random_state=51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "loss = 'binary_crossentropy'\n",
    "metrics = ['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 51\n",
    "\n",
    "sample_size = 2000\n",
    "event_count = 200\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Session IDs: 11779\n",
      "Sample Session IDs: 2000\n"
     ]
    }
   ],
   "source": [
    "# select all the unique session ids\n",
    "all_session_ids = df_source_labels['session_id'].unique()\n",
    "print('All Session IDs:', len(all_session_ids))\n",
    "\n",
    "# create a sample for testing\n",
    "session_ids = np.random.choice(all_session_ids, size=sample_size, replace=False)\n",
    "print('Sample Session IDs:', len(session_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1400\n",
      "Validation: 300\n",
      "Test: 300\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into train, validation and test sets\n",
    "train, test = train_test_split(session_ids, test_size=0.3)\n",
    "test, val = train_test_split(test, test_size=0.5)\n",
    "\n",
    "# print the number of sessions in each set\n",
    "print(f'Train: {len(train)}')\n",
    "print(f'Validation: {len(val)}')\n",
    "print(f'Test: {len(test)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training dataset\n",
    "X_train, y_train = create_dataset(\n",
    "    X=df_source, y=df_source_labels, session_list=train, event_count=event_count)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the validation dataset\n",
    "X_val, y_val = create_dataset(\n",
    "    X=df_source, y=df_source_labels, session_list=val, event_count=event_count)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the test dataset\n",
    "X_test, y_test = create_dataset(\n",
    "    X=df_source, y=df_source_labels, session_list=test, event_count=event_count)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.LSTM(32, input_shape=(event_count, len(vector_columns))))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_model(\n",
    "    model=model,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    optimizer=optimizers.RMSprop(learning_rate=0.0001),\n",
    "    loss=loss,\n",
    "    metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = test_model(\n",
    "    model=model,\n",
    "    history=history,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsm010",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "429f3e8d45833d845e6e031dbf3e229703adf0bd2129019c99d8da7ba46dd29e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
