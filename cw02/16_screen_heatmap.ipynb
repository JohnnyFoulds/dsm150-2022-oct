{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Screen Coordinates\n",
    "\n",
    "Attempt to train a model using a heatmap of the screen coordinates in the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 11:43:51.181054: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-03 11:43:51.751345: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-03 11:43:51.751387: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-03 11:43:51.751393: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import logging\n",
    "from typing import Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import callbacks\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13174211, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>event_name</th>\n",
       "      <th>name</th>\n",
       "      <th>level</th>\n",
       "      <th>page</th>\n",
       "      <th>room_coor_x</th>\n",
       "      <th>room_coor_y</th>\n",
       "      <th>screen_coor_x</th>\n",
       "      <th>screen_coor_y</th>\n",
       "      <th>hover_duration</th>\n",
       "      <th>text</th>\n",
       "      <th>fqid</th>\n",
       "      <th>room_fqid</th>\n",
       "      <th>text_fqid</th>\n",
       "      <th>fullscreen</th>\n",
       "      <th>hq</th>\n",
       "      <th>music</th>\n",
       "      <th>level_group</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>0</td>\n",
       "      <td>cutscene_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>undefined</td>\n",
       "      <td>intro</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.intro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>1323</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Whatcha doing over there, Jo?</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>831</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just talking to Teddy.</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              session_id  elapsed_time      event_name   name  level  page  \\\n",
       "index                                                                        \n",
       "0      20090312431273200             0  cutscene_click  basic      0   NaN   \n",
       "1      20090312431273200          1323    person_click  basic      0   NaN   \n",
       "2      20090312431273200           831    person_click  basic      0   NaN   \n",
       "\n",
       "       room_coor_x  room_coor_y  screen_coor_x  screen_coor_y  hover_duration  \\\n",
       "index                                                                           \n",
       "0      -413.991405  -159.314686          380.0          494.0             NaN   \n",
       "1      -413.991405  -159.314686          380.0          494.0             NaN   \n",
       "2      -413.991405  -159.314686          380.0          494.0             NaN   \n",
       "\n",
       "                                text    fqid                       room_fqid  \\\n",
       "index                                                                          \n",
       "0                          undefined   intro  tunic.historicalsociety.closet   \n",
       "1      Whatcha doing over there, Jo?  gramps  tunic.historicalsociety.closet   \n",
       "2             Just talking to Teddy.  gramps  tunic.historicalsociety.closet   \n",
       "\n",
       "                                               text_fqid  fullscreen  hq  \\\n",
       "index                                                                      \n",
       "0                   tunic.historicalsociety.closet.intro         NaN NaN   \n",
       "1      tunic.historicalsociety.closet.gramps.intro_0_...         NaN NaN   \n",
       "2      tunic.historicalsociety.closet.gramps.intro_0_...         NaN NaN   \n",
       "\n",
       "       music level_group  \n",
       "index                     \n",
       "0        NaN         0-4  \n",
       "1        NaN         0-4  \n",
       "2        NaN         0-4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the source training set\n",
    "df_source = pd.read_csv('data/train.csv.gz', compression='gzip', index_col=1)\n",
    "\n",
    "print(df_source.shape)\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_source.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(212022, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200_q1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312433251036_q1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090314121766812_q1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             session_id  correct\n",
       "0  20090312431273200_q1        1\n",
       "1  20090312433251036_q1        0\n",
       "2  20090314121766812_q1        1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the source training labels\n",
    "df_source_labels = pd.read_csv('data/train_labels.csv')\n",
    "\n",
    "print(df_source_labels.shape)\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_source_labels.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_question_to_level_group(question_number):\n",
    "    \"\"\"\n",
    "    Maps the question number to the level group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question_number : int\n",
    "        The question number.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The level group.\n",
    "    \"\"\"\n",
    "    if question_number in [1, 2, 3]:\n",
    "        return '0-4'\n",
    "    elif question_number in [4, 5, 6, 7, 8, 9, 10, 11, 12, 13]:\n",
    "        return '5-12'\n",
    "    elif question_number in [14, 15, 16, 17, 18]:\n",
    "        return '13-22'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def prepare_label_dataset(data : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the label dataset and add columns for the level group \n",
    "    and the question number.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The label dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The prepared label dataset.\n",
    "    \"\"\"\n",
    "    # add the columns to determine the level group\n",
    "    df_labels = data \\\n",
    "        .rename(columns={'session_id': 'id'}) \\\n",
    "        .assign(session_id=lambda df: df['id'].str.split('_').str[0].astype(int)) \\\n",
    "        .assign(question_id=lambda df: df['id'].str.split('_').str[1]) \\\n",
    "        .assign(question_num=lambda df: df['question_id'].str[1:].astype(int)) \\\n",
    "        [['session_id', 'question_num', 'correct']]\n",
    "    \n",
    "    # add the level group column\n",
    "    df_labels['level_group'] = df_labels['question_num'].apply(map_question_to_level_group) \n",
    "        \n",
    "    return df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_main_dataset(data : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the main dataset by removing duplicates and removing \n",
    "    columns that are not needed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The main dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The prepared main dataset.\n",
    "    \"\"\"\n",
    "    empty_columns = ['fullscreen', 'hq', 'music', 'page', 'hover_duration']\n",
    "\n",
    "    df_main = data \\\n",
    "        .drop_duplicates() \\\n",
    "        .reset_index(drop=True) \\\n",
    "        .drop(empty_columns, axis=1) \\\n",
    "        .drop('text', axis=1)\n",
    "\n",
    "    return df_main"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_screen_coor(data: pd.DataFrame, session_id: int):\n",
    "    # get the session data\n",
    "    session_data = df_source[df_source['session_id'] == session_id]\n",
    "    session_data = session_data[['screen_coor_x', 'screen_coor_y']]\n",
    "    session_data = session_data.dropna()\n",
    "\n",
    "    # get the screen coordinates\n",
    "    screen_coor_x = session_data['screen_coor_x'].values\n",
    "    screen_coor_y = session_data['screen_coor_y'].values\n",
    "    \n",
    "    return screen_coor_x, screen_coor_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_heatmap(screen_coor_x, screen_coor_y):\n",
    "    # Create the 2D histogram\n",
    "    heatmap, xedges, yedges = np.histogram2d(screen_coor_y, screen_coor_x, bins=50)\n",
    "\n",
    "    # Apply logarithmic transformation\n",
    "    heatmap = np.log(heatmap + 1)\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "# test the heatmap function\n",
    "session_1_screen_coor_x, session_1_screen_coor_y =  \\\n",
    "    get_session_screen_coor(df_source, 20090312431273200)\n",
    "\n",
    "session_1_heatmap = create_heatmap(session_1_screen_coor_x, session_1_screen_coor_y)\n",
    "print(session_1_heatmap.shape)\n",
    "\n",
    "plt.imshow(session_1_heatmap, cmap='hot', origin='lower')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reshape for convolutional neural network \n",
    "# a = session_1_heatmap.reshape((session_1_heatmap.shape[0], session_1_heatmap.shape[1], 1))\n",
    "\n",
    "# # try expand\n",
    "# b = np.expand_dims(session_1_heatmap, axis=-1)\n",
    "\n",
    "# np.array_equal(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50, 1)\n",
      "(50, 50, 1)\n",
      "(50, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "def create_heatmap_vector_array(session_data: pd.DataFrame, \n",
    "                        level_group: int) -> np.array:\n",
    "    \"\"\"\n",
    "    Creates a vector array for a specific session and question number.\n",
    "    \"\"\"\n",
    "    # get the data for the session and level group\n",
    "    df_session = session_data.query('level_group == @level_group')\n",
    "\n",
    "    # get the data to create a heatmap\n",
    "    heatmap_data = df_session[['screen_coor_x', 'screen_coor_y']].dropna()\n",
    "    screen_coor_x = heatmap_data['screen_coor_x'].values\n",
    "    screen_coor_y = heatmap_data['screen_coor_y'].values\n",
    "\n",
    "    # create the heatmap\n",
    "    heatmap = create_heatmap(screen_coor_x, screen_coor_y)\n",
    "\n",
    "    # reshape the heatmap for the convolutional neural network\n",
    "    vector_array = heatmap.reshape((heatmap.shape[0], heatmap.shape[1], 1))\n",
    "\n",
    "    return vector_array\n",
    "\n",
    "# test the vector array function\n",
    "df_test = df_source[df_source['session_id'] == 21040510125933256]\n",
    "test_vector_array_1 = create_heatmap_vector_array(df_source, '0-4')\n",
    "test_vector_array_2 = create_heatmap_vector_array(df_source, '5-12')\n",
    "test_vector_array_3 = create_heatmap_vector_array(df_source, '13-22')\n",
    "\n",
    "print(test_vector_array_1.shape)\n",
    "print(test_vector_array_2.shape)\n",
    "print(test_vector_array_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_source = prepare_main_dataset(df_source)\n",
    "# df_source_labels = prepare_label_dataset(df_source_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X: pd.DataFrame,\n",
    "                   y: pd.DataFrame,\n",
    "                   session_list: list) -> Tuple[np.array, np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Creates a dataset for a specific set of sessions and question numbers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        The main dataset.\n",
    "\n",
    "    y : pd.DataFrame\n",
    "        The label dataset.\n",
    "\n",
    "    session_ids : list\n",
    "        The list of session ids.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.array, np.array]\n",
    "        The features vector, image vector, and the labels.\n",
    "    \"\"\"\n",
    "    X_image = []\n",
    "    X_features = []\n",
    "    y_dataset = []\n",
    "\n",
    "    for session_id in tqdm(session_list):\n",
    "        # get the session labels\n",
    "        df_session_labels = y.query('session_id == @session_id')\n",
    "        df_session = X.query('session_id == @session_id')\n",
    "\n",
    "        # create the level group heatmaps\n",
    "        vector_arrays = {\n",
    "            '0-4': create_heatmap_vector_array(df_session, '0-4'),\n",
    "            '5-12': create_heatmap_vector_array(df_session, '5-12'),\n",
    "            '13-22': create_heatmap_vector_array(df_session, '13-22'),\n",
    "        }\n",
    "        # iterate over all the questions answered in the session\n",
    "        for _, row in df_session_labels.iterrows():\n",
    "            question_number = row['question_num']\n",
    "            correct = row['correct']\n",
    "            level_group = map_question_to_level_group(question_number)\n",
    "\n",
    "            # append the output features\n",
    "            X_features.append(row['question_num'])\n",
    "\n",
    "            # append the output image\n",
    "            X_image.append(vector_arrays[level_group])\n",
    "\n",
    "            # add the label to the dataset\n",
    "            y_dataset.append(correct)\n",
    "\n",
    "    X_out_features = np.array(X_features, dtype=np.float64)\n",
    "    X_out_image = np.array(X_image, dtype=np.float64)\n",
    "    y_out = np.array(y_dataset)\n",
    "\n",
    "    return X_out_features, X_out_image, y_out\n",
    "\n",
    "# # test the function\n",
    "# session_list = [20090312431273200, 21040510125933256, 21040510125933256]\n",
    "# X_test_features, X_test_image, y_test = create_dataset(df_source, df_source_labels, session_list)\n",
    "\n",
    "# print(X_test_features.shape)\n",
    "# print(X_test_image.shape)\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_sessions(\n",
    "        y: pd.DataFrame,\n",
    "        sample_size: int,\n",
    "        random_state: int=1337) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Selects a sample of sessions from the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : pd.DataFrame\n",
    "        The label dataset.\n",
    "    sample_size : int\n",
    "        The number of sessions to select.\n",
    "    random_state : int\n",
    "        The random state to use.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, np.ndarray, np.ndarray]\n",
    "        The selected session ids, the main dataset and the label dataset.\n",
    "    \"\"\"\n",
    "    # select all the unique session ids\n",
    "    all_session_ids = y['session_id'].unique()\n",
    "\n",
    "    # create a sample for testing\n",
    "    session_ids = np.random.choice(all_session_ids, size=sample_size, replace=False)\n",
    "\n",
    "    # split the dataset into train, validation and test sets\n",
    "    train, test = train_test_split(session_ids, test_size=0.3)\n",
    "    test, val = train_test_split(test, test_size=0.5)\n",
    "\n",
    "    # print the number of sessions in each set\n",
    "    print(f'Train: {len(train)}')\n",
    "    print(f'Validation: {len(val)}')\n",
    "    print(f'Test: {len(test)}')\n",
    "\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history: callbacks.History, figsize: Tuple[int, int] = (5, 3)) -> None:\n",
    "    \"\"\"\n",
    "    Plot the loss and validation loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    history : keras.callbacks.History\n",
    "        The history of the model training.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(epochs, history.history['loss'])\n",
    "    \n",
    "    if ('val_loss' in history.history):\n",
    "        plt.plot(epochs, history.history['val_loss'])\n",
    "        plt.legend(['Training loss', 'Validation loss'], loc='upper left')\n",
    "        plt.title('Training and validation loss')\n",
    "    else:\n",
    "        plt.title('Training loss')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(history: callbacks.History, figsize: Tuple[int, int] = (5, 3)) -> None:\n",
    "    \"\"\"\n",
    "    Plot the accuracy and validation accuracy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    history : keras.callbacks.History\n",
    "        The history of the model training.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(epochs, history.history['accuracy'])\n",
    "\n",
    "    if ('val_accuracy' in history.history):\n",
    "        plt.plot(epochs, history.history['val_accuracy'])\n",
    "        plt.legend(['Training acc', 'Validation acc'], loc='upper left')\n",
    "        plt.title('Training and validation accuracy')\n",
    "    else:\n",
    "        plt.title('Training accuracy')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(\n",
    "        model,\n",
    "        history: callbacks.History,\n",
    "        X_test: np.ndarray,\n",
    "        y_test: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Test the model based on the test data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : keras.models\n",
    "        The model to test.\n",
    "    history : keras.callbacks.History\n",
    "        The history of the training.\n",
    "    X_test : np.ndarray\n",
    "        The test data.\n",
    "    y_test : np.ndarray\n",
    "        The test labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The optimized threshold for the best F1 score.\n",
    "    \"\"\"\n",
    "    \n",
    "    plot_loss(history)\n",
    "    plot_accuracy(history)\n",
    "\n",
    "    y_test_score = model.predict(X_test)\n",
    "    threshold, _, _ = optimize_f1(y_test, y_test_score)\n",
    "\n",
    "    print(classification_report(y_test, y_test_score > threshold))\n",
    "    print(f'Optimized threshold for best F1: {threshold:.2f}')\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_f1(y_true: np.ndarray, y_score: np.ndarray) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Optimize the F1 score.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        The true labels.\n",
    "    y_score : np.ndarray\n",
    "        The predicted labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[float, float, float]\n",
    "        The optimized threshold, precision, and recall.\n",
    "    \"\"\"\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0\n",
    "    best_precision = 0\n",
    "    best_recall = 0\n",
    "\n",
    "    for threshold in np.arange(0, 1, 0.01):\n",
    "        y_pred = (y_score > threshold).astype(int)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=1)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_precision = precision\n",
    "            best_recall = recall\n",
    "\n",
    "    return best_threshold, best_precision, best_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-defined Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13173445, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>event_name</th>\n",
       "      <th>name</th>\n",
       "      <th>level</th>\n",
       "      <th>room_coor_x</th>\n",
       "      <th>room_coor_y</th>\n",
       "      <th>screen_coor_x</th>\n",
       "      <th>screen_coor_y</th>\n",
       "      <th>fqid</th>\n",
       "      <th>room_fqid</th>\n",
       "      <th>text_fqid</th>\n",
       "      <th>level_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>0</td>\n",
       "      <td>cutscene_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>intro</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.intro</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>1323</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>831</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          session_id  elapsed_time      event_name   name  level  room_coor_x  \\\n",
       "0  20090312431273200             0  cutscene_click  basic      0  -413.991405   \n",
       "1  20090312431273200          1323    person_click  basic      0  -413.991405   \n",
       "2  20090312431273200           831    person_click  basic      0  -413.991405   \n",
       "\n",
       "   room_coor_y  screen_coor_x  screen_coor_y    fqid  \\\n",
       "0  -159.314686          380.0          494.0   intro   \n",
       "1  -159.314686          380.0          494.0  gramps   \n",
       "2  -159.314686          380.0          494.0  gramps   \n",
       "\n",
       "                        room_fqid  \\\n",
       "0  tunic.historicalsociety.closet   \n",
       "1  tunic.historicalsociety.closet   \n",
       "2  tunic.historicalsociety.closet   \n",
       "\n",
       "                                           text_fqid level_group  \n",
       "0               tunic.historicalsociety.closet.intro         0-4  \n",
       "1  tunic.historicalsociety.closet.gramps.intro_0_...         0-4  \n",
       "2  tunic.historicalsociety.closet.gramps.intro_0_...         0-4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare the main dataset\n",
    "df_source = prepare_main_dataset(df_source)\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    print(df_source.shape)\n",
    "    display(df_source.head(3))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>question_num</th>\n",
       "      <th>correct</th>\n",
       "      <th>level_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21476</th>\n",
       "      <td>22010116250792520</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84068</th>\n",
       "      <td>21000111433937450</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171219</th>\n",
       "      <td>21040510125933256</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>13-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               session_id  question_num  correct level_group\n",
       "21476   22010116250792520             2        1         0-4\n",
       "84068   21000111433937450             8        1        5-12\n",
       "171219  21040510125933256            15        0       13-22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare the label dataset\n",
    "df_source_labels = prepare_label_dataset(df_source_labels)\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_source_labels.sample(n=3, random_state=51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "loss = 'binary_crossentropy'\n",
    "metrics = ['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 51\n",
    "#sample_size = df_source_labels['session_id'].nunique()\n",
    "sample_size = 1000\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 700\n",
      "Validation: 150\n",
      "Test: 150\n"
     ]
    }
   ],
   "source": [
    "train, val, test = select_sessions(\n",
    "    y=df_source_labels,\n",
    "    sample_size=sample_size,\n",
    "    random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6cbcbc3784494ca22f87d60988e49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "779"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the training dataset\n",
    "X_train_features, X_train_image, y_train = create_dataset(\n",
    "    X=df_source, y=df_source_labels, session_list=train)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21bd19d6c7e94b5cb65a41d69c080858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "323"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the validation dataset\n",
    "X_val_features, X_val_image, y_val = create_dataset(\n",
    "    X=df_source, y=df_source_labels, session_list=val)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8cceefa287447da7f5de724489cb6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1172"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the test dataset\n",
    "X_test_features, X_test_image, y_test = create_dataset(\n",
    "    X=df_source, y=df_source_labels, session_list=test)\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
