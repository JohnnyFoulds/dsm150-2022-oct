{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Screen Coordinates\n",
    "\n",
    "Attempt to train a model using a heatmap of the screen coordinates in the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 17:02:03.922929: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-03 17:02:04.422667: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-03 17:02:04.422720: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-03 17:02:04.422726: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import logging\n",
    "from typing import Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, concatenate\n",
    "from keras import callbacks\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13174211, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>event_name</th>\n",
       "      <th>name</th>\n",
       "      <th>level</th>\n",
       "      <th>page</th>\n",
       "      <th>room_coor_x</th>\n",
       "      <th>room_coor_y</th>\n",
       "      <th>screen_coor_x</th>\n",
       "      <th>screen_coor_y</th>\n",
       "      <th>hover_duration</th>\n",
       "      <th>text</th>\n",
       "      <th>fqid</th>\n",
       "      <th>room_fqid</th>\n",
       "      <th>text_fqid</th>\n",
       "      <th>fullscreen</th>\n",
       "      <th>hq</th>\n",
       "      <th>music</th>\n",
       "      <th>level_group</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>0</td>\n",
       "      <td>cutscene_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>undefined</td>\n",
       "      <td>intro</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.intro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>1323</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Whatcha doing over there, Jo?</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>831</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just talking to Teddy.</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              session_id  elapsed_time      event_name   name  level  page  \\\n",
       "index                                                                        \n",
       "0      20090312431273200             0  cutscene_click  basic      0   NaN   \n",
       "1      20090312431273200          1323    person_click  basic      0   NaN   \n",
       "2      20090312431273200           831    person_click  basic      0   NaN   \n",
       "\n",
       "       room_coor_x  room_coor_y  screen_coor_x  screen_coor_y  hover_duration  \\\n",
       "index                                                                           \n",
       "0      -413.991405  -159.314686          380.0          494.0             NaN   \n",
       "1      -413.991405  -159.314686          380.0          494.0             NaN   \n",
       "2      -413.991405  -159.314686          380.0          494.0             NaN   \n",
       "\n",
       "                                text    fqid                       room_fqid  \\\n",
       "index                                                                          \n",
       "0                          undefined   intro  tunic.historicalsociety.closet   \n",
       "1      Whatcha doing over there, Jo?  gramps  tunic.historicalsociety.closet   \n",
       "2             Just talking to Teddy.  gramps  tunic.historicalsociety.closet   \n",
       "\n",
       "                                               text_fqid  fullscreen  hq  \\\n",
       "index                                                                      \n",
       "0                   tunic.historicalsociety.closet.intro         NaN NaN   \n",
       "1      tunic.historicalsociety.closet.gramps.intro_0_...         NaN NaN   \n",
       "2      tunic.historicalsociety.closet.gramps.intro_0_...         NaN NaN   \n",
       "\n",
       "       music level_group  \n",
       "index                     \n",
       "0        NaN         0-4  \n",
       "1        NaN         0-4  \n",
       "2        NaN         0-4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the source training set\n",
    "df_source = pd.read_csv('data/train.csv.gz', compression='gzip', index_col=1)\n",
    "\n",
    "print(df_source.shape)\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_source.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(212022, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200_q1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312433251036_q1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090314121766812_q1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             session_id  correct\n",
       "0  20090312431273200_q1        1\n",
       "1  20090312433251036_q1        0\n",
       "2  20090314121766812_q1        1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the source training labels\n",
    "df_source_labels = pd.read_csv('data/train_labels.csv')\n",
    "\n",
    "print(df_source_labels.shape)\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_source_labels.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_question_to_level_group(question_number):\n",
    "    \"\"\"\n",
    "    Maps the question number to the level group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question_number : int\n",
    "        The question number.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The level group.\n",
    "    \"\"\"\n",
    "    if question_number in [1, 2, 3]:\n",
    "        return '0-4'\n",
    "    elif question_number in [4, 5, 6, 7, 8, 9, 10, 11, 12, 13]:\n",
    "        return '5-12'\n",
    "    elif question_number in [14, 15, 16, 17, 18]:\n",
    "        return '13-22'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def prepare_label_dataset(data : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the label dataset and add columns for the level group \n",
    "    and the question number.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The label dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The prepared label dataset.\n",
    "    \"\"\"\n",
    "    # add the columns to determine the level group\n",
    "    df_labels = data \\\n",
    "        .rename(columns={'session_id': 'id'}) \\\n",
    "        .assign(session_id=lambda df: df['id'].str.split('_').str[0].astype(int)) \\\n",
    "        .assign(question_id=lambda df: df['id'].str.split('_').str[1]) \\\n",
    "        .assign(question_num=lambda df: df['question_id'].str[1:].astype(int)) \\\n",
    "        [['session_id', 'question_num', 'correct']]\n",
    "    \n",
    "    # add the level group column\n",
    "    df_labels['level_group'] = df_labels['question_num'].apply(map_question_to_level_group) \n",
    "        \n",
    "    return df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_main_dataset(data : pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the main dataset by removing duplicates and removing \n",
    "    columns that are not needed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The main dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The prepared main dataset.\n",
    "    \"\"\"\n",
    "    empty_columns = ['fullscreen', 'hq', 'music', 'page', 'hover_duration']\n",
    "\n",
    "    df_main = data \\\n",
    "        .drop_duplicates() \\\n",
    "        .reset_index(drop=True) \\\n",
    "        .drop(empty_columns, axis=1) \\\n",
    "        .drop('text', axis=1)\n",
    "\n",
    "    return df_main"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_screen_coor(data: pd.DataFrame, session_id: int):\n",
    "    # get the session data\n",
    "    session_data = df_source[df_source['session_id'] == session_id]\n",
    "    session_data = session_data[['screen_coor_x', 'screen_coor_y']]\n",
    "    session_data = session_data.dropna()\n",
    "\n",
    "    # get the screen coordinates\n",
    "    screen_coor_x = session_data['screen_coor_x'].values\n",
    "    screen_coor_y = session_data['screen_coor_y'].values\n",
    "    \n",
    "    return screen_coor_x, screen_coor_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_heatmap(screen_coor_x, screen_coor_y):\n",
    "    # Create the 2D histogram\n",
    "    heatmap, xedges, yedges = np.histogram2d(screen_coor_y, screen_coor_x, bins=50)\n",
    "\n",
    "    # Apply logarithmic transformation\n",
    "    heatmap = np.log(heatmap + 1)\n",
    "\n",
    "    # Normalize the heatmap\n",
    "    normalized_heatmap = (heatmap - np.min(heatmap)) / (np.max(heatmap) - np.min(heatmap))\n",
    "\n",
    "    # Scale the heatmap to the range 0-255\n",
    "    scaled_heatmap = (normalized_heatmap * 255).astype(np.uint8)\n",
    "\n",
    "    return scaled_heatmap\n",
    "\n",
    "# test the heatmap function\n",
    "session_1_screen_coor_x, session_1_screen_coor_y =  \\\n",
    "    get_session_screen_coor(df_source, 20090312431273200)\n",
    "\n",
    "session_1_heatmap = create_heatmap(session_1_screen_coor_x, session_1_screen_coor_y)\n",
    "print(session_1_heatmap.shape)\n",
    "\n",
    "plt.imshow(session_1_heatmap, cmap='hot', origin='lower')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reshape for convolutional neural network \n",
    "# a = session_1_heatmap.reshape((session_1_heatmap.shape[0], session_1_heatmap.shape[1], 1))\n",
    "\n",
    "# # try expand\n",
    "# b = np.expand_dims(session_1_heatmap, axis=-1)\n",
    "\n",
    "# np.array_equal(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50, 1)\n",
      "(50, 50, 1)\n",
      "(50, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "def create_heatmap_vector_array(session_data: pd.DataFrame, \n",
    "                        level_group: int) -> np.array:\n",
    "    \"\"\"\n",
    "    Creates a vector array for a specific session and question number.\n",
    "    \"\"\"\n",
    "    # get the data for the session and level group\n",
    "    df_session = session_data.query('level_group == @level_group')\n",
    "\n",
    "    # get the data to create a heatmap\n",
    "    heatmap_data = df_session[['screen_coor_x', 'screen_coor_y']].dropna()\n",
    "    screen_coor_x = heatmap_data['screen_coor_x'].values\n",
    "    screen_coor_y = heatmap_data['screen_coor_y'].values\n",
    "\n",
    "    # create the heatmap\n",
    "    heatmap = create_heatmap(screen_coor_x, screen_coor_y)\n",
    "\n",
    "    # reshape the heatmap for the convolutional neural network\n",
    "    vector_array = heatmap.reshape((heatmap.shape[0], heatmap.shape[1], 1))\n",
    "\n",
    "    return vector_array\n",
    "\n",
    "# test the vector array function\n",
    "df_test = df_source[df_source['session_id'] == 21040510125933256]\n",
    "test_vector_array_1 = create_heatmap_vector_array(df_source, '0-4')\n",
    "test_vector_array_2 = create_heatmap_vector_array(df_source, '5-12')\n",
    "test_vector_array_3 = create_heatmap_vector_array(df_source, '13-22')\n",
    "\n",
    "print(test_vector_array_1.shape)\n",
    "print(test_vector_array_2.shape)\n",
    "print(test_vector_array_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_source = prepare_main_dataset(df_source)\n",
    "# df_source_labels = prepare_label_dataset(df_source_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X: pd.DataFrame,\n",
    "                   y: pd.DataFrame,\n",
    "                   session_list: list) -> Tuple[np.array, np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Creates a dataset for a specific set of sessions and question numbers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        The main dataset.\n",
    "\n",
    "    y : pd.DataFrame\n",
    "        The label dataset.\n",
    "\n",
    "    session_ids : list\n",
    "        The list of session ids.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.array, np.array]\n",
    "        The features vector, image vector, and the labels.\n",
    "    \"\"\"\n",
    "    X_image = []\n",
    "    X_features = []\n",
    "    y_dataset = []\n",
    "\n",
    "    for session_id in tqdm(session_list):\n",
    "        # get the session labels\n",
    "        df_session_labels = y.query('session_id == @session_id')\n",
    "        df_session = X.query('session_id == @session_id')\n",
    "\n",
    "        # create the level group heatmaps\n",
    "        vector_arrays = {\n",
    "            '0-4': create_heatmap_vector_array(df_session, '0-4'),\n",
    "            '5-12': create_heatmap_vector_array(df_session, '5-12'),\n",
    "            '13-22': create_heatmap_vector_array(df_session, '13-22'),\n",
    "        }\n",
    "        # iterate over all the questions answered in the session\n",
    "        for _, row in df_session_labels.iterrows():\n",
    "            question_number = row['question_num']\n",
    "            correct = row['correct']\n",
    "            level_group = map_question_to_level_group(question_number)\n",
    "\n",
    "            # append the output features\n",
    "            X_features.append([row['question_num'], 1])\n",
    "\n",
    "            # append the output image\n",
    "            X_image.append(vector_arrays[level_group])\n",
    "\n",
    "            # add the label to the dataset\n",
    "            y_dataset.append(correct)\n",
    "\n",
    "    X_out_features = np.array(X_features, dtype=np.float64)\n",
    "    X_out_image = np.array(X_image, dtype=np.float64)\n",
    "    y_out = np.array(y_dataset)\n",
    "\n",
    "    return X_out_features, X_out_image, y_out\n",
    "\n",
    "# # test the function\n",
    "# session_list = [20090312431273200, 21040510125933256, 21040510125933256]\n",
    "# X_test_features, X_test_image, y_test = create_dataset(df_source, df_source_labels, session_list)\n",
    "\n",
    "# print(X_test_features.shape)\n",
    "# print(X_test_image.shape)\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_sessions(\n",
    "        y: pd.DataFrame,\n",
    "        sample_size: int,\n",
    "        random_state: int=1337) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Selects a sample of sessions from the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : pd.DataFrame\n",
    "        The label dataset.\n",
    "    sample_size : int\n",
    "        The number of sessions to select.\n",
    "    random_state : int\n",
    "        The random state to use.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, np.ndarray, np.ndarray]\n",
    "        The selected session ids, the main dataset and the label dataset.\n",
    "    \"\"\"\n",
    "    # select all the unique session ids\n",
    "    all_session_ids = y['session_id'].unique()\n",
    "\n",
    "    # create a sample for testing\n",
    "    session_ids = np.random.choice(all_session_ids, size=sample_size, replace=False)\n",
    "\n",
    "    # split the dataset into train, validation and test sets\n",
    "    train, test = train_test_split(session_ids, test_size=0.3)\n",
    "    test, val = train_test_split(test, test_size=0.5)\n",
    "\n",
    "    # print the number of sessions in each set\n",
    "    print(f'Train: {len(train)}')\n",
    "    print(f'Validation: {len(val)}')\n",
    "    print(f'Test: {len(test)}')\n",
    "\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history: callbacks.History, figsize: Tuple[int, int] = (5, 3)) -> None:\n",
    "    \"\"\"\n",
    "    Plot the loss and validation loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    history : keras.callbacks.History\n",
    "        The history of the model training.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(epochs, history.history['loss'])\n",
    "    \n",
    "    if ('val_loss' in history.history):\n",
    "        plt.plot(epochs, history.history['val_loss'])\n",
    "        plt.legend(['Training loss', 'Validation loss'], loc='upper left')\n",
    "        plt.title('Training and validation loss')\n",
    "    else:\n",
    "        plt.title('Training loss')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(history: callbacks.History, figsize: Tuple[int, int] = (5, 3)) -> None:\n",
    "    \"\"\"\n",
    "    Plot the accuracy and validation accuracy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    history : keras.callbacks.History\n",
    "        The history of the model training.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(epochs, history.history['accuracy'])\n",
    "\n",
    "    if ('val_accuracy' in history.history):\n",
    "        plt.plot(epochs, history.history['val_accuracy'])\n",
    "        plt.legend(['Training acc', 'Validation acc'], loc='upper left')\n",
    "        plt.title('Training and validation accuracy')\n",
    "    else:\n",
    "        plt.title('Training accuracy')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(\n",
    "        model,\n",
    "        history: callbacks.History,\n",
    "        X_test: np.ndarray,\n",
    "        y_test: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Test the model based on the test data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : keras.models\n",
    "        The model to test.\n",
    "    history : keras.callbacks.History\n",
    "        The history of the training.\n",
    "    X_test : np.ndarray\n",
    "        The test data.\n",
    "    y_test : np.ndarray\n",
    "        The test labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The optimized threshold for the best F1 score.\n",
    "    \"\"\"\n",
    "    \n",
    "    plot_loss(history)\n",
    "    plot_accuracy(history)\n",
    "\n",
    "    y_test_score = model.predict(X_test)\n",
    "    threshold, _, _ = optimize_f1(y_test, y_test_score)\n",
    "\n",
    "    print(classification_report(y_test, y_test_score > threshold))\n",
    "    print(f'Optimized threshold for best F1: {threshold:.2f}')\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_f1(y_true: np.ndarray, y_score: np.ndarray) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Optimize the F1 score.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        The true labels.\n",
    "    y_score : np.ndarray\n",
    "        The predicted labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[float, float, float]\n",
    "        The optimized threshold, precision, and recall.\n",
    "    \"\"\"\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0\n",
    "    best_precision = 0\n",
    "    best_recall = 0\n",
    "\n",
    "    for threshold in np.arange(0, 1, 0.01):\n",
    "        y_pred = (y_score > threshold).astype(int)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=1)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_precision = precision\n",
    "            best_recall = recall\n",
    "\n",
    "    return best_threshold, best_precision, best_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-defined Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13173445, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>event_name</th>\n",
       "      <th>name</th>\n",
       "      <th>level</th>\n",
       "      <th>room_coor_x</th>\n",
       "      <th>room_coor_y</th>\n",
       "      <th>screen_coor_x</th>\n",
       "      <th>screen_coor_y</th>\n",
       "      <th>fqid</th>\n",
       "      <th>room_fqid</th>\n",
       "      <th>text_fqid</th>\n",
       "      <th>level_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>0</td>\n",
       "      <td>cutscene_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>intro</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.intro</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>1323</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090312431273200</td>\n",
       "      <td>831</td>\n",
       "      <td>person_click</td>\n",
       "      <td>basic</td>\n",
       "      <td>0</td>\n",
       "      <td>-413.991405</td>\n",
       "      <td>-159.314686</td>\n",
       "      <td>380.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>gramps</td>\n",
       "      <td>tunic.historicalsociety.closet</td>\n",
       "      <td>tunic.historicalsociety.closet.gramps.intro_0_...</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          session_id  elapsed_time      event_name   name  level  room_coor_x  \\\n",
       "0  20090312431273200             0  cutscene_click  basic      0  -413.991405   \n",
       "1  20090312431273200          1323    person_click  basic      0  -413.991405   \n",
       "2  20090312431273200           831    person_click  basic      0  -413.991405   \n",
       "\n",
       "   room_coor_y  screen_coor_x  screen_coor_y    fqid  \\\n",
       "0  -159.314686          380.0          494.0   intro   \n",
       "1  -159.314686          380.0          494.0  gramps   \n",
       "2  -159.314686          380.0          494.0  gramps   \n",
       "\n",
       "                        room_fqid  \\\n",
       "0  tunic.historicalsociety.closet   \n",
       "1  tunic.historicalsociety.closet   \n",
       "2  tunic.historicalsociety.closet   \n",
       "\n",
       "                                           text_fqid level_group  \n",
       "0               tunic.historicalsociety.closet.intro         0-4  \n",
       "1  tunic.historicalsociety.closet.gramps.intro_0_...         0-4  \n",
       "2  tunic.historicalsociety.closet.gramps.intro_0_...         0-4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare the main dataset\n",
    "df_source = prepare_main_dataset(df_source)\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    print(df_source.shape)\n",
    "    display(df_source.head(3))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>question_num</th>\n",
       "      <th>correct</th>\n",
       "      <th>level_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21476</th>\n",
       "      <td>22010116250792520</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84068</th>\n",
       "      <td>21000111433937450</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171219</th>\n",
       "      <td>21040510125933256</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>13-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               session_id  question_num  correct level_group\n",
       "21476   22010116250792520             2        1         0-4\n",
       "84068   21000111433937450             8        1        5-12\n",
       "171219  21040510125933256            15        0       13-22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare the label dataset\n",
    "df_source_labels = prepare_label_dataset(df_source_labels)\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_source_labels.sample(n=3, random_state=51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 51\n",
    "#sample_size = df_source_labels['session_id'].nunique()\n",
    "sample_size = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 350\n",
      "Validation: 75\n",
      "Test: 75\n"
     ]
    }
   ],
   "source": [
    "train, val, test = select_sessions(\n",
    "    y=df_source_labels,\n",
    "    sample_size=sample_size,\n",
    "    random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a522b18b8a004e7da06dd46f484289a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the training dataset\n",
    "X_train_features, X_train_image, y_train = create_dataset(\n",
    "    X=df_source, y=df_source_labels, session_list=train)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2064ead6f8b442d4947a9febedce3f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the validation dataset\n",
    "X_val_features, X_val_image, y_val = create_dataset(\n",
    "    X=df_source, y=df_source_labels, session_list=val)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae41bd3a20fd40df96e21e79c0ed1fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the test dataset\n",
    "X_test_features, X_test_image, y_test = create_dataset(\n",
    "    X=df_source, y=df_source_labels, session_list=test)\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "reference: [Conv2d for image with additional features as input layer](https://datascience.stackexchange.com/questions/106801/conv2d-for-image-with-additional-features-as-input-layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 17:03:02.911605: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-03 17:03:02.924369: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-03-03 17:03:02.924394: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-03-03 17:03:02.924684: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 1s 665us/step - loss: 0.2557 - accuracy: 0.9264\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 1s 652us/step - loss: 0.1113 - accuracy: 0.9675\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 1s 654us/step - loss: 0.0768 - accuracy: 0.9772\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 1s 654us/step - loss: 0.0574 - accuracy: 0.9823\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 1s 671us/step - loss: 0.0445 - accuracy: 0.9865\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 1s 662us/step - loss: 0.0350 - accuracy: 0.9890\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 1s 663us/step - loss: 0.0296 - accuracy: 0.9906\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 1s 677us/step - loss: 0.0228 - accuracy: 0.9932\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 1s 661us/step - loss: 0.0183 - accuracy: 0.9944\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 1s 663us/step - loss: 0.0160 - accuracy: 0.9952\n",
      "313/313 - 0s - loss: 0.0808 - accuracy: 0.9770 - 193ms/epoch - 617us/step\n",
      "\n",
      "Test loss: 0.08083678036928177\n",
      "Test accuracy: 0.9769999980926514\n"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Preprocess the images by scaling them to the range [0, 1]\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Define the model architecture using a sequential API\n",
    "model = keras.Sequential([\n",
    "  keras.layers.Flatten(input_shape=(28, 28)), # Flatten the input images to a vector of 784 pixels\n",
    "  keras.layers.Dense(128, activation='relu'), # Add a hidden layer with 128 neurons and ReLU activation\n",
    "  keras.layers.Dense(10) # Add an output layer with 10 neurons for each class (0-9)\n",
    "])\n",
    "\n",
    "# Compile the model with an optimizer, a loss function and a metric\n",
    "model.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data for 10 epochs\n",
    "model.fit(train_images, train_labels, epochs=10)\n",
    "\n",
    "# Evaluate the model on the test data and print the results\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print('\\nTest loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7165 - accuracy: 0.4900\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 896us/step - loss: 0.7095 - accuracy: 0.5500\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 800us/step - loss: 0.7044 - accuracy: 0.5800\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 774us/step - loss: 0.7003 - accuracy: 0.5700\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 780us/step - loss: 0.6986 - accuracy: 0.5300\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 780us/step - loss: 0.6969 - accuracy: 0.5100\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 779us/step - loss: 0.6960 - accuracy: 0.5200\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 812us/step - loss: 0.6955 - accuracy: 0.5400\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 796us/step - loss: 0.6951 - accuracy: 0.5500\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 784us/step - loss: 0.6946 - accuracy: 0.5600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f788c547f40>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Generate some random data\n",
    "X = np.random.rand(100, 10)\n",
    "y = np.random.randint(0, 2, size=(100,))\n",
    "\n",
    "# Define a simple model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the GPU working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 0s 952us/step - loss: 0.7066 - accuracy: 0.5400\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 877us/step - loss: 0.7021 - accuracy: 0.5400\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 849us/step - loss: 0.6985 - accuracy: 0.5900\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 863us/step - loss: 0.6958 - accuracy: 0.6300\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 904us/step - loss: 0.6929 - accuracy: 0.6400\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 793us/step - loss: 0.6903 - accuracy: 0.6400\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 752us/step - loss: 0.6883 - accuracy: 0.6400\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 777us/step - loss: 0.6866 - accuracy: 0.6500\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 813us/step - loss: 0.6846 - accuracy: 0.6600\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 761us/step - loss: 0.6830 - accuracy: 0.6500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7889cdd280>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate some random data\n",
    "X = np.random.rand(100, 10)\n",
    "y = np.random.randint(0, 2, size=(100,))\n",
    "\n",
    "# Define a simple model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if the conv2d layer is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_image.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6300, 50, 50, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3150/3150 [==============================] - 2s 572us/step - loss: 0.5893 - accuracy: 0.7113 - val_loss: 0.5961 - val_accuracy: 0.6896\n",
      "Epoch 2/10\n",
      "3150/3150 [==============================] - 2s 547us/step - loss: 0.5729 - accuracy: 0.7135 - val_loss: 0.5914 - val_accuracy: 0.6896\n",
      "Epoch 3/10\n",
      "3150/3150 [==============================] - 2s 570us/step - loss: 0.5710 - accuracy: 0.7132 - val_loss: 0.5938 - val_accuracy: 0.6896\n",
      "Epoch 4/10\n",
      "3150/3150 [==============================] - 2s 560us/step - loss: 0.5694 - accuracy: 0.7132 - val_loss: 0.5966 - val_accuracy: 0.6896\n",
      "Epoch 5/10\n",
      "3150/3150 [==============================] - 2s 576us/step - loss: 0.5693 - accuracy: 0.7135 - val_loss: 0.6007 - val_accuracy: 0.6896\n",
      "Epoch 6/10\n",
      "3150/3150 [==============================] - 2s 581us/step - loss: 0.5687 - accuracy: 0.7135 - val_loss: 0.5905 - val_accuracy: 0.6896\n",
      "Epoch 7/10\n",
      "3150/3150 [==============================] - 2s 571us/step - loss: 0.5679 - accuracy: 0.7135 - val_loss: 0.5870 - val_accuracy: 0.6896\n",
      "Epoch 8/10\n",
      "3150/3150 [==============================] - 2s 554us/step - loss: 0.5660 - accuracy: 0.7132 - val_loss: 0.6073 - val_accuracy: 0.6896\n",
      "Epoch 9/10\n",
      "3150/3150 [==============================] - 2s 561us/step - loss: 0.5668 - accuracy: 0.7135 - val_loss: 0.5966 - val_accuracy: 0.6896\n",
      "Epoch 10/10\n",
      "3150/3150 [==============================] - 2s 565us/step - loss: 0.5646 - accuracy: 0.7135 - val_loss: 0.5892 - val_accuracy: 0.6896\n"
     ]
    }
   ],
   "source": [
    "# # create the model\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), input_shape=(50, 50, 1)))\n",
    "# model.add(Flatten())\n",
    "# #model.add(Dense(1, activation='sigmoid'))\n",
    "# #keras.utils.plot_model(model, show_shapes=True)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "#                         input_shape=(50, 50, 1)))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dense(10, activation='relu', input_shape=(2,)))\n",
    "model.add(layers.Flatten()) \n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(\n",
    "    x=X_train_features,\n",
    "    y=y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val_features, y_val),\n",
    "    batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 106/3150 [>.............................] - ETA: 4:31 - loss: 17.0711 - accuracy: 0.5708"
     ]
    }
   ],
   "source": [
    "# # create the model\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), input_shape=(50, 50, 1)))\n",
    "# model.add(Flatten())\n",
    "# #model.add(Dense(1, activation='sigmoid'))\n",
    "# #keras.utils.plot_model(model, show_shapes=True)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "#                         input_shape=(50, 50, 1)))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dense(10, activation='relu', input_shape=(50, 50, 1)))\n",
    "model.add(layers.Flatten()) \n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(\n",
    "    x=X_train_image,\n",
    "    y=y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val_image, y_val),\n",
    "    batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_features.shape, X_val_image.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = Input(shape=(50, 50, 1))\n",
    "input_features = Input(shape=(0,))\n",
    "\n",
    "# apply convolutional layers to image branch\n",
    "x = Conv2D(54, 3)(input_image)\n",
    "x = Conv2D(64, 3)(x)\n",
    "x = MaxPool2D(2)(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "# concatenate flattened image branch with input features\n",
    "concat = concatenate([x, input_features])\n",
    "\n",
    "# apply dense layers on concatenated data\n",
    "dense = Dense(64)(concat)\n",
    "output = Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "# create models using inputs and output specified above\n",
    "model = Model(inputs=[input_image, input_features], outputs=output)\n",
    "\n",
    "# plot the model architecture\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=RMSprop(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([X_train_image, X_train_features], y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "history = model.fit(\n",
    "    x=[X_train_image, X_train_features],\n",
    "    y=y_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=([X_val_image, X_val_features], y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
