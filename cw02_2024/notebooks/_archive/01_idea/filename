 1/1: from memgpt import MemGPT
 1/2:
import os
from memgpt import MemGPT
 1/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
 1/4: export OPENAI_API_KEY=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
 1/5: %export OPENAI_API_KEY=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
 1/6: %%export OPENAI_API_KEY=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
 1/7:
%
export OPENAI_API_KEY=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
 1/8:
%%
export OPENAI_API_KEY=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
 1/9: os.environ["OPENAI_API_KEY"] = "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"
1/10:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
1/11:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  quickstart="openai",
  config={
    "openai_api_key": api_key
  }
)
1/12:
# You can set many more parameters, this is just a basic example
agent_state = client.create_agent(
  agent_config={
    "persona": "sam_pov",
    "human": "cs_phd",
  }
)
1/13:
# Now that we have an agent_name identifier, we can send it a message!
# The response will have data from the MemGPT agent
my_message = "Hi MemGPT! How's it going?"
response = client.user_message(agent_id=agent_state.id, message=my_message)
 2/1:
import os
from memgpt import MemGPT
 2/2: os.environ["OPENAI_API_KEY"] = "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"
 2/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
 2/4: ?MemGPT
 2/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  quickstart="openai",
  config={
    "openai_api_key": api_key
  }
)
 2/6: ?client.create_agent
 2/7:
# You can set many more parameters, this is just a basic example
agent_state = client.create_agent(
  agent_config={
    "persona": "sam_pov",
    "human": "cs_phd",
  }
)
 2/8:
# Now that we have an agent_name identifier, we can send it a message!
# The response will have data from the MemGPT agent
my_message = "Hi MemGPT! How's it going?"
response = client.user_message(agent_id=agent_state.id, message=my_message)
 4/1:
import os
from memgpt import MemGPT
 4/2: os.environ["OPENAI_API_KEY"] = "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"
 4/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
 4/4: ?MemGPT
 4/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  auto_save=True,
  quickstart=QuickstartChoice.memgpt_hosted,
  config={
    "openai_api_key": api_key
  }
)
 5/1:
import os
from memgpt import MemGPT
from memgpt.cli.cli import QuickstartChoice
 6/1:
import os
from memgpt import MemGPT
from memgpt.cli.cli import QuickstartChoice
 6/2: os.environ["OPENAI_API_KEY"] = "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"
 6/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
 6/4: ?MemGPT
 6/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  auto_save=True,
  quickstart=QuickstartChoice.memgpt_hosted,
  config={
    "openai_api_key": api_key
  }
)
 6/6: ?QuickstartChoice
 7/1:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  auto_save=True,
  quickstart=QuickstartChoice.openai,
  config={
    "openai_api_key": api_key
  }
)
 8/1:
import os
from memgpt import MemGPT
from memgpt.cli.cli import QuickstartChoice
 8/2: os.environ["OPENAI_API_KEY"] = "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"
 8/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
 8/4: ?MemGPT
 8/5: ?QuickstartChoice
 8/6:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  auto_save=True,
  quickstart=QuickstartChoice.openai,
  config={
    "openai_api_key": api_key
  }
)
 9/1:
import os
from memgpt import MemGPT
from memgpt.cli.cli import QuickstartChoice
 9/2: os.environ["OPENAI_API_KEY"] = "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"
 9/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
 9/4: ?MemGPT
 9/5: ?QuickstartChoice
 9/6:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key
  }
)
10/1:
import os
from memgpt import MemGPT
from memgpt.cli.cli import QuickstartChoice
10/2: os.environ["OPENAI_API_KEY"] = "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"
10/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
10/4: ?MemGPT
10/5: ?QuickstartChoice
10/6:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key
  }
)
10/7: client
11/1:
import os
from memgpt import MemGPT
from memgpt.cli.cli import QuickstartChoice
11/2: os.environ["OPENAI_API_KEY"] = "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"
11/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
11/4: ?MemGPT
11/5: ?QuickstartChoice
11/6:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key
  }
)
11/7: client
11/8: ?client.create_agent
12/1:
import os
from memgpt import MemGPT
from memgpt.cli.cli import QuickstartChoice
12/2: os.environ["OPENAI_API_KEY"] = "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"
12/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
12/4: ?MemGPT
12/5: ?QuickstartChoice
12/6:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key
  }
)
12/7: client
12/8: ?client.create_agent
12/9:
# Create an AgentConfig with default persona and human txt
# In this case, assume we wrote a custom persona file "my_persona.txt", located at ~/.memgpt/personas/my_persona.txt
# Same for a custom user file "my_user.txt", located at ~/.memgpt/humans/my_user.txt
agent_config = AgentConfig(
    name="CustomAgent",
    persona="my_persona",
    human="my_user",
    preset="memgpt_chat",
    model="gpt-3.5-turbo",
)
13/1:
import os
from memgpt import MemGPT
from memgpt.cli.cli import QuickstartChoice
13/2: os.environ["OPENAI_API_KEY"] = "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"
13/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
13/4: ?MemGPT
13/5: ?QuickstartChoice
13/6:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key
  }
)
13/7: client
13/8: ?client.create_agent
13/9:
# Create an AgentConfig with default persona and human txt
# In this case, assume we wrote a custom persona file "my_persona.txt", located at ~/.memgpt/personas/my_persona.txt
# Same for a custom user file "my_user.txt", located at ~/.memgpt/humans/my_user.txt
agent_config = AgentConfig(
    name="CustomAgent",
    persona="my_persona",
    human="my_user",
    preset="memgpt_chat",
    model="gpt-3.5-turbo",
)
13/10:
import os

from memgpt import MemGPT
from memgpt.config import AgentConfig
from memgpt.cli.cli import QuickstartChoice
13/11:
# Create an AgentConfig with default persona and human txt
# In this case, assume we wrote a custom persona file "my_persona.txt", located at ~/.memgpt/personas/my_persona.txt
# Same for a custom user file "my_user.txt", located at ~/.memgpt/humans/my_user.txt
agent_config = AgentConfig(
    name="CustomAgent",
    persona="my_persona",
    human="my_user",
    preset="memgpt_chat",
    model="gpt-3.5-turbo",
)
13/12:
# Now that we have an agent_name identifier, we can send it a message!
# The response will have data from the MemGPT agent
my_message = "Hi MemGPT! How's it going?"
response = client.user_message(agent_id=agent_state.id, message=my_message)
13/13:
# Create the agent according to AgentConfig we set up. If an agent with
# the same name already exists it will simply return, unless you set
# throw_if_exists to 'True'
agent_state = client.create_agent(agent_config=agent_config)
14/1:
import os

from memgpt import MemGPT
from memgpt.config import AgentConfig
from memgpt.cli.cli import QuickstartChoice
14/2: os.environ["OPENAI_API_KEY"] = "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"
14/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
14/4: ?MemGPT
14/5: ?QuickstartChoice
14/6:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key
  }
)
14/7: client
14/8: ?client.create_agent
14/9:
# Create an AgentConfig with default persona and human txt
# In this case, assume we wrote a custom persona file "my_persona.txt", located at ~/.memgpt/personas/my_persona.txt
# Same for a custom user file "my_user.txt", located at ~/.memgpt/humans/my_user.txt
agent_config = AgentConfig(
    name="CustomAgent",
    persona="my_persona",
    human="my_user",
    preset="memgpt_chat",
    model="gpt-3.5-turbo",
)
14/10:
# Create the agent according to AgentConfig we set up. If an agent with
# the same name already exists it will simply return, unless you set
# throw_if_exists to 'True'
agent_state = client.create_agent(agent_config=agent_config)
14/11: agent_config
14/12: ?agent_config
14/13: client.create_agent(agent_config)
14/14:
# Create an AgentConfig with default persona and human txt
# In this case, assume we wrote a custom persona file "my_persona.txt", located at ~/.memgpt/personas/my_persona.txt
# Same for a custom user file "my_user.txt", located at ~/.memgpt/humans/my_user.txt
agent_config = AgentConfig(
    name="CustomAgent",
    persona="my_persona",
    human="my_user",
    preset="memgpt_chat",
    model="gpt-3.5-turbo",
)
14/15:
# Create an AgentConfig with default persona and human txt
# In this case, assume we wrote a custom persona file "my_persona.txt", located at ~/.memgpt/personas/my_persona.txt
# Same for a custom user file "my_user.txt", located at ~/.memgpt/humans/my_user.txt
agent_config = AgentConfig(
    name="CustomAgent",
    persona="my_persona",
    human="my_user",
    preset="memgpt_chat",
    model="gpt-3.5-turbo",
)
14/16: ?agent_config
14/17:
# Create an AgentConfig with default persona and human txt
# In this case, assume we wrote a custom persona file "my_persona.txt", located at ~/.memgpt/personas/my_persona.txt
# Same for a custom user file "my_user.txt", located at ~/.memgpt/humans/my_user.txt
agent_config = AgentState(
    name="CustomAgent",
    persona="my_persona",
    human="my_user",
    preset="memgpt_chat",
    model="gpt-3.5-turbo",
)
14/18:
import os

from memgpt import MemGPT
from memgpt.config import AgentState
from memgpt.cli.cli import QuickstartChoice
14/19:
# Create an AgentConfig with default persona and human txt
# In this case, assume we wrote a custom persona file "my_persona.txt", located at ~/.memgpt/personas/my_persona.txt
# Same for a custom user file "my_user.txt", located at ~/.memgpt/humans/my_user.txt
agent_config = AgentState(
    name="CustomAgent",
    persona="my_persona",
    human="my_user",
    preset="memgpt_chat",
    model="gpt-3.5-turbo",
)
14/20:
# Create an AgentConfig with default persona and human txt
# In this case, assume we wrote a custom persona file "my_persona.txt", located at ~/.memgpt/personas/my_persona.txt
# Same for a custom user file "my_user.txt", located at ~/.memgpt/humans/my_user.txt
agent_config = AgentConfig(
    name="CustomAgent",
    persona="my_persona",
    human="my_user",
    preset="memgpt_chat",
    model="gpt-3.5-turbo",
)
14/21: agent_config.to_dict()
14/22: agent_config.to_agent_config_dict()
14/23:
# Create an AgentConfig with default persona and human txt
# In this case, assume we wrote a custom persona file "my_persona.txt", located at ~/.memgpt/personas/my_persona.txt
# Same for a custom user file "my_user.txt", located at ~/.memgpt/humans/my_user.txt
agent_config = AgentState(
    name="CustomAgent",
    persona="my_persona",
    human="my_user",
    preset="memgpt_chat",
)
14/24:
# Create an AgentConfig with default persona and human txt
# In this case, assume we wrote a custom persona file "my_persona.txt", located at ~/.memgpt/personas/my_persona.txt
# Same for a custom user file "my_user.txt", located at ~/.memgpt/humans/my_user.txt
agent_config = AgentConfig(
    name="CustomAgent",
    persona="my_persona",
    human="my_user",
    preset="memgpt_chat",
    model="gpt-3.5-turbo",
)
14/25: agent_config.to_agent_state()
14/26: agent_config.llm_config
14/27:
# Create an AgentConfig with default persona and human txt
# In this case, assume we wrote a custom persona file "my_persona.txt", located at ~/.memgpt/personas/my_persona.txt
# Same for a custom user file "my_user.txt", located at ~/.memgpt/humans/my_user.txt
agent_config = AgentConfig(
    name="CustomAgent",
    persona="my_persona",
    human="my_user",
    preset="memgpt_chat",
    model="gpt-3.5-turbo",
)
14/28: agent_state = client.create_agent(agent_config=agent_config)
14/29: agent_config.to_agent_state()
14/30:
import os

from memgpt import MemGPT
from memgpt.config import AgentConfig, AgentState
from memgpt.cli.cli import QuickstartChoice
14/31:
agent_state = AgentState(
    name=agent_config.name,
    preset=agent_config.preset,
    persona=agent_config.persona,
    human=agent_config.human,
    llm_config=agent_config.llm_config,
    embedding_config=agent_config.embedding_config,
    create_time=agent_config.create_time,
)
14/32:
agent_state = AgentState(
    name=agent_config.name,
    preset=agent_config.preset,
    persona=agent_config.persona,
    human=agent_config.human,
    llm_config=agent_config.llm_config,
    embedding_config=agent_config.embedding_config,
)
14/33:
# You can set many more parameters, this is just a basic example
agent_state = client.create_agent(
    agent_config={
      "persona": "sam_pov",
      "human": "cs_phd",
    }
)
14/34:
# You can set many more parameters, this is just a basic example
agent_state = client.create_agent(
    agent_config={
      "name1": "CustomAgent",
      "persona": "sam_pov",
      "human": "cs_phd",
    }
)
14/35:
# You can set many more parameters, this is just a basic example
agent_state = client.create_agent(
    agent_config={
      "name": "CustomAgent",
      "persona": "sam_pov",
      "human": "cs_phd",
    }
)
14/36:
agent_state = AgentState(
    name=agent_config.name,
    preset=agent_config.preset,
    persona=agent_config.persona,
    human=agent_config.human,
    llm_config=agent_config.llm_config,
    embedding_config=agent_config.embedding_config,
)
14/37:
agent_state = AgentState(
    name=agent_config.name,
    preset=agent_config.preset,
    persona=agent_config.persona,
    human=agent_config.human,
    #llm_config=agent_config.llm_config,
    #embedding_config=agent_config.embedding_config,
)
14/38:
agent_state = client.create_agent(
    agent_config={
      "persona": "sam_pov",
      "human": "cs_phd",
    }
)
16/1:
import os

from memgpt import MemGPT
from memgpt.config import AgentConfig, AgentState
from memgpt.cli.cli import QuickstartChoice
16/2: os.environ["OPENAI_API_KEY"] = "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"
16/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
16/4:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key
  }
)
16/5:
agent_state = client.create_agent(
    agent_config={
      "persona": "sam_pov",
      "human": "cs_phd",
    }
)
16/6:
# Create the agent according to AgentConfig we set up. If an agent with
# the same name already exists it will simply return, unless you set
# throw_if_exists to 'True'
agent_state = client.create_agent(agent_config=agent_config)
16/7:
# Now that we have an agent_name identifier, we can send it a message!
# The response will have data from the MemGPT agent
my_message = "Hi MemGPT! How's it going?"
response = client.user_message(agent_id=agent_state.id, message=my_message)
17/1:
import os

from memgpt import MemGPT
from memgpt.config import AgentConfig, AgentState
from memgpt.cli.cli import QuickstartChoice
17/2: os.environ["OPENAI_API_KEY"] = "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"
17/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
17/4:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key
  }
)
17/5:
agent_state = client.create_agent(
    agent_config={
      "persona": "sam_pov",
      "human": "cs_phd",
      "model": "gpt-3.5-turbo",
    }
)
17/6:
# Now that we have an agent_name identifier, we can send it a message!
# The response will have data from the MemGPT agent
my_message = "Hi MemGPT! How's it going?"
response = client.user_message(agent_id=agent_state.id, message=my_message)
19/1:
import os

from memgpt import MemGPT
from memgpt.config import AgentConfig, AgentState
from memgpt.cli.cli import QuickstartChoice
19/2:
#os.environ["OPENAI_API_KEY"] = "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"
os.environ["OPENAI_API_KEY"] = "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"
19/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
19/4:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key
  }
)
19/5:
agent_state = client.create_agent(
    agent_config={
      "persona": "sam_pov",
      "human": "cs_phd",
      "model": "gpt-3.5-turbo",
    }
)
19/6:
# Now that we have an agent_name identifier, we can send it a message!
# The response will have data from the MemGPT agent
my_message = "Hi MemGPT! How's it going?"
response = client.user_message(agent_id=agent_state.id, message=my_message)
19/7: print(response)
21/1:
import os

from memgpt import MemGPT
from memgpt.config import AgentConfig, AgentState
from memgpt.cli.cli import QuickstartChoice
21/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
21/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
21/4:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key
  }
)
21/5:
agent_state = client.create_agent(
    agent_config={
      "persona": "sam_pov",
      "human": "cs_phd",
      "model": "gpt-3.5-turbo",
    }
)
21/6:
# Now that we have an agent_name identifier, we can send it a message!
# The response will have data from the MemGPT agent
my_message = "Hi MemGPT! How's it going?"
response = client.user_message(agent_id=agent_state.id, message=my_message)
23/1:
import os

from memgpt import MemGPT
from memgpt.config import AgentConfig, AgentState
from memgpt.cli.cli import QuickstartChoice
23/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
23/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
23/4:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key
  }
)
23/5:
agent_state = client.create_agent(
    agent_config={
      "persona": "sam_pov",
      "human": "cs_phd",
      "model": "gpt-3.5-turbo",
    }
)
23/6:
# Now that we have an agent_name identifier, we can send it a message!
# The response will have data from the MemGPT agent
my_message = "Hi MemGPT! How's it going?"
response = client.user_message(agent_id=agent_state.id, message=my_message)
25/1:
import os

from memgpt import MemGPT
from memgpt.config import AgentConfig, AgentState
from memgpt.cli.cli import QuickstartChoice
25/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
25/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
25/4:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key
  }
)
25/5:
agent_state = client.create_agent(
    agent_config={
      "persona": "sam_pov",
      "human": "cs_phd",
      "model": "gpt-3.5-turbo",
    }
)
25/6:
# Now that we have an agent_name identifier, we can send it a message!
# The response will have data from the MemGPT agent
my_message = "Hi MemGPT! How's it going?"
response = client.user_message(agent_id=agent_state.id, message=my_message)
27/1:
import os

from memgpt import MemGPT
from memgpt.config import AgentConfig, AgentState
from memgpt.cli.cli import QuickstartChoice
27/2:
#os.environ["OPENAI_API_KEY"] = "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"
os.environ["OPENAI_API_KEY"] = "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"
27/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
27/4:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key
  }
)
27/5:
agent_state = client.create_agent(
    agent_config={
      "persona": "sam_pov",
      "human": "cs_phd",
      "model": "gpt-3.5-turbo",
    }
)
27/6:
# Now that we have an agent_name identifier, we can send it a message!
# The response will have data from the MemGPT agent
my_message = "Hi MemGPT! How's it going?"
response = client.user_message(agent_id=agent_state.id, message=my_message)
27/7: print(response)
28/1:
import os

from memgpt import MemGPT
from memgpt.config import AgentConfig, AgentState
from memgpt.cli.cli import QuickstartChoice
28/2:
#os.environ["OPENAI_API_KEY"] = "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"
os.environ["OPENAI_API_KEY"] = "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"
28/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
28/4:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key
  }
)
28/5: agent_state = AgentState(
28/6: ?AgentState
28/7:
import os
import uuid

from memgpt import MemGPT
from memgpt.config import AgentConfig, AgentState
from memgpt.cli.cli import QuickstartChoice
28/8:
get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))


telegram_id = 1237109343
28/9:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))


telegram_id = 1237109343
28/10:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))


telegram_id = 1237109343
print(get_user_id(telegram_id))
28/11:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))


telegram_id = 1237109343
print(get_user_id(telegram_id))
28/12:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))


telegram_id = 1237109341
print(get_user_id(telegram_id))
28/13:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))


telegram_id = 1237109342
print(get_user_id(telegram_id))
28/14:
agent_state = AgentState(
    name="ideationGPT",
    user_id=get_user_id(telegram_id),
    persona="./persona.txt"
)
28/15:
agent_config = AgentConfig(
    name="CustomAgent",
    persona="my_persona",
    human="my_user",
    preset="memgpt_chat",
    model="gpt-4",
)
28/16:
agent_config = AgentConfig(
    name="CustomAgent",
    persona="my_persona",
    human="my_user",
    preset="memgpt_chat",
    model="gpt-4",
)

agent_config.llm_config
28/17:
import os
import uuid

from memgpt import MemGPT

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
28/18:
agent_state = AgentState(
    name="ideationGPT",
    user_id=get_user_id(telegram_id),
    persona="./persona.txt",
    human="./human.txt",
    llm_config=LLMConfig(
        model="gpt-3.5-turbo"
    ),
    embedding_config=EmbeddingConfig()
    preset="memgpt_chat",
)
28/19:
agent_state = AgentState(
    name="ideationGPT",
    user_id=get_user_id(telegram_id),
    persona="./persona.txt",
    human="./human.txt",
    llm_config=LLMConfig(
        model="gpt-3.5-turbo"
    ),
    embedding_config=EmbeddingConfig(),
    preset="memgpt_chat",
)
28/20:
# Now that we have an agent_name identifier, we can send it a message!
# The response will have data from the MemGPT agent
my_message = "Hi MemGPT! How's it going?"
response = client.user_message(agent_id=agent_state.id, message=my_message)
30/1:
import os
import uuid

from memgpt import MemGPT

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
30/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
30/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
30/4:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))

# test the function
telegram_id = 1237109342
print(get_user_id(telegram_id))
30/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key
  }
)
30/6: ?AgentState
30/7:
agent_config = AgentConfig(
    name="CustomAgent",
    persona="my_persona",
    human="my_user",
    preset="memgpt_chat",
    model="gpt-4",
)

agent_config.llm_config
30/8:
agent_state = AgentState(
    name="ideationGPT",
    user_id=get_user_id(telegram_id),
    persona="./persona.txt",
    human="./human.txt",
    llm_config=LLMConfig(
        model="gpt-3.5-turbo"
    ),
    embedding_config=EmbeddingConfig(),
    preset="memgpt_chat",
)
30/9:
# Now that we have an agent_name identifier, we can send it a message!
# The response will have data from the MemGPT agent
my_message = "Hi MemGPT! How's it going?"
response = client.user_message(agent_id=agent_state.id, message=my_message)
31/1:
import os
import uuid

from memgpt import MemGPT

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
31/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
31/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
31/4:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))

# test the function
telegram_id = 1237109342
print(get_user_id(telegram_id))
31/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key
  }
)
31/6: ?AgentState
31/7:
agent_config = AgentConfig(
    name="CustomAgent",
    persona="my_persona",
    human="my_user",
    preset="memgpt_chat",
    model="gpt-4",
)

agent_config.llm_config
31/8:
agent_state = AgentState(
    name="ideationGPT",
    user_id=get_user_id(telegram_id),
    persona="./persona.txt",
    human="./human.txt",
    llm_config=LLMConfig(
        model="gpt-3.5-turbo"
    ),
    embedding_config=EmbeddingConfig(),
    preset="memgpt_chat",
)
31/9:
# Now that we have an agent_name identifier, we can send it a message!
# The response will have data from the MemGPT agent
my_message = "Hi MemGPT! How's it going?"
response = client.user_message(agent_id=agent_state.id, message=my_message)
32/1:
import os
import uuid

from memgpt import MemGPT

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
32/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
32/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
32/4:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))

# test the function
telegram_id = 1237109342
print(get_user_id(telegram_id))
32/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key
  }
)
32/6: ?AgentState
32/7:
agent_config = AgentConfig(
    name="CustomAgent",
    persona="my_persona",
    human="my_user",
    preset="memgpt_chat",
    model="gpt-4",
)

agent_config.llm_config
32/8:
agent_state = AgentState(
    name="ideationGPT",
    user_id=get_user_id(telegram_id),
    persona="./persona.txt",
    human="./human.txt",
    llm_config=LLMConfig(
        model="gpt-3.5-turbo"
    ),
    embedding_config=EmbeddingConfig(),
    preset="memgpt_chat",
)
32/9:
# Create a helper that sends a message and prints the assistant response only
def send_message(message: str):
    """
    sends a message and prints the assistant output only.
    :param message: the message to send
    """
    response = client.user_message(agent_id=agent_state.id, message=message)
    for r in response:
        # Can also handle other types "function_call", "function_return", "function_message"
        if "assistant_message" in r:
            print("ASSISTANT:", r["assistant_message"])
        elif "internal_monologue" in r:
            print("THOUGHTS:", r["internal_monologue"])

# Send a message and see the response
send_message("Please introduce yourself and tell me about your abilities!")
35/1:
import os
import uuid

from memgpt import MemGPT

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
35/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
35/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
35/4:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))

# test the function
telegram_id = 1237109342
print(get_user_id(telegram_id))
35/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key
  }
)
35/6:
agent_config = AgentConfig(
    name=f"ideationGPT",
    persona="p_clapvontrapp",
    human=f"Adam",
)
agent_state = mem_client.create_agent(agent_config=agent_config)
35/7:
agent_config = AgentConfig(
    name=f"ideationGPT",
    persona="p_clapvontrapp",
    human=f"Adam",
)
agent_state = client.create_agent(agent_config=agent_config)
36/1:
import os
import uuid

from memgpt import MemGPT

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
36/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
36/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
36/4:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))

# test the function
telegram_id = 1237109342
print(get_user_id(telegram_id))
36/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key
  }
)
36/6: ?client.create_agent
36/7:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key
  },
  user_id=get_user_id(telegram_id)
)
37/1:
import os
import uuid

from memgpt import MemGPT

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
37/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
37/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
37/4:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))

# test the function
telegram_id = 1237109342
print(get_user_id(telegram_id))
37/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key
  },
  user_id=get_user_id(telegram_id)
)
37/6:
agent = client.create_agent(
    "user_id": get_user_id(telegram_id),
    "name": 
)
37/7:
agent = client.create_agent(
    "user_id": get_user_id(telegram_id),
    "name": "ideationGPT"
)
37/8:
agent = client.create_agent({
    "user_id": get_user_id(telegram_id),
    "name": "ideationGPT"
})
37/9:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key
    "model": "gpt-3.5-turbo",
  },
  user_id=get_user_id(telegram_id)
)
37/10:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key,
    "model": "gpt-3.5-turbo",
  },
  user_id=get_user_id(telegram_id)
)
38/1:
import os
import uuid

from memgpt import MemGPT

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
38/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
38/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
38/4:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))

# test the function
telegram_id = 1237109342
print(get_user_id(telegram_id))
38/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key,
    "model": "gpt-3.5-turbo",
  },
  user_id=get_user_id(telegram_id)
)
38/6: ?client.create_agent
38/7:
agent = client.create_agent({
    "user_id": get_user_id(telegram_id),
    "name": "ideationGPT"
})
38/8: client.list_agents()
38/9:
import os
import uuid

from memgpt import MemGPT

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
38/10:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
38/11:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
38/12:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))

# test the function
telegram_id = 1237109342
print(get_user_id(telegram_id))
38/13:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key,
    "model": "gpt-3.5-turbo",
  },
  user_id=get_user_id(telegram_id)
)
38/14: client.list_agents()
38/15:
agent = client.create_agent({
    "user_id": get_user_id(telegram_id),
    "name": "ideationGPT"
})
38/16:
if client.agent_exists("ideationGPT"):
    print("Agent already exists")
38/17:
agent = client.create_agent({
    "user_id": get_user_id(telegram_id),
    "name": "ideationGPT"
})
38/18:
if client.agent_exists(get_user_id(telegram_id)):
    print("Agent already exists")
38/19:
if client.agent_exists(get_user_id(telegram_id)):
    print("Agent already exists")
38/20:
if client.agent_exists(get_user_id(telegram_id)):
    print("Agent already exists")
38/21:
if client.agent_exists(get_user_id(telegram_id)):
    print("Agent already exists")
38/22:
if client.agent_exists(get_user_id(telegram_id)):
    print("Agent already exists")
38/23:
if client.get_agent('test_agent') is None:
    client.create_agent('test_agent', AgentConfig(
        state=AgentState.active,
        llm_config=LLMConfig(
            max_tokens=100,
            temperature=0.5,
            top_p=1.0,
            frequency_penalty=0.0,
            presence_penalty=0.0,
            stop=["\n", "Human:", "AI:"]
        ),
        embedding_config=EmbeddingConfig(
            min_similarity=0.5,
            max_similarity=0.9,
            max_history=3,
            max_tokens=100
        )
    ))
38/24:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key,
    "model": "gpt-3.5-turbo",
  },
  user_id=get_user_id(telegram_id)
)
38/25:
agent = client.create_agent({
    "user_id": get_user_id(telegram_id),
    "name": "ideationGPT"
})
38/26:
if client.agent_exists(agent_name="ideationGPT"):
    print("Agent already exists")
    #agent = client.get_agent("ideationGPT")
38/27:
if client.agent_exists(agent_name="ideationGPT"):
    print("Agent already exists")
    agent = client.get_agent("ideationGPT")
38/28:
if client.agent_exists(agent_name="ideationGPT"):
    print("Agent already exists")
    agent = client.server._get_or_load_agent(agent_name="ideationGPT")
38/29: ?client.server
38/30:
if client.agent_exists(agent_name="ideationGPT"):
    print("Agent already exists")
38/31: type(client.list_agents())
38/32: client.list_agents()
38/33:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_id = None
if client.agent_exists(agent_name="ideationGPT"):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name="ideationGPT",
        agents_dict=agents_dict)
else:
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": "ideationGPT"
    })
    agent_id = agent['id']
38/34:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_id = None
if client.agent_exists(agent_name="ideationGPT1"):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name="ideationGPT1",
        agents_dict=agents_dict)
else:
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": "ideationGPT1"
    })
    agent_id = agent['id']
38/35:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_id = None
if client.agent_exists(agent_name="ideationGPT1"):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name="ideationGPT1",
        agents_dict=agents_dict)
else:
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": "ideationGPT1"
    })
    agent_id = agent['id']
38/36: agent
38/37: agent.id
38/38:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_id = None
if client.agent_exists(agent_name="ideationGPT1"):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name="ideationGPT1",
        agents_dict=agents_dict)
else:
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": "ideationGPT1"
    })
    agent_id = agent.id
38/39:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_id = None
if client.agent_exists(agent_name="ideationGPT0"):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name="ideationGPT1",
        agents_dict=agents_dict)
else:
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": "ideationGPT0"
    })
    agent_id = agent.id
38/40: agent.id
38/41:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_id = None
if client.agent_exists(agent_name="ideationGPT0"):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name="ideationGPT1",
        agents_dict=agents_dict)
else:
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": "ideationGPT0"
    })
    agent_id = agent.id
38/42: agent_id
39/1:
import os
import uuid

from memgpt import MemGPT

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
39/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
39/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
39/4:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))

# test the function
telegram_id = 1237109342
print(get_user_id(telegram_id))
39/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key,
    "model": "gpt-3.5-turbo",
  },
  user_id=get_user_id(telegram_id)
)
39/6: client.list_agents()
39/7: ?client.server
39/8:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_id = None
if client.agent_exists(agent_name="ideationGPT0"):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name="ideationGPT1",
        agents_dict=agents_dict)
else:
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": "ideationGPT0"
    })
    agent_id = agent.id
39/9: agent_id
39/10:
agent_config = AgentConfig(
    name="CustomAgent",
    persona="my_persona",
    human="my_user",
    preset="memgpt_chat",
    model="gpt-4",
)

agent_config.llm_config
39/11:
agent_state = AgentState(
    name="ideationGPT",
    user_id=get_user_id(telegram_id),
    persona="./persona.txt",
    human="./human.txt",
    llm_config=LLMConfig(
        model="gpt-3.5-turbo"
    ),
    embedding_config=EmbeddingConfig(),
    preset="memgpt_chat",
)
39/12:
# Create a helper that sends a message and prints the assistant response only
def send_message(message: str):
    """
    sends a message and prints the assistant output only.
    :param message: the message to send
    """
    response = client.user_message(agent_id=agent_state.id, message=message)
    for r in response:
        # Can also handle other types "function_call", "function_return", "function_message"
        if "assistant_message" in r:
            print("ASSISTANT:", r["assistant_message"])
        elif "internal_monologue" in r:
            print("THOUGHTS:", r["internal_monologue"])

# Send a message and see the response
send_message("Please introduce yourself and tell me about your abilities!")
39/13:
# Create a helper that sends a message and prints the assistant response only
def send_message(message: str):
    """
    sends a message and prints the assistant output only.
    :param message: the message to send
    """
    response = client.user_message(agent_id=agent_state.id, message=message)
    for r in response:
        # Can also handle other types "function_call", "function_return", "function_message"
        if "assistant_message" in r:
            print("ASSISTANT:", r["assistant_message"])
        elif "internal_monologue" in r:
            print("THOUGHTS:", r["internal_monologue"])

# Send a message and see the response
send_message("Please introduce yourself and tell me about your abilities!")
39/14:
# Create a helper that sends a message and prints the assistant response only
def send_message(message: str):
    """
    sends a message and prints the assistant output only.
    :param message: the message to send
    """
    response = client.user_message(agent_id=agent_id, message=message)
    for r in response:
        # Can also handle other types "function_call", "function_return", "function_message"
        if "assistant_message" in r:
            print("ASSISTANT:", r["assistant_message"])
        elif "internal_monologue" in r:
            print("THOUGHTS:", r["internal_monologue"])

# Send a message and see the response
send_message("Please introduce yourself and tell me about your abilities!")
40/1:
import os
import uuid

from memgpt import MemGPT

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
40/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
40/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
40/4:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))

# test the function
telegram_id = 1237109342
print(get_user_id(telegram_id))
40/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key,
    "model": "gpt-3.5-turbo",
  },
  user_id=get_user_id(telegram_id)
)
40/6: client.list_agents()
40/7: ?client.server
40/8:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_id = None
if client.agent_exists(agent_name="ideationGPT0"):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name="ideationGPT1",
        agents_dict=agents_dict)
else:
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": "ideationGPT0"
    })
    agent_id = agent.id
40/9: agent_id
40/10:
# Create a helper that sends a message and prints the assistant response only
def send_message(message: str):
    """
    sends a message and prints the assistant output only.
    :param message: the message to send
    """
    response = client.user_message(agent_id=agent_id, message=message)
    for r in response:
        # Can also handle other types "function_call", "function_return", "function_message"
        if "assistant_message" in r:
            print("ASSISTANT:", r["assistant_message"])
        elif "internal_monologue" in r:
            print("THOUGHTS:", r["internal_monologue"])

# Send a message and see the response
send_message("Please introduce yourself and tell me about your abilities!")
41/1:
import os
import uuid

from memgpt import MemGPT

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
41/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
41/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
41/4:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))

# test the function
telegram_id = 1237109342
print(get_user_id(telegram_id))
41/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key,
    "model": "gpt-3.5-turbo",
  },
  user_id=get_user_id(telegram_id)
)
41/6: client.list_agents()
41/7: ?client.server
41/8:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_id = None
if client.agent_exists(agent_name="ideationGPT2"):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name="ideationGPT1",
        agents_dict=agents_dict)
else:
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": "ideationGPT2",
        "human": "Dorkus Malorkus",
    })
    agent_id = agent.id
42/1:
import os
import uuid

from memgpt import MemGPT

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
42/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
42/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
42/4:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))

# test the function
telegram_id = 1237109342
print(get_user_id(telegram_id))
42/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key,
    "model": "gpt-3.5-turbo",
  },
  user_id=get_user_id(telegram_id)
)
42/6: client.list_agents()
42/7: ?client.server
42/8:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_id = None
if client.agent_exists(agent_name="ideationGPT2"):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name="ideationGPT2",
        agents_dict=agents_dict)
else:
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": "ideationGPT2",
        "human": "Dorkus Malorkus",
    })
    agent_id = agent.id
42/9:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_name = "ideationDorkus"
agent_id = None

if client.agent_exists(agent_name=agent_name):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name=agent_name,
        agents_dict=agents_dict)
else:
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": agent_name,
        "human": "Dorkus Malorkus",
    })
    agent_id = agent.id
42/10:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_name = "ideationDorkus"
agent_id = None

if client.agent_exists(agent_name=agent_name):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name=agent_name,
        agents_dict=agents_dict)
else:
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": agent_name,
        "human": "Dorkus Malorkus",
    })
    agent_id = agent.id
42/11:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_name = "ideationDorkus"
agent_id = None

if client.agent_exists(agent_name=agent_name):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name=agent_name,
        agents_dict=agents_dict)
else:
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": agent_name,
        "human": "dorkus",
    })
    agent_id = agent.id
42/12:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_name = "ideationDorkus"
agent_id = None

if client.agent_exists(agent_name=agent_name):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name=agent_name,
        agents_dict=agents_dict)
else:
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": agent_name,
        "human": "dorkus",
        "human_notes": "I am a dorkus",
    })
    agent_id = agent.id
42/13:
import os
import uuid

from memgpt import MemGPT
from memgpt import constants

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
42/14:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_name = "ideationDorkus"
agent_id = None

if client.agent_exists(agent_name=agent_name):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name=agent_name,
        agents_dict=agents_dict)
else:
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": agent_name,
        "human": "dorkus",
        "human_notes": "I am a dorkus",
    })
    agent_id = agent.id
42/15:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_name = "ideationDorkus"
agent_id = None

if client.agent_exists(agent_name=agent_name):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name=agent_name,
        agents_dict=agents_dict)
else:
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": agent_name,
        "human": "dorkus"
    })
    agent_id = agent.id
42/16: constants.DEFAULT_HUMAN
42/17:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_name = "ideationDorkus"
agent_id = None

if client.agent_exists(agent_name=agent_name):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name=agent_name,
        agents_dict=agents_dict)
else:
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": agent_name,
        "human_notes": "Dorkus Malorkus"
    })
    agent_id = agent.id
42/18: agent_id
42/19:
# Create a helper that sends a message and prints the assistant response only
def send_message(message: str):
    """
    sends a message and prints the assistant output only.
    :param message: the message to send
    """
    response = client.user_message(agent_id=agent_id, message=message)
    for r in response:
        # Can also handle other types "function_call", "function_return", "function_message"
        if "assistant_message" in r:
            print("ASSISTANT:", r["assistant_message"])
        elif "internal_monologue" in r:
            print("THOUGHTS:", r["internal_monologue"])

# Send a message and see the response
send_message("Please introduce yourself and tell me about your abilities!")
43/1:
import os
import uuid

from memgpt import MemGPT
from memgpt import constants

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
43/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
43/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
43/4:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))

# test the function
telegram_id = 1237109342
print(get_user_id(telegram_id))
43/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key,
    "model": "gpt-3.5-turbo",
  },
  user_id=get_user_id(telegram_id)
)
43/6: client.list_agents()
43/7: ?client.server
43/8:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_name = "ideationDorkus-01"
agent_id = None

if client.agent_exists(agent_name=agent_name):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name=agent_name,
        agents_dict=agents_dict)
else:
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": agent_name,
        "human_notes": "Name: Dorkus Malorkus"
    })
    agent_id = agent.id
43/9: agent_id
43/10:
# Create a helper that sends a message and prints the assistant response only
def send_message(message: str):
    """
    sends a message and prints the assistant output only.
    :param message: the message to send
    """
    response = client.user_message(agent_id=agent_id, message=message)
    for r in response:
        # Can also handle other types "function_call", "function_return", "function_message"
        if "assistant_message" in r:
            print("ASSISTANT:", r["assistant_message"])
        elif "internal_monologue" in r:
            print("THOUGHTS:", r["internal_monologue"])

# Send a message and see the response
send_message("Please introduce yourself and tell me about your abilities!")
43/11:
# Create a helper that sends a message and prints the assistant response only
def send_message(message: str):
    """
    sends a message and prints the assistant output only.
    :param message: the message to send
    """
    response = client.user_message(agent_id=agent_id, message=message)
    for r in response:
        # Can also handle other types "function_call", "function_return", "function_message"
        if "assistant_message" in r:
            print("ASSISTANT:", r["assistant_message"])
        elif "internal_monologue" in r:
            print("THOUGHTS:", r["internal_monologue"])

# Send a message and see the response
send_message("Please introduce yourself and tell me about your abilities!")
43/12:
# Create a helper that sends a message and prints the assistant response only
def send_message(message: str):
    """
    sends a message and prints the assistant output only.
    :param message: the message to send
    """
    response = client.user_message(agent_id=agent_id, message=message)
    for r in response:
        # Can also handle other types "function_call", "function_return", "function_message"
        if "assistant_message" in r:
            print("ASSISTANT:", r["assistant_message"])
        elif "internal_monologue" in r:
            print("THOUGHTS:", r["internal_monologue"])

# Send a message and see the response
send_message("What is my name? Please introduce yourself and tell me about your abilities!")
43/13:
# Create a helper that sends a message and prints the assistant response only
def send_message(message: str):
    """
    sends a message and prints the assistant output only.
    :param message: the message to send
    """
    response = client.user_message(agent_id=agent_id, message=message)
    for r in response:
        # Can also handle other types "function_call", "function_return", "function_message"
        if "assistant_message" in r:
            print("ASSISTANT:", r["assistant_message"])
        elif "internal_monologue" in r:
            print("THOUGHTS:", r["internal_monologue"])

# Send a message and see the response
send_message("What is my name? Please introduce yourself and tell me about your abilities!")
44/1:
import os
import uuid

from memgpt import MemGPT
from memgpt import constants

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
44/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
44/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
44/4:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))

# test the function
telegram_id = 1237109342
print(get_user_id(telegram_id))
44/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key,
    "model": "gpt-3.5-turbo",
  },
  user_id=get_user_id(telegram_id)
)
44/6: client.list_agents()
44/7: ?client.server
44/8:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_name = "ideationDorkus-02"
agent_id = None

if client.agent_exists(agent_name=agent_name):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name=agent_name,
        agents_dict=agents_dict)
else:
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": agent_name,
        "human": "Dorkus"
        "human_notes": "Name: Dorkus Malorkus"
    })
    agent_id = agent.id
45/1:
import os
import uuid

from memgpt import MemGPT
from memgpt import constants

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
45/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
45/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
45/4:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))

# test the function
telegram_id = 1237109342
print(get_user_id(telegram_id))
45/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key,
    "model": "gpt-3.5-turbo",
  },
  user_id=get_user_id(telegram_id)
)
45/6: client.list_agents()
45/7: ?client.server
45/8:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_name = "ideationDorkus-03"
agent_id = None

if client.agent_exists(agent_name=agent_name):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name=agent_name,
        agents_dict=agents_dict)
else:
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": agent_name,
        "human": "Dorkus",
        "human_notes": "Name: Dorkus Malorkus"
    })
    agent_id = agent.id
46/1:
import os
import uuid

from memgpt import MemGPT
from memgpt import constants

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
46/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
46/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
46/4:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))

# test the function
telegram_id = 1237109342
print(get_user_id(telegram_id))
46/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key,
    "model": "gpt-3.5-turbo",
  },
  user_id=get_user_id(telegram_id)
)
46/6: client.list_agents()
46/7: ?client.server
46/8:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_name = "ideationDorkus-03"
agent_id = None

if client.agent_exists(agent_name=agent_name):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name=agent_name,
        agents_dict=agents_dict)
else:
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": agent_name,
        "human": "Dorkus",
        "human_notes": "Name: Dorkus Malorkus"
    })
    agent_id = agent.id
46/9:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_name = "ideationDorkus-03"
agent_id = None

if client.agent_exists(agent_name=agent_name):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name=agent_name,
        agents_dict=agents_dict)
else:
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": agent_name,
        "human": "Dorkus",
        "human_notes": "Name: Dorkus Malorkus"
    })
    agent_id = agent.id
46/10:
import os
import uuid

from memgpt import MemGPT
from memgpt import constants

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice'
from memgpt.cli.cli_config import configure, list, add, delete
46/11:
import os
import uuid

from memgpt import MemGPT
from memgpt import constants

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
from memgpt.cli.cli_config import configure, list, add, delete
46/12:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_name = "ideationDorkus-03"
agent_id = None

if client.agent_exists(agent_name=agent_name):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name=agent_name,
        agents_dict=agents_dict)
else:
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": agent_name,
        "human": "dorkus",
    })
    agent_id = agent.id
47/1:
import os
import uuid

from memgpt import MemGPT
from memgpt import constants

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
from memgpt.cli.cli_config import configure, list, add, delete
47/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
47/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
47/4:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))

# test the function
telegram_id = 1237109342
print(get_user_id(telegram_id))
47/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key,
    "model": "gpt-3.5-turbo",
  },
  user_id=get_user_id(telegram_id)
)
47/6: client.list_agents()
47/7: ?client.server
47/8:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_name = "ideationDorkus-03"
agent_id = None

if client.agent_exists(agent_name=agent_name):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name=agent_name,
        agents_dict=agents_dict)
else:
    # add the human
    add(
        option="human",
        name="dorkus",
        text="Name: Dorkus Malorkus.",
    )

    # create the agent
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": agent_name,
        "human": "dorkus",
    })
    agent_id = agent.id
47/9: agent_id
47/10:
# Create a helper that sends a message and prints the assistant response only
def send_message(message: str):
    """
    sends a message and prints the assistant output only.
    :param message: the message to send
    """
    response = client.user_message(agent_id=agent_id, message=message)
    for r in response:
        # Can also handle other types "function_call", "function_return", "function_message"
        if "assistant_message" in r:
            print("ASSISTANT:", r["assistant_message"])
        elif "internal_monologue" in r:
            print("THOUGHTS:", r["internal_monologue"])

# Send a message and see the response
send_message("What is my name? Please introduce yourself and tell me about your abilities!")
47/11:
# Create a helper that sends a message and prints the assistant response only
def send_message(message: str):
    """
    sends a message and prints the assistant output only.
    :param message: the message to send
    """
    response = client.user_message(agent_id=agent_id, message=message)
    for r in response:
        # Can also handle other types "function_call", "function_return", "function_message"
        if "assistant_message" in r:
            print("ASSISTANT:", r["assistant_message"])
        elif "internal_monologue" in r:
            print("THOUGHTS:", r["internal_monologue"])

# Send a message and see the response
send_message("Please introduce yourself and tell me about your abilities!")
48/1:
import os
import uuid

from memgpt import MemGPT
from memgpt import constants

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
from memgpt.cli.cli_config import configure, list, add, delete
48/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
48/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
48/4:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))

# test the function
telegram_id = 1237109342
print(get_user_id(telegram_id))
48/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key,
    "model": "gpt-3.5-turbo",
  },
  user_id=get_user_id(telegram_id)
)
48/6: client.list_agents()
48/7: ?client.server
48/8:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_name = "ideationDorkus-03"
agent_id = None

if client.agent_exists(agent_name=agent_name):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name=agent_name,
        agents_dict=agents_dict)
else:
    # add the human
    add(
        option="human",
        name="dorkus",
        text="Name: Dorkus Malorkus.",
    )

    # create the agent
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": agent_name,
        "human": "dorkus",
    })
    agent_id = agent.id
48/9: agent_id
48/10:
# Create a helper that sends a message and prints the assistant response only
def send_message(message: str):
    """
    sends a message and prints the assistant output only.
    :param message: the message to send
    """
    response = client.user_message(agent_id=agent_id, message=message)
    for r in response:
        # Can also handle other types "function_call", "function_return", "function_message"
        if "assistant_message" in r:
            print("ASSISTANT:", r["assistant_message"])
        elif "internal_monologue" in r:
            print("THOUGHTS:", r["internal_monologue"])

# Send a message and see the response
send_message("Please introduce yourself and tell me about your abilities!")
49/1:
import os
import uuid

from memgpt import MemGPT
from memgpt import constants

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
from memgpt.cli.cli_config import configure, list, add, delete
49/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
49/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
49/4:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))

# test the function
telegram_id = 1237109342
print(get_user_id(telegram_id))
49/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key,
    "model": "gpt-3.5-turbo",
  },
  user_id=get_user_id(telegram_id)
)
49/6: client.list_agents()
49/7:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_name = "ideationDorkus"
agent_id = None

if client.agent_exists(agent_name=agent_name):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name=agent_name,
        agents_dict=agents_dict)
else:
    # add the human
    add(
        option="human",
        name="dorkus",
        text="Name: Dorkus Malorkus.",
    )

    # create the agent
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": agent_name,
        "human": "dorkus",
    })
    agent_id = agent.id
49/8:
# Create a helper that sends a message and prints the assistant response only
def send_message(message: str):
    """
    sends a message and prints the assistant output only.
    :param message: the message to send
    """
    response = client.user_message(agent_id=agent_id, message=message)
    for r in response:
        # Can also handle other types "function_call", "function_return", "function_message"
        if "assistant_message" in r:
            print("ASSISTANT:", r["assistant_message"])
        elif "internal_monologue" in r:
            print("THOUGHTS:", r["internal_monologue"])

# Send a message and see the response
send_message("Please introduce yourself and tell me about your abilities!")
49/9:
# Create a helper that sends a message and prints the assistant response only
def send_message(message: str):
    """
    sends a message and prints the assistant output only.
    :param message: the message to send
    """
    response = client.user_message(agent_id=agent_id, message=message)
    for r in response:
        # Can also handle other types "function_call", "function_return", "function_message"
        if "assistant_message" in r:
            print("ASSISTANT:", r["assistant_message"])
        elif "internal_monologue" in r:
            print("THOUGHTS:", r["internal_monologue"])

# Send a message and see the response
send_message("Please introduce yourself and tell me about your abilities!")
50/1:
import os
import uuid

from memgpt import MemGPT
from memgpt import constants

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
from memgpt.cli.cli_config import configure, list, add, delete
50/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
50/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
50/4:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))

# test the function
telegram_id = 1237109342
print(get_user_id(telegram_id))
50/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,\

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key,
    "model": "gpt-3.5-turbo",
  },
  user_id=get_user_id(telegram_id)
)
50/6: client.list_agents()
50/7:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_name = "ideationDorkus"
agent_id = None

if client.agent_exists(agent_name=agent_name):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name=agent_name,
        agents_dict=agents_dict)
else:
    # add the human
    add(
        option="human",
        name="dorkus",
        text="Name: Dorkus Malorkus.",
    )

    # create the agent
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": agent_name,
        "human": "dorkus",
    })
    agent_id = agent.id
50/8:
# Create a helper that sends a message and prints the assistant response only
def send_message(message: str):
    """
    sends a message and prints the assistant output only.
    :param message: the message to send
    """
    response = client.user_message(agent_id=agent_id, message=message)
    for r in response:
        # Can also handle other types "function_call", "function_return", "function_message"
        if "assistant_message" in r:
            print("ASSISTANT:", r["assistant_message"])
        elif "internal_monologue" in r:
            print("THOUGHTS:", r["internal_monologue"])

# Send a message and see the response
send_message("Do you know what my name is? Please introduce yourself and tell me about your abilities!")
51/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../../src"))
51/2: from personas import Personas
51/3: from memory.persona import Persona
51/4:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
51/5: from memory.persona import Persona
52/1: Persona.init_persona()
52/2: Persona.init_persona()
52/3:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
52/4: from memory.persona import Persona
52/5: Persona.init_persona()
53/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
53/2: from memory.persona import Persona
53/3: Persona.init_persona()
54/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
54/2: from memory.persona import Persona
54/3: Persona.init_persona()
54/4: Persona.init_persona()
55/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
55/2: from memory.persona import Persona
55/3: Persona.init_persona()
55/4:
from config import Config
from memory.persona import Persona
55/5:
from config.config import Config
from memory.persona import Persona
56/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
56/2:
from config.config import Config
from memory.persona import Persona
56/3: Persona.init_persona()
57/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
57/2:
from config.config import Config
from memory.persona import Persona
57/3: Persona.init_persona()
57/4: init_config()
58/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
58/2:
from config.config import Config
from config.confg import init_config
from memory.persona import Persona
59/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
59/2:
from config.config import Config
from config.config import init_config
from memory.persona import Persona
60/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
60/2:
from config.config import Config
from config.config import init_logging
from memory.persona import Persona
60/3: init_logging()
60/4: Persona.init_persona()
60/5: Persona.init_persona()
60/6: Persona.init_persona()
60/7: Persona.init_persona()
60/8: Persona.init_persona()
60/9: Persona.init_persona()
61/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
61/2:
from config.config import Config
from config.config import init_logging
from memory.persona import Persona
61/3: init_logging()
61/4: Persona.init_persona()
62/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
62/2: from memory.persona import Persona
62/3: Persona.init_persona()
62/4: Persona.init_persona()
63/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
63/2: from memory.persona import Persona
63/3: Persona.init_persona()
63/4: Persona.init_persona()
63/5: Persona.init_persona()
63/6: Persona.init_persona()
63/7: Persona.init_persona()
63/8: Persona.init_persona()
63/9: Persona.init_persona()
64/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
64/2: from memory.persona import Persona
64/3: Persona.init_persona()
64/4:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
64/5: from memory.persona import Persona
64/6: Persona.init_persona()
50/9: client.list_agents()
50/10: ?client.list_agents
50/11: ?client.list_agents
50/12: client.list_agents()
50/13: client.user_message(agent_id=agent_id, message=message)
50/14: client.user_message(agent_id=agent_id, message="Thundercats are go!")
50/15:
response = client.user_message(agent_id=agent_id, message="Thundercats are go!")
print(response)
print(type(response))
65/1: from memory import UserManager
66/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
66/2: from memory import UserManager
66/3: from memory.user import UserManager
66/4:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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"
    first_name="Johannes",
    last_name="Fould",
    message="Good morning!"
)
66/5:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    first_name="Johannes",
    last_name="Fould",
    message="Good morning!"
)
66/6:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    first_name="Johannes",
    last_name="Fould",
    message="Good morning!"
)
66/7:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    first_name="Johannes",
    last_name="Fould",
    message="Good morning!"
)
66/8:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    first_name="Johannes",
    last_name="Foulds",
    message="Good morning!"
)
67/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
67/2: from memory.persona import Persona
67/3: Persona.init_persona()
67/4:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
67/5: from memory.persona import Persona
67/6: Persona.init_persona()
67/7:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
67/8: from memory.persona import Persona
67/9: Persona.init_persona()
68/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
68/2: from memory.user import UserManager
68/3:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    first_name="Johannes",
    last_name="Foulds",
    message="Good morning!"
)
50/16:
import os
import uuid

from memgpt import MemGPT
from memgpt import constants

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
from memgpt.cli.cli_config import configure, list, add, delete
50/17:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
50/18:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
50/19:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))

# test the function
telegram_id = 1237109342
print(get_user_id(telegram_id))
50/20:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key,
    "model": "gpt-3.5-turbo",
  },
  user_id=get_user_id(telegram_id)
)
50/21: client.list_agents()
50/22: ?client.list_agents
50/23:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_name = "ideationDorkus"
agent_id = None

if client.agent_exists(agent_name=agent_name):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name=agent_name,
        agents_dict=agents_dict)
else:
    # add the human
    add(
        option="human",
        name="dorkus",
        text="Name: Dorkus Malorkus.",
    )

    # create the agent
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": agent_name,
        "human": "dorkus",
    })
    agent_id = agent.id
50/24:
# Create a helper that sends a message and prints the assistant response only
def send_message(message: str):
    """
    sends a message and prints the assistant output only.
    :param message: the message to send
    """
    response = client.user_message(agent_id=agent_id, message=message)
    for r in response:
        # Can also handle other types "function_call", "function_return", "function_message"
        if "assistant_message" in r:
            print("ASSISTANT:", r["assistant_message"])
        elif "internal_monologue" in r:
            print("THOUGHTS:", r["internal_monologue"])

# Send a message and see the response
send_message("Do you know what my name is? Please introduce yourself and tell me about your abilities!")
50/25:
response = client.user_message(agent_id=agent_id, message="Thundercats are go!")
print(response)
print(type(response))
68/4:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
68/5: from memory.user import UserManager
68/6:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    first_name="Johannes",
    last_name="Foulds",
    message="Good morning!"
)
68/7:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    first_name="Johannes",
    last_name="Foulds",
    message="Good morning!"
)
68/8:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    first_name="Johannes",
    last_name="Foulds",
    message="Good morning!"
)
68/9:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    first_name="Johannes",
    last_name="Foulds",
    message="Good morning!"
)
68/10:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    first_name="Johannes",
    last_name="Foulds",
    message="Good morning!"
)
68/11:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    first_name="Johannes",
    last_name="Foulds",
    message="Good morning!"
)
68/12:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    first_name="Johannes",
    last_name="Foulds",
    message="Good morning!"
)
68/13:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    first_name="Johannes",
    last_name="Foulds",
    message="Good morning!"
)
69/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
69/2: from memory.user import UserManager
69/3:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    first_name="Johannes",
    last_name="Foulds",
    message="Good morning!"
)
70/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
70/2: from memory.user import UserManager
70/3:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    first_name="Johannes",
    last_name="Foulds",
    message="Good morning!"
)
70/4:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
70/5: from memory.user import UserManager
70/6:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    first_name="Johannes",
    last_name="Foulds",
    message="Good morning!"
)
70/7:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
70/8: from memory.user import UserManager
70/9:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    first_name="Johannes",
    last_name="Foulds",
    message="Good morning!"
)
70/10:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    first_name="Johannes",
    last_name="Foulds",
    message="Good morning!"
)
70/11:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    first_name="Johannes",
    last_name="Foulds",
    message="Good morning!"
)
67/10:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
67/11: from memory.persona import Persona
67/12: Persona.init_persona()
71/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
71/2: from memory.persona import Persona
71/3: Persona.init_persona()
71/4:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
71/5: from memory.persona import Persona
71/6: Persona.init_persona()
72/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
72/2: from memory.user import UserManager
72/3:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    first_name="Johannes",
    last_name="Foulds",
    message="Good morning!"
)
72/4:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    first_name="Johannes",
    last_name="Foulds",
    message="Good morning!"
)
75/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
75/2: from memory.user import UserManager
75/3:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)
78/1:
import os
import uuid

from memgpt import MemGPT
from memgpt import constants

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
from memgpt.cli.cli_config import configure, list, add, delete
78/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
78/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
78/4:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))

# test the function
telegram_id = 1237109342
print(get_user_id(telegram_id))
78/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key,
    "model": "gpt-3.5-turbo",
  },
  user_id=get_user_id(telegram_id)
)
78/6: client.list_agents()
78/7: ?client.list_agents
78/8:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_name = "ideationDorkus"
agent_id = None

if client.agent_exists(agent_name=agent_name):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name=agent_name,
        agents_dict=agents_dict)
else:
    # add the human
    add(
        option="human",
        name="dorkus",
        text="Name: Dorkus Malorkus.",
    )

    # create the agent
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": agent_name,
        "human": "dorkus",
    })
    agent_id = agent.id
78/9:
# Create a helper that sends a message and prints the assistant response only
def send_message(message: str):
    """
    sends a message and prints the assistant output only.
    :param message: the message to send
    """
    response = client.user_message(agent_id=agent_id, message=message)
    for r in response:
        # Can also handle other types "function_call", "function_return", "function_message"
        if "assistant_message" in r:
            print("ASSISTANT:", r["assistant_message"])
        elif "internal_monologue" in r:
            print("THOUGHTS:", r["internal_monologue"])

# Send a message and see the response
send_message("Do you know what my name is? Please introduce yourself and tell me about your abilities!")
78/10:
response = client.user_message(agent_id=agent_id, message="Thundercats are go!")
print(response)
print(type(response))
78/11: client.list_agents()
79/1:
import os
import uuid

from memgpt import MemGPT
from memgpt import constants

from memgpt.config import (
    AgentConfig,
    AgentState,
    EmbeddingConfig,
    LLMConfig,
)

from memgpt.cli.cli import QuickstartChoice
from memgpt.cli.cli_config import configure, list, add, delete
79/2:
os.environ["OPENAI_API_KEY"] = "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"
#os.environ["OPENAI_API_KEY"] = "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"
79/3:
# get the openai api key from the environment
api_key = os.environ['OPENAI_API_KEY']
79/4:
def get_user_id(telegram_id:int) -> uuid.UUID:
    """Get the user id from the telegram id"""
    return uuid.uuid5(uuid.NAMESPACE_DNS, str(telegram_id))

# test the function
telegram_id = 1237109342
print(get_user_id(telegram_id))
79/5:
# Create a MemGPT client object (sets up the persistent state)
client = MemGPT(
  # When auto_save is 'True' then the agent(s) will be saved after every
  # user message.  This may have performance implications, so you
  # can otherwise choose when to save explicitly using client.save().  
  auto_save=True,

  # Quickstart will automatically configure MemGPT (without having to run `memgpt configure`
  # If you choose 'openai' then you must set the api key (env or in config)
  quickstart=QuickstartChoice.openai,

  # Allows you to override default config generated by quickstart or `memgpt configure`
  config={
    "openai_api_key": api_key,
    "model": "gpt-3.5-turbo",
  },
  user_id=get_user_id(telegram_id)
)
79/6: client.list_agents()
79/7: ?client.list_agents
79/8:
def find_agent_id(agent_name, agents_dict):
    for agent in agents_dict['agents']:
        if agent['name'] == agent_name:
            return agent['id']
    return None

agent_name = "ideationDorkus"
agent_id = None

if client.agent_exists(agent_name=agent_name):
    print("Agent already exists")
    agents_dict = client.list_agents()
    agent_id = find_agent_id(
        agent_name=agent_name,
        agents_dict=agents_dict)
else:
    # add the human
    add(
        option="human",
        name="dorkus",
        text="Name: Dorkus Malorkus.",
    )

    # create the agent
    agent = client.create_agent({
        "user_id": get_user_id(telegram_id),
        "name": agent_name,
        "human": "dorkus",
    })
    agent_id = agent.id
79/9:
# Create a helper that sends a message and prints the assistant response only
def send_message(message: str):
    """
    sends a message and prints the assistant output only.
    :param message: the message to send
    """
    response = client.user_message(agent_id=agent_id, message=message)
    for r in response:
        # Can also handle other types "function_call", "function_return", "function_message"
        if "assistant_message" in r:
            print("ASSISTANT:", r["assistant_message"])
        elif "internal_monologue" in r:
            print("THOUGHTS:", r["internal_monologue"])

# Send a message and see the response
send_message("Do you know what my name is? Please introduce yourself and tell me about your abilities!")
79/10:
response = client.user_message(agent_id=agent_id, message="Thundercats are go!")
print(response)
print(type(response))
80/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
80/2: from memory.persona import Persona
80/3: Persona.init_persona()
80/4:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
80/5: from memory.persona import Persona
80/6: Persona.init_persona()
77/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
77/2: from memory.user import UserManager
77/3:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)
81/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
81/2: from memory.persona import Persona
81/3: Persona.init_persona()
82/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
82/2:
from config.config import Config
from memory.user import UserManager
82/3:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)
83/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
83/2:
import config.config as config
from memory.user import UserManager
83/3:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)
81/4:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
81/5: from memory.persona import Persona
81/6: Persona.init_persona()
84/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
84/2:
import config.config as config
from memory.user import UserManager
84/3:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)
85/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
85/2:
import config.config as config
from memory.user import UserManager
85/3:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)
86/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
86/2:
import config.config as config
from memory.user import UserManager
86/3: config.init_logging()
86/4:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)
87/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
87/2:
import config.config as config
from memory.user import UserManager
87/3:
import logging
config.init_logging()
87/4:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)
89/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
89/2:
import config.config as config
from memory.user import UserManager
89/3:
import logging
#config.init_logging()
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s",
)
89/4:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)
89/5: logging.info("test")
90/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
90/2:
#import config.config as config
from memory.user import UserManager
90/3:
import logging
#config.init_logging()
# logging.basicConfig(
#     level=logging.INFO,
#     format="%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s",
# )
90/4: logging.info("test")
90/5:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)
90/6:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
90/7:
#import config.config as config
from memory.user import UserManager
90/8:
import logging
#config.init_logging()
# logging.basicConfig(
#     level=logging.INFO,
#     format="%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s",
# )
90/9: logging.info("test")
90/10:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)
90/11: logging.info("test")
91/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
91/2:
#import config.config as config
from memory.user import UserManager
91/3:
import logging
#config.init_logging()
# logging.basicConfig(
#     level=logging.INFO,
#     format="%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s",
# )
91/4: logging.info("test")
91/5:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)
92/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
92/2:
#import config.config as config
from memory.user import UserManager
92/3:
import logging
#config.init_logging()
# logging.basicConfig(
#     level=logging.INFO,
#     format="%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s",
# )
92/4: logging.info("test")
92/5:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)
92/6:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

logging.info("done")
92/7:
user_manager = UserManager()
user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

logging.info("done")
92/8:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
92/9: logging.info("done")
93/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
93/2:
#import config.config as config
from memory.user import UserManager
93/3:
import logging
#config.init_logging()
# logging.basicConfig(
#     level=logging.INFO,
#     format="%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s",
# )
93/4: logging.info("test")
93/5:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
93/6: logging.info("done")
93/7:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
93/8:
#import config.config as config
from memory.user import UserManager
93/9:
import logging
#config.init_logging()
# logging.basicConfig(
#     level=logging.INFO,
#     format="%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s",
# )
93/10: logging.info("test")
93/11:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
93/12: logging.info("done")
94/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
94/2:
#import config.config as config
from memory.user import UserManager
94/3:
import logging
#config.init_logging()
# logging.basicConfig(
#     level=logging.INFO,
#     format="%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s",
# )
94/4: logging.info("test")
94/5:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
94/6: logging.info("done")
95/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
95/2:
#import config.config as config
from memory.user import UserManager
95/3:
import logging
#config.init_logging()
# logging.basicConfig(
#     level=logging.INFO,
#     format="%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s",
# )
95/4: logging.info("test")
95/5:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
95/6: logging.info("done")
96/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
96/2:
#import config.config as config
from memory.user import UserManager
96/3:
import logging
#config.init_logging()
# logging.basicConfig(
#     level=logging.INFO,
#     format="%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s",
# )
96/4: logging.info("test")
96/5:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
96/6: logging.info("done")
97/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
97/2:
#import config.config as config
from memory.user import UserManager
97/3:
import logging
#config.init_logging()
# logging.basicConfig(
#     level=logging.INFO,
#     format="%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s",
# )
97/4: logging.info("test")
97/5:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
97/6: logging.info("done")
98/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
98/2:
#import config.config as config
from memory.user import UserManager
98/3:
import logging
#config.init_logging()
# logging.basicConfig(
#     level=logging.INFO,
#     format="%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s",
# )
98/4: logging.info("test")
98/5:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
98/6: logging.info("done")
99/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
99/2:
#import config.config as config
from memory.user import UserManager
99/3:
import logging
#config.init_logging()
# logging.basicConfig(
#     level=logging.INFO,
#     format="%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s",
# )
99/4: logging.info("test")
99/5:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
99/6: logging.info("done")
100/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
100/2:
#import config.config as config
from memory.user import UserManager
100/3:
import logging
#config.init_logging()
# logging.basicConfig(
#     level=logging.INFO,
#     format="%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s",
# )
100/4: logging.info("test")
100/5:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
100/6: logging.info("test")
100/7: logging.info("test")
100/8: logging.info("test")
100/9: logging.info("test")
101/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
101/2:
import config.config as config
#from memory.user import UserManager
102/1:
%load_ext autoreload
%autoreload 2

import os
import sys
import logging

sys.path.append(os.path.abspath("../src"))
102/2: logging.info("test")
102/3:
import config.config as config
#from memory.user import UserManager
102/4: logging.info("test")
102/5: logging.info("test")
103/1:
%load_ext autoreload
%autoreload 2

import os
import sys
import logging

sys.path.append(os.path.abspath("../src"))
103/2:

#config.init_logging()
# logging.basicConfig(
#     level=logging.INFO,
#     format="%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s",
# )
103/3:
import config.config as config
#from memory.user import UserManager
103/4: logging.info("test")
104/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
104/2:
import logging
import config.config as config
#from memory.user import UserManager
104/3: logging.info("test")
104/4:
import logging
import config.config as config
#from memory.user import UserManager
104/5: logging.info("test")
106/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
106/2:
import logging
import config.config as config
#from memory.user import UserManager
106/3:
config.init_logging()
# logging.basicConfig(
#     level=logging.INFO,
#     format="%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s",
# )
106/4: logging.info("test")
106/5: logging.info("done")
107/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
107/2:
import logging
import config.config as config
#from memory.user import UserManager
107/3: logging.info("test")
107/4: logging.info("test")
107/5: logging.info("done")
107/6: logging.info("done")
107/7:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
108/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
108/2:
import logging
import config.config as config
from memory.user import UserManager
108/3:
#config.init_logging()
# logging.basicConfig(
#     level=logging.INFO,
#     format="%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s",
# )
108/4: logging.info("test")
108/5:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
108/6:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
108/7:
import logging
import config.config as config
from memory.user import UserManager
108/8: logging.info("test")
108/9:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
108/10: logging.info("done")
108/11: logging.info("done")
109/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
109/2:
import logging
import config.config as config
from memory.user import UserManager
109/3: logging.info("test")
109/4:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
109/5: logging.info("done")
109/6: logging.info("done")
109/7: logger
109/8: logging._Level
109/9: logging.basicConfig
109/10:
# Get the root logger
root_logger = logging.getLogger()

# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
109/11:
# Get the root logger
root_logger = logging.getLogger()
109/12:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
109/13:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
110/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
110/2:
import logging
import config.config as config
from memory.user import UserManager
110/3:
# Get the root logger
root_logger = logging.getLogger()
110/4:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
110/5: logging.info("test")
110/6:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
110/7:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
110/8: logging.info("done")
111/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
111/2:
import logging
import config.config as config
from memory.user import UserManager
111/3:
# Get the root logger
root_logger = logging.getLogger()
111/4: logging.info("test")
111/5:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
111/6:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
111/7:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
111/8: logging.info("done")
112/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
112/2:
import logging
import config.config as config
from memory.user import UserManager
112/3:
# Get the root logger
root_logger = logging.getLogger()
112/4: logging.info("test")
112/5:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
112/6:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
112/7:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
112/8: logging.info("done")
112/9:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
112/10:
import logging
import config.config as config
from memory.user import UserManager
112/11:
# Get the root logger
root_logger = logging.getLogger()
112/12: logging.info("test")
112/13:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
112/14:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
112/15:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
112/16: logging.info("done")
113/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
113/2:
import logging
import config.config as config
from memory.user import UserManager
113/3:
# Get the root logger
root_logger = logging.getLogger()
113/4: logging.info("test")
113/5:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
113/6:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
113/7:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
113/8: logging.info("done")
113/9:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
113/10:
import logging
import config.config as config
from memory.user import UserManager
113/11:
# Get the root logger
root_logger = logging.getLogger()
113/12: logging.info("test")
113/13:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
113/14:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
113/15:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
113/16: logging.info("done")
113/17:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
113/18:
import logging
import config.config as config
from memory.user import UserManager
113/19:
# Get the root logger
root_logger = logging.getLogger()
113/20: logging.info("test")
113/21:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
113/22:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
113/23:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
113/24: logging.info("done")
114/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
114/2:
import logging
import config.config as config
from memory.user import UserManager
114/3:
# Get the root logger
root_logger = logging.getLogger()
114/4: logging.info("test")
114/5:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
114/6:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
114/7:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
114/8: logging.info("done")
114/9:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
114/10:
import logging
import config.config as config
from memory.user import UserManager
114/11:
# Get the root logger
root_logger = logging.getLogger()
114/12: logging.info("test")
114/13:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
114/14:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
114/15:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
114/16: logging.info("done")
114/17:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
114/18:
import logging
import config.config as config
from memory.user import UserManager
114/19:
# Get the root logger
root_logger = logging.getLogger()
114/20: logging.info("test")
114/21:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
114/22:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
114/23:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
114/24: logging.info("done")
115/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
115/2:
import logging
import config.config as config
from memory.user import UserManager
115/3:
# Get the root logger
root_logger = logging.getLogger()
115/4: logging.info("test")
115/5:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
115/6:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print(result)
logging.info("done")
115/7:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
115/8: logging.info("done")
116/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
116/2:
import logging
import config.config as config
from memory.user import UserManager
116/3:
# Get the root logger
root_logger = logging.getLogger()
116/4: logging.info("test")
116/5:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
116/6:
user_manager = UserManager()
# result = user_manager.user_message(
#     user_id=1237109342,
#     openai_api_key="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",
#     full_name="Johannes Foulds",
#     message="Good morning!"
# )

# print(result)
logging.info("done")
116/7:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
116/8: logging.info("done")
117/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
117/2:
import logging
import config.config as config
from memory.user import UserManager
117/3:
# Get the root logger
root_logger = logging.getLogger()
117/4: logging.info("test")
117/5:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
117/6:
user_manager = UserManager()
# result = user_manager.user_message(
#     user_id=1237109342,
#     openai_api_key="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",
#     full_name="Johannes Foulds",
#     message="Good morning!"
# )

# print(result)
logging.info("done")
117/7:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
117/8: logging.info("done")
118/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
118/2:
import logging
import config.config as config
from memory.user import UserManager
118/3:
# Get the root logger
root_logger = logging.getLogger()
118/4: logging.info("test")
118/5:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
118/6:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

# print(result)
logging.info("done")
118/7:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
118/8: logging.info("done")
119/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
119/2:
import logging
import config.config as config
from memory.user import UserManager
119/3:
# Get the root logger
root_logger = logging.getLogger()
119/4: logging.info("test")
119/5:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
119/6:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

# print(result)
logging.info("done")
119/7:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
119/8: logging.info("done")
120/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
120/2:
import logging
import config.config as config
from memory.user import UserManager
120/3:
# Get the root logger
root_logger = logging.getLogger()
120/4: logging.info("test")
120/5:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
120/6:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

# print(result)
logging.info("done")
120/7:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
120/8: logging.info("done")
121/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
121/2:
import logging
import config.config as config
from memory.user import UserManager
121/3:
# Get the root logger
root_logger = logging.getLogger()
121/4: logging.info("test")
121/5:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
121/6:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

# print(result)
logging.info("done")
121/7:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
121/8: logging.info("done")
121/9:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
121/10:
import logging
import config.config as config
from memory.user import UserManager
121/11:
# Get the root logger
root_logger = logging.getLogger()
121/12: logging.info("test")
121/13:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
121/14:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

# print(result)
logging.info("done")
121/15:
# Print the level of the root logger
print(f'Root logger level: {root_logger.level}')
121/16: logging.info("done")
123/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
123/2:
import logging
import config.config as config
from memory.user import UserManager
123/3:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print('-'*80)
print(result)
logging.info("done")
123/4: logging.info("done")
123/5:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print('-'*80)
print(result)
logging.info("done")
123/6:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print('-'*80)
print(result)
logging.info("done")
123/7:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
123/8:
import logging
import config.config as config
from memory.user import UserManager
123/9:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print('-'*80)
print(result)
logging.info("done")
123/10: logging.info("done")
124/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
124/2:
import logging
import config.config as config
from memory.user import UserManager
124/3:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print('-'*80)
print(result)
logging.info("done")
125/1: !memgpt list agents
126/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
126/2:
import logging
import config.config as config
from memory.user import UserManager
126/3:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print('-'*80)
print(result)
logging.info("done")
126/4: logging.info("done")
125/2: !memgpt list agents
127/1: from memgpt.metadata import MetadataStore, save_agent
127/2: !memgpt list agents
127/3:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
127/4: !memgpt list agents
128/1:
# does the config exist
MemGPTConfig.exists()
128/2:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
128/3: !memgpt list agents
128/4:
# does the config exist
MemGPTConfig.exists()
129/1:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
129/2: !memgpt list agents
129/3:
# does the config exist
MemGPTConfig.exists()
129/4:
# get the config
config = MemGPTConfig()
129/5:
# get the config
config = MemGPTConfig()
config
129/6:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent

from IPython import display, Markdown
129/7:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent

from IPython.display import display, Markdown


from
129/8:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent

from IPython.display import display, Markdown
129/9:
# get the config
config = MemGPTConfig()
display(config)
129/10:
# get the config
config = MemGPTConfig()
display(MarkDown(config))
129/11:
# get the config
config = MemGPTConfig()
display(Markdown(config))
129/12:
# get the config
config = MemGPTConfig()
display(Markdown(str(config)))
129/13:
# get the config
config = MemGPTConfig()
display(Markdown(str(config)))
129/14:
ms = MetadataStore(config)
ms
129/15:
ms = MetadataStore(config)
ms
129/16:
ms = MetadataStore(config)
ms
129/17:
ms = MetadataStore(config)
ms
129/18:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState

from IPython.display import display, Markdown
131/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
131/2:
import logging
import config.config as config
from memory.user import UserManager
131/3:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print('-'*80)
print(result)
logging.info("done")
131/4: logging.info("done")
132/1:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState

from IPython.display import display, Markdown
132/2: !memgpt list agents
132/3:
# does the config exist
MemGPTConfig.exists()
132/4:
# get the config
config = MemGPTConfig()
display(Markdown(str(config)))
132/5:
# get the metadata store from the config
ms = MetadataStore(config)
ms
132/6:
agent_state = AgentState(
    name="notebook_agent",
    user_id=user.id,
    persona=persona if persona else config.persona,
    human=human if human else config.human,
    preset=preset if preset else config.preset,
    llm_config=llm_config,
    embedding_config=embedding_config,
)
132/7:
# the existing user id
user_id = uuid.UUID('6c2b73e1-740b-5b7b-a9a6-9c73008ff899')
132/8:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState

import uuid
from IPython.display import display, Markdown
132/9:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState

import uuid
from IPython.display import display, Markdown
132/10: !memgpt list agents
132/11:
# does the config exist
MemGPTConfig.exists()
132/12:
# get the config
config = MemGPTConfig()
display(Markdown(str(config)))
132/13:
# get the metadata store from the config
ms = MetadataStore(config)
ms
132/14:
# the existing user id
user_id = uuid.UUID('6c2b73e1-740b-5b7b-a9a6-9c73008ff899')
132/15:
# create the agent state
agent_state = AgentState(
    name="notebook_agent",
    user_id=user.id,
    persona=persona if persona else config.persona,
    human=human if human else config.human,
    preset=preset if preset else config.preset,
    llm_config=llm_config,
    embedding_config=embedding_config,
)
133/1:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState

import uuid
from IPython.display import display, Markdown
133/2: !memgpt list agents
133/3:
# does the config exist
MemGPTConfig.exists()
133/4:
# get the config
config = MemGPTConfig()
display(Markdown(str(config)))
133/5:
# get the metadata store from the config
ms = MetadataStore(config)
ms
133/6:
# the existing user id
user_id = uuid.UUID('6c2b73e1-740b-5b7b-a9a6-9c73008ff899')
133/7:
# create the agent state
agent_state = AgentState(
    name="notebook_agent",
    user_id=user.id,
    persona=persona if persona else config.persona,
    human=human if human else config.human,
    preset=preset if preset else config.preset,
    llm_config=llm_config,
    embedding_config=embedding_config,
)
133/8: !memgpt list humans
133/9:
# create the agent state
agent_state = AgentState(
    name="notebook_agent",
    user_id=user_id,
    persona="sam_pov",
    human=human,
    preset=config.preset,
    llm_config=config.default_llm_config,
    embedding_config=config.default_embedding_config,
)
133/10:
# the existing user id (03_user.ipynb)
user_id = uuid.UUID('6c2b73e1-740b-5b7b-a9a6-9c73008ff899')
human = "1237109342"
133/11:
# create the agent state
agent_state = AgentState(
    name="notebook_agent",
    user_id=user_id,
    persona="sam_pov",
    human=human,
    preset=config.preset,
    llm_config=config.default_llm_config,
    embedding_config=config.default_embedding_config,
)
133/12:
# create the agent
ms.create_agent(agent_state)
133/13: !memgpt list agents
133/14:
# create agent
preset = ms.get_preset(preset_name=agent_state.preset, user_id=user_id)
memgpt_agent = presets.create_agent_from_preset(
    agent_state=agent_state,
    preset=preset,
    interface=interface(),
)
save_agent(agent=memgpt_agent, ms=ms)
133/15: ?ms
133/16:
# create agent
memgpt_agent = presets.create_agent_from_preset(
    agent_state=agent_state,
    interface=interface(),
)
save_agent(agent=memgpt_agent, ms=ms)
133/17:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 

import uuid
from IPython.display import display, Markdown
133/18:
# create agent
memgpt_agent = presets.create_agent_from_preset(
    agent_state=agent_state,
    interface=interface(),
)
save_agent(agent=memgpt_agent, ms=ms)
133/19: !memgpt list agents
133/20: !memgpt list agents
133/21:
# create agent
memgpt_agent = presets.create_agent_from_preset(
    agent_state=agent_state,
    interface=interface(),
)
save_agent(agent=memgpt_agent, ms=ms)
133/22: !memgpt list agents
134/1:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 

import uuid
from IPython.display import display, Markdown
134/2: !memgpt list humans
134/3: !memgpt list agents
134/4:
# does the config exist
MemGPTConfig.exists()
134/5:
# get the config
config = MemGPTConfig()
display(Markdown(str(config)))
134/6:
# get the metadata store from the config
ms = MetadataStore(config)
ms
134/7:
# the existing user id (03_user.ipynb)
user_id = uuid.UUID('6c2b73e1-740b-5b7b-a9a6-9c73008ff899')
human = "1237109342"
134/8:
# create the agent state
agent_state = AgentState(
    name="notebook_agent",
    user_id=user_id,
    persona="sam_pov",
    human=human,
    preset=config.preset,
    llm_config=config.default_llm_config,
    embedding_config=config.default_embedding_config,
)
134/9:
# initialize agent creation
ms.create_agent(agent_state)
131/5:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
131/6:
import logging
import config.config as config
from memory.user import UserManager
131/7:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print('-'*80)
print(result)
logging.info("done")
131/8: logging.info("done")
134/10:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 

import uuid
from IPython.display import display, Markdown
134/11: !memgpt list humans
134/12: !memgpt list agents
134/13:
# does the config exist
MemGPTConfig.exists()
134/14:
# get the config
config = MemGPTConfig()
display(Markdown(str(config)))
134/15:
# get the metadata store from the config
ms = MetadataStore(config)
ms
134/16:
# the existing user id (03_user.ipynb)
user_id = uuid.UUID('6c2b73e1-740b-5b7b-a9a6-9c73008ff899')
human = "1237109342"
134/17:
# create the agent state
agent_state = AgentState(
    name="notebook_agent",
    user_id=user_id,
    persona="sam_pov",
    human=human,
    preset=config.preset,
    llm_config=config.default_llm_config,
    embedding_config=config.default_embedding_config,
)
134/18:
# initialize agent creation
ms.create_agent(agent_state)
134/19: !memgpt list agents
134/20:
# create agent
memgpt_agent = presets.create_agent_from_preset(
    agent_state=agent_state,
    interface=interface(),
)
save_agent(agent=memgpt_agent, ms=ms)
134/21: !memgpt list agents
134/22:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
frin memgpt.cli import create_default_user_or_exit

import uuid
from IPython.display import display, Markdown
134/23:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
fron memgpt.cli import create_default_user_or_exit

import uuid
from IPython.display import display, Markdown
134/24:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
from memgpt.cli import create_default_user_or_exit

import uuid
from IPython.display import display, Markdown
134/25:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
from memgpt.cli.cli import create_default_user_or_exit

import uuid
from IPython.display import display, Markdown
134/26:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
from memgpt.cli.cli import create_default_user_or_exit

import uuid
from IPython.display import display, Markdown
134/27: !memgpt list humans
134/28: !memgpt list agents
134/29:
# does the config exist
MemGPTConfig.exists()
134/30:
# get the config
config = MemGPTConfig()
display(Markdown(str(config)))
134/31:
# get the metadata store from the config
ms = MetadataStore(config)
ms
134/32:
# the existing user id (03_user.ipynb)
user_id = uuid.UUID('6c2b73e1-740b-5b7b-a9a6-9c73008ff899')
human = "1237109342"
134/33:
# create a seperate user for testing
user = create_default_user_or_exit(config, ms)
134/34: config.anon_clientid
131/9:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
131/10:
import logging
import config.config as config
from memory.user import UserManager
131/11:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print('-'*80)
print(result)
logging.info("done")
131/12: logging.info("done")
136/1:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
from memgpt.cli.cli import create_default_user_or_exit

import uuid
from IPython.display import display, Markdown
136/2: !memgpt list humans
136/3: !memgpt list agents
136/4:
# does the config exist
MemGPTConfig.exists()
136/5:
# get the config
config = MemGPTConfig()
display(Markdown(str(config)))
136/6:
# get the metadata store from the config
ms = MetadataStore(config)
ms
136/7: config.anon_clientid
138/1:
# create a new user
ms.create_user(User(id=user_id))
user = ms.get_user(user_id=user_id)
138/2:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState, User
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
#from memgpt.cli.cli import create_default_user_or_exit

import uuid
from IPython.display import display, Markdown
138/3:
# create agent
memgpt_agent = presets.create_agent_from_preset(
    agent_state=agent_state,
    interface=interface(),
)
save_agent(agent=memgpt_agent, ms=ms)
139/1:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState, User
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
#from memgpt.cli.cli import create_default_user_or_exit

import uuid
from IPython.display import display, Markdown
139/2: !memgpt list humans
139/3: !memgpt list agents
139/4:
# does the config exist
MemGPTConfig.exists()
139/5:
# get the config
config = MemGPTConfig()
display(Markdown(str(config)))
139/6:
# get the metadata store from the config
ms = MetadataStore(config)
ms
139/7: config.anon_clientid
139/8:
# the existing user id (03_user.ipynb)
user_id = uuid.UUID('6c2b73e1-740b-5b7b-a9a6-9c73008ff899')
human = "1237109342"
139/9:
# create a new user
ms.create_user(User(id=user_id))
user = ms.get_user(user_id=user_id)
139/10:
# create a seperate user for testing
user = create_default_user_or_exit(config, ms)
139/11: !memgpt list agents
140/1:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState, User
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
#from memgpt.cli.cli import create_default_user_or_exit

import uuid
from IPython.display import display, Markdown
140/2: !memgpt list humans
142/1:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState, User
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
#from memgpt.cli.cli import create_default_user_or_exit

import uuid
from IPython.display import display, Markdown
142/2: !memgpt list humans
142/3: !memgpt list agents
142/4:
# does the config exist
MemGPTConfig.exists()
142/5:
# get the config
config = MemGPTConfig()
display(Markdown(str(config)))
142/6:
# get the metadata store from the config
ms = MetadataStore(config)
ms
142/7: config.anon_clientid
142/8:
# the existing user id (03_user.ipynb)
user_id = uuid.UUID('6c2b73e1-740b-5b7b-a9a6-9c73008ff899')
human = "1237109342"
142/9:
# create a new user
ms.create_user(User(id=user_id))
user = ms.get_user(user_id=user_id)
142/10: user
142/11: user.default_agent
142/12: !memgpt list agents
142/13:
# create the agent state
agent_state = AgentState(
    name="notebook_agent",
    user_id=user_id,
    persona="sam_pov",
    human=human,
    preset=config.preset,
    llm_config=config.default_llm_config,
    embedding_config=config.default_embedding_config,
)
142/14:
# initialize agent creation
ms.create_agent(agent_state)
142/15: !memgpt list agents
142/16:
# create agent
memgpt_agent = presets.create_agent_from_preset(
    agent_state=agent_state,
    interface=interface(),
)
save_agent(agent=memgpt_agent, ms=ms)
142/17: config,human
142/18: config.human
143/1:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState, User
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
#from memgpt.cli.cli import create_default_user_or_exit

import uuid
from IPython.display import display, Markdown
143/2: !memgpt list humans
143/3: !memgpt list agents
143/4:
# does the config exist
MemGPTConfig.exists()
143/5:
# get the config
config = MemGPTConfig()
display(Markdown(str(config)))
143/6:
# get the metadata store from the config
ms = MetadataStore(config)
ms
143/7: config.anon_clientid
143/8:
# the existing user id (03_user.ipynb)
user_id = uuid.UUID('6c2b73e1-740b-5b7b-a9a6-9c73008ff899')
human = "1237109342"
143/9:
# create a new user
ms.create_user(User(id=user_id))
user = ms.get_user(user_id=user_id)

# overwrite the human for the test
human = config.human
143/10:
# create the agent state
agent_state = AgentState(
    name="notebook_agent",
    user_id=user_id,
    persona="sam_pov",
    human=human,
    preset=config.preset,
    llm_config=config.default_llm_config,
    embedding_config=config.default_embedding_config,
)
143/11:
# initialize agent creation
ms.create_agent(agent_state)
143/12: !memgpt list agents
143/13:
# create agent
memgpt_agent = presets.create_agent_from_preset(
    agent_state=agent_state,
    interface=interface(),
)
save_agent(agent=memgpt_agent, ms=ms)
143/14: !memgpt list agents
143/15: !memgpt list agents
144/1:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState, User
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
#from memgpt.cli.cli import create_default_user_or_exit

import uuid
from IPython.display import display, Markdown
144/2: !memgpt list humans
144/3: !memgpt list agents
144/4:
# does the config exist
MemGPTConfig.exists()
144/5:
# get the config
config = MemGPTConfig()
display(Markdown(str(config)))
144/6:
# get the metadata store from the config
ms = MetadataStore(config)
ms
144/7: config.anon_clientid
144/8: config.anon_clientid
144/9: uuid.UUID()
144/10: uuid.UUID(None)
145/1:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState, User
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
#from memgpt.cli.cli import create_default_user_or_exit

import uuid
from IPython.display import display, Markdown
145/2: !memgpt list humans
145/3:
# does the config exist
MemGPTConfig.exists()
145/4:
# get the config
config = MemGPTConfig()
display(Markdown(str(config)))
145/5:
# get the metadata store from the config
ms = MetadataStore(config)
ms
145/6: config.anon_clientid
146/1:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState, User
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
#from memgpt.cli.cli import create_default_user_or_exit

import uuid
from IPython.display import display, Markdown
146/2: !memgpt list humans
146/3: !memgpt list agents
146/4:
# does the config exist
MemGPTConfig.exists()
146/5:
# get the config
config = MemGPTConfig()
display(Markdown(str(config)))
146/6:
# get the metadata store from the config
ms = MetadataStore(config)
ms
147/1:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState, User
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
from memgpt.cli.cli import create_default_user_or_exit

import uuid
from IPython.display import display, Markdown
147/2: !memgpt list humans
147/3: !memgpt list agents
147/4:
# does the config exist
MemGPTConfig.exists()
147/5:
# get the config
config = MemGPTConfig()
display(Markdown(str(config)))
147/6:
# get the metadata store from the config
ms = MetadataStore(config)
ms
147/7: user = create_default_user_or_exit(config, ms)
147/8: user = create_default_user_or_exit(config, ms)
147/9:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState, User
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
from memgpt.cli.cli import create_default_user_or_exit
  from memgpt.migrate import config_is_compatible

import uuid
from IPython.display import display, Markdown
147/10:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState, User
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
from memgpt.cli.cli import create_default_user_or_exit
from memgpt.migrate import config_is_compatible

import uuid
from IPython.display import display, Markdown
147/11: config_is_compatible(allow_empty=True)
148/1:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState, User
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
from memgpt.cli.cli import create_default_user_or_exit
from memgpt.migrate import config_is_compatible

import uuid
from IPython.display import display, Markdown
148/2: !memgpt list humans
148/3: !memgpt list agents
148/4: config_is_compatible(allow_empty=True)
148/5:
# does the config exist
MemGPTConfig.exists()
148/6:
# get the config
config = MemGPTConfig()
display(Markdown(str(config)))
148/7: config.memgpt_version
148/8:
# get the metadata store from the config
ms = MetadataStore(config)
ms
148/9: user = create_default_user_or_exit(config, ms)
149/1:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState, User
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
from memgpt.cli.cli import create_default_user_or_exit
from memgpt.migrate import config_is_compatible

import uuid
from IPython.display import display, Markdown
149/2: !memgpt list humans
149/3: !memgpt list agents
149/4: config_is_compatible(allow_empty=True)
149/5:
# does the config exist
MemGPTConfig.exists()
149/6:
# get the config
config = MemGPTConfig().load()
display(Markdown(str(config)))
149/7: config.memgpt_version
149/8:
# get the metadata store from the config
ms = MetadataStore(config)
ms
149/9: user = create_default_user_or_exit(config, ms)
150/1:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState, User
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
from memgpt.cli.cli import create_default_user_or_exit
from memgpt.migrate import config_is_compatible

import uuid
from IPython.display import display, Markdown
150/2: !memgpt list humans
150/3: !memgpt list agents
150/4: config_is_compatible(allow_empty=True)
150/5:
# does the config exist
MemGPTConfig.exists()
150/6:
# get the config
config = MemGPTConfig().load()
display(Markdown(str(config)))
150/7: config.memgpt_version
150/8:
# get the metadata store from the config
ms = MetadataStore(config)
ms
150/9: user = create_default_user_or_exit(config, ms)
150/10:
agents = ms.list_agents(user_id=user.id)
agents = [a.name for a in agents]
150/11:
agents = ms.list_agents(user_id=user.id)
agents = [a.name for a in agents]

agents
150/12: user = create_default_user_or_exit(config, ms)
150/13:
agents = ms.list_agents(user_id=user.id)
agents = [a.name for a in agents]

agents
150/14: config.anon_clientid
150/15:
# create a new user
# ms.create_user(User(id=user_id))
# user = ms.get_user(user_id=user_id)

# overwrite the human for the test
human = config.human
150/16:
# create the agent state
agent_state = AgentState(
    name="notebook_agent",
    user_id=user.id,
    persona="sam_pov",
    human=human,
    preset=config.preset,
    llm_config=config.default_llm_config,
    embedding_config=config.default_embedding_config,
)
150/17:
# initialize agent creation
ms.create_agent(agent_state)
150/18: !memgpt list agents
150/19:
# create agent
memgpt_agent = presets.create_agent_from_preset(
    agent_state=agent_state,
    interface=interface(),
)
save_agent(agent=memgpt_agent, ms=ms)
150/20: !memgpt list agents
150/21:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState, User
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
from memgpt.cli.cli import create_default_user_or_exit
from memgpt.migrate import config_is_compatible

import uuid
from IPython.display import display, Markdown
150/22: !memgpt list humans
150/23: !memgpt list agents
150/24: config_is_compatible(allow_empty=True)
150/25:
# does the config exist
MemGPTConfig.exists()
150/26:
# get the config
config = MemGPTConfig().load()
display(Markdown(str(config)))
150/27: config.memgpt_version
150/28:
# get the metadata store from the config
ms = MetadataStore(config)
ms
150/29: user = create_default_user_or_exit(config, ms)
150/30:
agents = ms.list_agents(user_id=user.id)
agents = [a.name for a in agents]

agents
150/31: config.anon_clientid
150/32:
# # the existing user id (03_user.ipynb)
# user_id = uuid.UUID('6c2b73e1-740b-5b7b-a9a6-9c73008ff899')
# human = "1237109342"
150/33:
# create a new user
# ms.create_user(User(id=user_id))
# user = ms.get_user(user_id=user_id)

# overwrite the human for the test
human = config.human
150/34:
# create the agent state
agent_state = AgentState(
    name="notebook_agent",
    user_id=user.id,
    persona="sam_pov",
    human=human,
    preset=config.preset,
    llm_config=config.default_llm_config,
    embedding_config=config.default_embedding_config,
)
150/35:
# initialize agent creation
ms.create_agent(agent_state)
150/36: !memgpt list agents
150/37:
# create agent
memgpt_agent = presets.create_agent_from_preset(
    agent_state=agent_state,
    interface=interface(),
)
save_agent(agent=memgpt_agent, ms=ms)
150/38: !memgpt list agents
151/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
151/2: from memory.user import UserManager
151/3:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print('-'*80)
print(result)
152/1:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState, User
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
from memgpt.cli.cli import create_default_user_or_exit
from memgpt.migrate import config_is_compatible

import uuid
from IPython.display import display, Markdown
152/2:
# get the config
config = MemGPTConfig().load()
display(Markdown(str(config)))
152/3:
# get the metadata store from the config
ms = MetadataStore(config)
ms
153/1:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState, User
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
from memgpt.cli.cli import create_default_user_or_exit
from memgpt.migrate import config_is_compatible

import uuid
from IPython.display import display, Markdown
153/2:
# get the config
config = MemGPTConfig().load()
display(Markdown(str(config)))
153/3:
# get the metadata store from the config
ms = MetadataStore(config)
ms
155/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
155/2: from memory.user import UserManager
155/3:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print('-'*80)
print(result)
155/4:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
155/5: from memory.user import UserManager
155/6:
user_manager = UserManager()
result = user_manager.user_message(
    user_id=1237109342,
    openai_api_key="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",
    full_name="Johannes Foulds",
    message="Good morning!"
)

print('-'*80)
print(result)
156/1: !memgpt list sources
156/2: !memgpt list agents
157/1: !memgpt list agents
157/2: !memgpt list sources
158/1:
from memgpt.config import MemGPTConfig
from memgpt.metadata import MetadataStore, save_agent
from memgpt.data_types import AgentState, User
import memgpt.presets.presets as presets
from memgpt.interface import CLIInterface as interface 
from memgpt.cli.cli import create_default_user_or_exit
from memgpt.migrate import config_is_compatible

import uuid
from IPython.display import display, Markdown
157/3: import memgpt.cli.cli_load as cli_load
157/4: file_path = "../../data"
157/5: !ls $file_path
157/6:
file_path = "../data"
file = "2212.10496.pdf"
157/7:
file_path = "../data"
file_name = "2212.10496.pdf"
157/8:
cli_load.load_file(
    name=data_source,
    input_dir=file_path,
    input_files=[file_name]
)
157/9:
cli_load.load_directory(
    name=data_source,
    input_dir=file_path,
    input_files=[file_name]
)
157/10:
data_source = "arxiv"
file_path = "../data"
file_name = "2212.10496.pdf"
157/11:
cli_load.load_directory(
    name=data_source,
    input_dir=file_path,
    input_files=[file_name]
)
157/12: !memgpt list sources
157/13:
# add a second document
cli_load.load_directory(
    name=data_source,
    input_dir=file_path,
    input_files=["poem.png"]
)
157/14: !memgpt list sources
157/15:
import memgpt.cli.cli as cli
import memgpt.cli.cli_load as cli_load
157/16: agent_name = "jarvis_1237109343"
157/17:
cli.attach(
    agent=agent_name,
    data_source=data_source
)
157/18:
cli.attach(
    agent_name=agent_name,
    data_source=data_source
)
157/19: !memgpt list sources
157/20:
import memgpt.cli.cli as cli
import memgpt.cli.cli_load as cli_load

from memory.user import UserManager
157/21:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
157/22:
import memgpt.cli.cli as cli
import memgpt.cli.cli_load as cli_load

from memory.user import UserManager
157/23: user_message("Good afternoon.")
157/24:
user_manager = UserManager()

def user_message(message: str):
    global user_id
    global openai_api_key
    global full_name
    global user_manager

    return user_manager.user_message(
        user_id=user_id
        openai_api_key=openai_api_key,
        full_name=full_name,
        message=message
    )
157/25:
user_manager = UserManager()

def user_message(message: str):
    global user_id
    global openai_api_key
    global full_name
    global user_manager

    return user_manager.user_message(
        user_id=user_idm
        openai_api_key=openai_api_key,
        full_name=full_name,
        message=message
    )
157/26:
user_manager = UserManager()

def user_message(message: str):
    global user_id
    global openai_api_key
    global full_name
    global user_manager

    return user_manager.user_message(
        user_id=user_id,
        openai_api_key=openai_api_key,
        full_name=full_name,
        message=message
    )
157/27: user_message("Good afternoon.")
157/28:
user_id = 1237109342
openai_api_key="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"
full_name="Johannes Foulds"
157/29:
user_manager = UserManager()

def user_message(message: str):
    global user_id
    global openai_api_key
    global full_name
    global user_manager

    return user_manager.user_message(
        user_id=user_id,
        openai_api_key=openai_api_key,
        full_name=full_name,
        message=message
    )
157/30: user_message("Good afternoon.")
157/31: user_message("Tell me about HyDE")
157/32: print(user_message("Explain to me how to practically implement it?"))
157/33: print(user_message("What are your sources?"))
157/34: print(user_message("Which paper introduced it?"))
157/35: print(user_message("Give me a summary of the paper"))
157/36: result = user_message("Give me a summary of the paper")
157/37: pprint(response)
157/38: print(response)
157/39: print(result)
157/40: pprint(result)
157/41:
import memgpt.cli.cli as cli
import memgpt.cli.cli_load as cli_load

from memory.user import UserManager
from pprint import pprint
157/42: pprint(result)
157/43: result = user_message("Write a summary of the paper and send it to me")
157/44: pprint(result)
161/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
161/2:
import memgpt.cli.cli as cli
import memgpt.cli.cli_load as cli_load

from memory.user import UserManager
from pprint import pprint
161/3: !memgpt list agents
161/4: !memgpt list sources
161/5:
data_source = "arxiv"
file_path = "../data"
file_name = "2212.10496.pdf"
161/6:
# add the initial document
cli_load.load_directory(
    name=data_source,
    input_dir=file_path,
    input_files=[file_name]
)
161/7: !memgpt list sources
161/8:
cli_load.load_directory(
    name=data_source,
    input_dir=file_path,
    input_files=["poem.png"]
)
161/9: agent_name = "jarvis_1237109343"
161/10:
cli.attach(
    agent_name=agent_name,
    data_source=data_source
)
161/11: !memgpt list sources
161/12:
user_id = 1237109342
openai_api_key="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"
full_name="Johannes Foulds"
161/13:
user_manager = UserManager()

def user_message(message: str):
    global user_id
    global openai_api_key
    global full_name
    global user_manager

    return user_manager.user_message(
        user_id=user_id,
        openai_api_key=openai_api_key,
        full_name=full_name,
        message=message
    )
161/14: user_message("Good afternoon.")
161/15: user_message("Tell me about HyDE")
161/16: print(user_message("Explain to me how to practically implement it?"))
161/17: print(user_message("What are your sources?"))
161/18: print(user_message("Which paper introduced it?"))
161/19: result = user_message("Give me a summary of the paper")
161/20: pprint(result)
161/21: result = user_message("Write a summary of the paper and send it to me")
161/22: pprint(result)
163/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
163/2:
import memgpt.cli.cli as cli
import memgpt.cli.cli_load as cli_load

from memory.user import UserManager
from pprint import pprint
163/3: !memgpt list agents
163/4: !memgpt list sources
163/5:
data_source = "arxiv"
file_path = "../data"
file_name = "2212.10496.pdf"
163/6:
# add the initial document
cli_load.load_directory(
    name=data_source,
    input_dir=file_path,
    input_files=[file_name]
)
163/7: !memgpt list sources
163/8:
cli_load.load_directory(
    name=data_source,
    input_dir=file_path,
    input_files=["poem.png"]
)
163/9: agent_name = "jarvis_1237109343"
163/10:
cli.attach(
    agent_name=agent_name,
    data_source=data_source
)
163/11: !memgpt list sources
164/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
164/2:
import memgpt.cli.cli as cli
import memgpt.cli.cli_load as cli_load

from memory.user import UserManager
from pprint import pprint
164/3: !memgpt list agents
164/4: !memgpt list sources
164/5:
data_source = "arxiv"
file_path = "../data"
file_name = "2212.10496.pdf"
164/6:
# add the initial document
cli_load.load_directory(
    name=data_source,
    input_dir=file_path,
    input_files=[file_name]
)
164/7: !memgpt list sources
164/8:
cli_load.load_directory(
    name=data_source,
    input_dir=file_path,
    input_files=["poem.png"]
)
164/9: agent_name = "jarvis_1237109343"
164/10:
cli.attach(
    agent_name=agent_name,
    data_source=data_source
)
164/11: !memgpt list sources
165/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
165/2:
import memgpt.cli.cli as cli
import memgpt.cli.cli_load as cli_load

from memory.user import UserManager
from pprint import pprint
165/3: !memgpt list agents
165/4: !memgpt list sources
165/5:
data_source = "arxiv"
file_path = "../data"
file_name = "2212.10496.pdf"
165/6:
# add the initial document
cli_load.load_directory(
    name=data_source,
    input_dir=file_path,
    input_files=[file_name]
)
165/7: !memgpt list sources
165/8:
cli_load.load_directory(
    name=data_source,
    input_dir=file_path,
    input_files=["poem.png"]
)
165/9: agent_name = "jarvis_1237109343"
165/10:
cli.attach(
    agent_name=agent_name,
    data_source=data_source
)
165/11: !memgpt list sources
165/12:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
165/13:
import memgpt.cli.cli as cli
import memgpt.cli.cli_load as cli_load

from memory.user import UserManager
from pprint import pprint
165/14: !memgpt list agents
165/15: !memgpt list sources
165/16:
data_source = "arxiv"
file_path = "../data"
file_name = "2212.10496.pdf"
165/17:
# add the initial document
cli_load.load_directory(
    name=data_source,
    input_dir=file_path,
    input_files=[file_name]
)
165/18:
# add the initial document
cli_load.load_directory(
    name=data_source,
    input_dir=file_path,
    input_files=[file_name]
)
165/19: !memgpt list sources
165/20:
# add the initial document
cli_load.load_directory(
    name=data_source,
    input_dir=file_path,
    input_files=[file_name]
)
165/21:
# add the initial document
cli_load.load_directory(
    name=data_source,
    input_dir=file_path,
    input_files=[file_name]
)
166/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
166/2:
import memgpt.cli.cli as cli
import memgpt.cli.cli_load as cli_load

from memory.user import UserManager
from pprint import pprint
166/3: !memgpt list agents
166/4: !memgpt list sources
166/5:
data_source = "arxiv"
file_path = "../data"
file_name = "2212.10496.pdf"
166/6:
# add the initial document
cli_load.load_directory(
    name=data_source,
    input_dir=file_path,
    input_files=[file_name]
)
166/7: !memgpt list sources
166/8:
cli_load.load_directory(
    name=data_source,
    input_dir=file_path,
    input_files=["poem.png"]
)
166/9: agent_name = "jarvis_1237109343"
166/10:
cli.attach(
    agent_name=agent_name,
    data_source=data_source
)
166/11: !memgpt list sources
168/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
168/2:
import memgpt.cli.cli as cli
import memgpt.cli.cli_load as cli_load

from memory.user import UserManager
from pprint import pprint
168/3: !memgpt list agents
168/4: !memgpt list sources
168/5:
data_source = "arxiv"
file_path = "../data"
file_name = "2212.10496.pdf"
168/6:
# add the initial document
cli_load.load_directory(
    name=data_source,
    input_dir=file_path,
    input_files=[file_name]
)
168/7: !memgpt list sources
168/8:
cli_load.load_directory(
    name=data_source,
    input_dir=file_path,
    input_files=["poem.png"]
)
168/9: agent_name = "jarvis_1237109343"
168/10:
cli.attach(
    agent_name=agent_name,
    data_source=data_source
)
168/11: !memgpt list sources
168/12:
user_id = 1237109342
openai_api_key="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"
full_name="Johannes Foulds"
168/13:
user_manager = UserManager()

def user_message(message: str):
    global user_id
    global openai_api_key
    global full_name
    global user_manager

    return user_manager.user_message(
        user_id=user_id,
        openai_api_key=openai_api_key,
        full_name=full_name,
        message=message
    )
168/14: result = user_message("Good afternoon.")
168/15: print(result)
168/16: result = user_message("Tell me about hypothetical documents")
168/17: print(result)
170/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
170/2:
import memgpt.cli.cli as cli
import memgpt.cli.cli_load as cli_load

from memory.user import UserManager
from pprint import pprint
170/3: !memgpt list agents
170/4: !memgpt list sources
170/5:
data_source = "arxiv"
file_path = "../data"
file_name = "2212.10496.pdf"
170/6:
# add the initial document
cli_load.load_directory(
    name=data_source,
    input_dir=file_path,
    input_files=[file_name]
)
170/7: !memgpt list sources
170/8:
cli_load.load_directory(
    name=data_source,
    input_dir=file_path,
    input_files=["poem.png"]
)
170/9: agent_name = "jarvis_1237109343"
170/10:
cli.attach(
    agent_name=agent_name,
    data_source=data_source
)
170/11: !memgpt list sources
170/12:
user_id = 1237109342
openai_api_key="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"
full_name="Johannes Foulds"
170/13:
user_manager = UserManager()

def user_message(message: str):
    global user_id
    global openai_api_key
    global full_name
    global user_manager

    return user_manager.user_message(
        user_id=user_id,
        openai_api_key=openai_api_key,
        full_name=full_name,
        message=message
    )
170/14: result = user_message("Good afternoon.")
170/15: pprint(result)
170/16: result = user_message("Tell me about hypothetical documents")
170/17: pprint(result)
170/18: result = user_message("I am thinking on context of the 'Precise Zero-Shot Dense Retrieval without Relevance Labels' paper")
170/19: pprint(result)
173/1:
from gradio_client import Client
import ast
174/1:
from gradio_client import Client
import ast
174/2: HOST_URL = "http://dragon:7860"
174/3:
# create the api client
client = Client(HOST_URL)
174/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who are you?')
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
print(response)
174/5:
# string of dict for input
kwargs = dict(instruction_nochat='Who are you?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
print(response)
174/6:
HOST_URL = "http://dragon:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
174/7:
# create the api client
client = Client(HOST_URL)
174/8:
# string of dict for input
kwargs = dict(instruction_nochat='Who are you?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
print(response)
174/9:
# string of dict for input
kwargs = dict(instruction_nochat='Who are you?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
print(response)
174/10:
HOST_URL = "http://localhost:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
175/1:
from gradio_client import Client
import ast
175/2:
HOST_URL = "http://localhost:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
175/3:
# create the api client
client = Client(HOST_URL)
175/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who are you?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
print(response)
176/1: from h2ogpt_client import Client
176/2:
# create the api client
client = Client(HOST_URL, h2ogpt_key=H2OGPT_KEY)
176/3:
HOST_URL = "http://0.0.0.0:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
176/4:
# create the api client
client = Client(HOST_URL, h2ogpt_key=H2OGPT_KEY)
176/5:
from h2ogpt_client import Client
import asyncio
176/6:
 # Text completion: synchronous
text_completion = client.text_completion.create()
response = text_completion.complete_sync("Hello world")
print("sync text completion response: %s" % response)
177/1:
from h2ogpt_client import Client
import asyncio
177/2:
HOST_URL = "http://0.0.0.0:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
177/3:
# create the api client
client = Client(HOST_URL, h2ogpt_key=H2OGPT_KEY)
177/4:
 # Text completion: synchronous
text_completion = client.text_completion.create()
response = text_completion.complete_sync("Hello world")
print("sync text completion response: %s" % response)
177/5:
from h2ogpt_client import Client
import asyncio
177/6:
HOST_URL = "http://0.0.0.0:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
177/7:
# create the api client
client = Client(HOST_URL, h2ogpt_key=H2OGPT_KEY)
177/8:
 # Text completion: synchronous
text_completion = client.text_completion.create()
response = text_completion.complete_sync("Hello world")
print("sync text completion response: %s" % response)
178/1:
from h2ogpt_client import Client
import asyncio
178/2:
HOST_URL = "http://127.0.0.1,:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
178/3:
# create the api client
client = Client(HOST_URL, h2ogpt_key=H2OGPT_KEY)
179/1:
from h2ogpt_client import Client
import asyncio
179/2:
HOST_URL = "http://127.0.0.1,:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
179/3:
# create the api client
client = Client(HOST_URL, h2ogpt_key=H2OGPT_KEY)
179/4:
from h2ogpt_client import Client
import asyncio
179/5:
HOST_URL = "http://127.0.0.1,:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
179/6:
# create the api client
client = Client(HOST_URL, h2ogpt_key=H2OGPT_KEY)
180/1:
from h2ogpt_client import Client
import asyncio
180/2:
HOST_URL = "http://127.0.0.1,:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
180/3:
# create the api client
client = Client(HOST_URL, h2ogpt_key=H2OGPT_KEY)
180/4:
# create the api client
#client = Client(HOST_URL, h2ogpt_key=H2OGPT_KEY)
client = Client(HOST_URL)
181/1:
from h2ogpt_client import Client
import asyncio
181/2:
HOST_URL = "http://127.0.0.1,:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
181/3:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
181/4:
# create the api client
#client = Client(HOST_URL, h2ogpt_key=H2OGPT_KEY)
client = Client(HOST_URL)
181/5:
 # Text completion: synchronous
text_completion = client.text_completion.create()
response = text_completion.complete_sync("Hello world")
print("sync text completion response: %s" % response)
181/6:
# create the api client
client = Client(HOST_URL, h2ogpt_key=H2OGPT_KEY)
#client = Client(HOST_URL)
181/7:
 # Text completion: synchronous
text_completion = client.text_completion.create()
response = text_completion.complete_sync("Hello world")
print("sync text completion response: %s" % response)
181/8:
# create the api client
client = Client(HOST_URL, h2ogpt_key=H2OGPT_KEY)
#client = Client(HOST_URL)
181/9:
 # Text completion: synchronous
text_completion = client.text_completion.create()
response = text_completion.complete_sync("Hello world")
print("sync text completion response: %s" % response)
181/10:
 # Text completion: synchronous
text_completion = client.text_completion.create()
response = text_completion.complete_sync("Hello world")
print("sync text completion response: %s" % response)
181/11:
 # Text completion: synchronous
text_completion = client.text_completion.create()
response = text_completion.complete_sync("Hello world")
print("sync text completion response: %s" % response)
183/1:
from h2ogpt_client import Client
import asyncio
183/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
183/3:
# create the api client
H2OGPT_KEY=None
client = Client(HOST_URL, h2ogpt_key=H2OGPT_KEY)
#client = Client(HOST_URL)
183/4:
 # Text completion: synchronous
text_completion = client.text_completion.create()
response = text_completion.complete_sync("Hello world")
print("sync text completion response: %s" % response)
183/5:
 # Text completion: synchronous
text_completion = client.text_completion.create()
response = text_completion.complete_sync("Hello world")
print("sync text completion response: %s" % response)
184/1:
from h2ogpt_client import Client
import asyncio
184/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
184/3:
# create the api client
H2OGPT_KEY=None
client = Client(HOST_URL, h2ogpt_key=H2OGPT_KEY)
#client = Client(HOST_URL)
184/4: client.models
184/5: client.models()
184/6: client.models
184/7: client.models.list()
184/8: client.server
185/1:
from h2ogpt_client import Client
import asyncio
185/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
185/3:
# create the api client
H2OGPT_KEY=None
client = Client(HOST_URL, h2ogpt_key=H2OGPT_KEY)
#client = Client(HOST_URL)
185/4:
# create the api client
H2OGPT_KEY=None
client = Client(HOST_URL, h2ogpt_key=H2OGPT_KEY)
#client = Client(HOST_URL)
172/1:
%load_ext autoreload
%autoreload 2

import os
import sys

sys.path.append(os.path.abspath("../src"))
172/2: from memory.user import UserManager
185/5:
# create the api client
H2OGPT_KEY=None
client = Client(HOST_URL, h2ogpt_key=H2OGPT_KEY)
#client = Client(HOST_URL)
187/1:
from h2ogpt_client import Client
import asyncio
187/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
187/3:
# create the api client
H2OGPT_KEY=None
client = Client(HOST_URL, h2ogpt_key=H2OGPT_KEY)
#client = Client(HOST_URL)
187/4: client.server
187/5:
 # Text completion: synchronous
text_completion = client.text_completion.create()
response = text_completion.complete_sync("Hello world")
print("sync text completion response: %s" % response)
188/1:
from h2ogpt_client import Client
import asyncio
188/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
188/3:
# create the api client
H2OGPT_KEY=None
client = Client(HOST_URL, h2ogpt_key=H2OGPT_KEY)
#client = Client(HOST_URL)
188/4: client.server
188/5:
 # Text completion: synchronous
text_completion = client.text_completion.create()
response = text_completion.complete_sync("Hello world")
print("sync text completion response: %s" % response)
188/6:
from gradio_client import Client
import ast
from h2ogpt_client import Client
import asyncio
188/7:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
188/8:
# create the api client
H2OGPT_KEY=None
client = Client(HOST_URL, h2ogpt_key=H2OGPT_KEY)
#client = Client(HOST_URL)
189/1:
from gradio_client import Client
import ast

HOST_URL = "http://localhost:7860"
client = Client(HOST_URL)

# string of dict for input
kwargs = dict(instruction_nochat='Who are you?')
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
print(response)
190/1:
from gradio_client import Client
import ast

HOST_URL = "http://localhost:7860"
client = Client(HOST_URL)

# string of dict for input
kwargs = dict(instruction_nochat='Who are you?')
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
print(response)
190/2:
from h2ogpt_client import Client
import asyncio
190/3:
from gradio_client import Client
import ast
191/1:
from gradio_client import Client
import ast
191/2:

HOST_URL = "http://localhost:7860"
client = Client(HOST_URL)

# string of dict for input
kwargs = dict(instruction_nochat='Who are you?')
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
print(response)
191/3:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
191/4: client = Client(HOST_URL)
191/5:
# string of dict for input
kwargs = dict(instruction_nochat='Who are you?')
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
print(response)
192/1:
from gradio_client import Client
import ast
192/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
192/3: client = Client(HOST_URL)
192/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who are you?')
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
print(response)
193/1:
from gradio_client import Client
import ast
193/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
193/3: client = Client(HOST_URL)
193/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who are you?')
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
print(response)
193/5:
# string of dict for input
kwargs = dict(instruction_nochat='Who are you?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
print(response)
194/1:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
print(response)
195/1:
from gradio_client import Client
import ast
195/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
195/3: client = Client(HOST_URL)
195/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
print(response)
195/5:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
196/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
196/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
196/3: client = Client(HOST_URL)
196/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
197/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
197/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
197/3: client = Client(HOST_URL)
197/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
198/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
198/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "501ACE1"
LANGCHAIN_MODE = "Vodacom"
198/3: client = Client(HOST_URL)
198/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
199/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
199/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "501ACE"
LANGCHAIN_MODE = "Vodacom"
199/3: client = Client(HOST_URL)
199/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
200/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
200/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
200/3: client = Client(HOST_URL)
200/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
200/5: new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'shared', user_path)
200/6: LANGCHAIN_MODE = "MyData"
200/7: new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'shared', user_path)
200/8: LANGCHAIN_MODE = langchain_mode = "MyData"
200/9: new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'shared', user_path)
200/10: langchain_mode = "MyData"
201/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
201/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
201/3: client = Client(HOST_URL)
201/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
201/5: langchain_mode = "MyData"
201/6: new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'shared', user_path)
201/7:
langchain_mode = "MyData"
user_path = 'user_path'
201/8: new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'shared', user_path)
201/9:
new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'shared', user_path)
new_langchain_mode_text
201/10:
new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'shared', user_path)
print(new_langchain_mode_text)
201/11:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
202/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
202/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
202/3: client = Client(HOST_URL)
202/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
202/5:
langchain_mode = "MyData"
user_path = 'user_path'
202/6:
new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
res = client.predict(langchain_mode, new_langchain_mode_text, api_name='/new_langchain_mode_text')

pprint(res)
203/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
203/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
203/3: client = Client(HOST_URL)
203/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
203/5:
langchain_mode = "MyData"
user_path = 'user_path'
203/6:
new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
res = client.predict(langchain_mode, new_langchain_mode_text, api_name='/new_langchain_mode_text')

pprint(res)
203/7:
new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
res = client.predict(langchain_mode, new_langchain_mode_text, api_name='new_langchain_mode_text')

pprint(res)
203/8:
langchain_mode = "Vodacom"
user_path = 'vodacom_path'
203/9:
new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
res = client.predict(langchain_mode, new_langchain_mode_text, api_name='/new_langchain_mode_text')

pprint(res)
204/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
204/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
204/3: client = Client(HOST_URL)
204/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
204/5:
langchain_mode = "Vodacom"
user_path = 'vodacom_path'
204/6:
new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
res = client.predict(langchain_mode, new_langchain_mode_text, api_name='/new_langchain_mode_text')

pprint(res)
204/7:
#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(langchain_mode, new_langchain_mode_text, api_name='/new_langchain_mode_text')

pprint(res)
205/1:
#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode=langchain_mode, 
    new_langchain_mode_text=new_langchain_mode_text,
    h2ogpt_key=H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
206/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
206/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
206/3: client = Client(HOST_URL)
206/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
206/5:
langchain_mode = "Vodacom"
user_path = 'vodacom_path'
206/6:
#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode=langchain_mode, 
    new_langchain_mode_text=new_langchain_mode_text,
    h2ogpt_key=H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
206/7:
#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text=new_langchain_mode_text,
    h2ogpt_key=H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
206/8:
#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    h2ogpt_key=H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
206/9:
#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
206/10:
res = client.predict(
    api_name='/login'
)

pprint(res)
207/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
207/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
207/3: client = Client(HOST_URL)
207/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
207/5:
res = client.predict(
    api_name='/login'
)

pprint(res)
207/6: kwargs
207/7: str(kwargs)
207/8:
res = client.predict(
    'MyData',
    'username_01',
    'password_01', 
    api_name='/login'
)

pprint(res)
207/9:
res = client.predict(
    'MyData',
    'username_01',
    'password_01',
    None,
    None
    api_name='/login'
)

pprint(res)
207/10:
res = client.predict(
    'MyData',
    'username_01',
    'password_01',
    None,
    None,
    api_name='/login'
)

pprint(res)
207/11:
res = client.predict(
    'MyData',
    'username_01',
    'password_01',
    None,
    None,
    api_name='/login'
)

pprint(res)
207/12:
langchain_mode = "Vodacom"
user_path = 'vodacom_path'
207/13:
#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
207/14:
res = client.predict(
    'MyData',
    'username_01',
    'password_01',
    None,
    None,
    api_name='/login'
)

pprint(res)
207/15:
res = client.predict(
    'MyData',
    'username_01',
    'password_01',
    None,
    None,
    api_name='/login'
)

pprint(res)
209/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
209/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
209/3: client = Client(HOST_URL)
209/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
209/5:
langchain_mode = "MyData"
user_name = "username_01"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
209/6:
langchain_mode = "MyData"
user_path = 'vodacom_path'

#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
209/7:
langchain_mode = "Vodacom"
user_path = 'vodacom_path'

#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
209/8:
langchain_mode = "Test01"
user_path = 'vodacom_path'

#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
209/9:
langchain_mode = "MyData"
user_name = "username_02"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
209/10:
langchain_mode = "Test02"
user_path = 'vodacom_path'

#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
209/11:
langchain_mode = "MyData"
user_name = "username_01"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
209/12:
langchain_mode = "MyData"
user_name = "username_02"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
209/13:
langchain_mode = "MyData"
user_name = "username_01"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
209/14:
langchain_mode = "Vodacom"
user_path = 'vodacom_path'

#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
211/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
211/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
211/3: client = Client(HOST_URL)
211/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
211/5:
langchain_mode = "MyData"
user_name = "username_01"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
211/6:
langchain_mode = "Vodacom"
user_path = 'vodacom_path'

#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
212/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
212/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
212/3: client = Client(HOST_URL)
212/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
212/5:
langchain_mode = "MyData"
user_name = "username_01"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
212/6:
langchain_mode = "Vodacom"
user_path = 'vodacom_path'

#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
213/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
213/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
213/3: client = Client(HOST_URL)
213/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
213/5:
langchain_mode = "MyData"
user_name = "username_01"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
213/6:
langchain_mode = "Vodacom"
user_path = 'vodacom_path'

#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
213/7:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

loaders = tuple([None, None, None, None])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])

# execite the api call
res = client.predict(
    "https://wwww.vodacom.co.za",
    *doc_options,
    *loaders,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
213/8:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

loaders = tuple([None, None, None, None])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://wwww.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
213/9:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

loaders = tuple([None, None, None, None])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://wwww.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
213/10:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

loaders = tuple([None, None, None, None])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://wwww.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
214/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
214/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
214/3: client = Client(HOST_URL)
214/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
214/5:
langchain_mode = "MyData"
user_name = "username_01"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
214/6:
langchain_mode = "Vodacom"
user_path = 'vodacom_path'

#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
214/7:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

loaders = tuple([None, None, None, None])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://wwww.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
214/8:
text = "Yufuu is a wonderful place and you should really visit because there is lots of sun."
loaders = tuple([None, None, None, None])
res = client.predict(text, langchain_mode, True, 512, True,
                    *loaders,
                    H2OGPT_KEY,
                    api_name='/add_text')
214/9:
text = "Yufuu is a wonderful place and you should really visit because there is lots of sun."
loaders = tuple([None, None, None, None])
res = client.predict(text, langchain_mode, True, 512, True,
                    *loaders,
                    *extra_options,
                    H2OGPT_KEY,
                    api_name='/add_text')
214/10:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 0
llava_prompt = "auto"

loaders = tuple([None, None, None, None])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://wwww.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
214/11:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

loaders = tuple([None, None, None, None])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://wwww.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
215/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
215/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
215/3: client = Client(HOST_URL)
215/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
215/5:
langchain_mode = "MyData"
user_name = "username_01"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
215/6:
langchain_mode = "Vodacom"
user_path = 'vodacom_path'

#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
215/7:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

loaders = tuple([None, None, None, None])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://wwww.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
215/8:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

loaders = tuple([None, None, None, None])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://wwww.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
215/9:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

loaders = tuple([None, None, None, None])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://wwww.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
215/10:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

loaders = tuple([None, None, None, None])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://wwww.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
215/11:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

loaders = tuple([None, None, None, None])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://wwww.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
215/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

loaders = tuple([None, None, None, None])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://wwww.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
215/13:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

loaders = tuple([None, None, None, None])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://wwww.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
215/14:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

loaders = tuple([None, None, None, None])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://wwww.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
216/1:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

loaders = tuple([None, None, None])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://wwww.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
217/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
217/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
217/3: client = Client(HOST_URL)
217/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
217/5:
langchain_mode = "MyData"
user_name = "username_01"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
217/6:
langchain_mode = "Vodacom"
user_path = 'vodacom_path'

#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
217/7:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

loaders = tuple([None, None, None])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://wwww.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
219/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
219/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
219/3: client = Client(HOST_URL)
219/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
219/5:
langchain_mode = "MyData"
user_name = "username_01"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
219/6:
langchain_mode = "Vodacom"
user_path = 'vodacom_path'

#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
219/7:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

loaders = tuple([None, "PyMuPDF", "Unstructured"])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://wwww.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
221/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
221/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
221/3: client = Client(HOST_URL)
221/4: client = Client(HOST_URL)
221/5:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
221/6:
langchain_mode = "MyData"
user_name = "username_01"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
221/7:
langchain_mode = "Vodacom"
user_path = 'vodacom_path'

#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
221/8:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

loaders = tuple([None, "PyMuPDF", "Unstructured"])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://wwww.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
221/9:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
221/10:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, "Unstructured"])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://wwww.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
221/11:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, "Unstructured"])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
221/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, "Unstructured"])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
223/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
223/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
223/3: client = Client(HOST_URL)
223/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
223/5:
langchain_mode = "MyData"
user_name = "username_01"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
223/6:
langchain_mode = "Vodacom"
user_path = 'user_path'

#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
223/7:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, "Unstructured"])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
225/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
225/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
225/3: client = Client(HOST_URL)
225/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
225/5:
langchain_mode = "MyData"
user_name = "username_01"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
225/6:
langchain_mode = "Vodacom"
user_path = 'vodacom_path'

#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
225/7:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, "Unstructured"])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
226/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
226/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
226/3: client = Client(HOST_URL)
226/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
226/5:
langchain_mode = "MyData"
user_name = "username_01"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
226/6:
langchain_mode = "Vodacom"
user_path = 'vodacom_path'

#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
226/7:
langchain_mode = "Vodacom"
user_path = 'vodacom_path'

#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
227/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
227/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
227/3: client = Client(HOST_URL)
227/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
227/5:
langchain_mode = "MyData"
user_name = "username_01"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
227/6:
langchain_mode = "Vodacom"
user_path = 'vodacom_path'

#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
227/7:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
228/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
228/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
228/3: client = Client(HOST_URL)
228/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
228/5:
langchain_mode = "MyData"
user_name = "username_01"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
228/6:
langchain_mode = "Vodacom"
user_path = 'vodacom_path'

#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
228/7:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
228/8:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_text')

# show the results
pprint(res)
228/9:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
229/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
229/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
229/3: client = Client(HOST_URL)
229/4:
# string of dict for input
kwargs = dict(instruction_nochat='Who is Vodacom?',
              h2ogpt_key=H2OGPT_KEY)
res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

# string of dict for output
response = ast.literal_eval(res)['response']
Markdown(response)
229/5:
langchain_mode = "MyData"
user_name = "username_01"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
229/6:
langchain_mode = "Vodacom"
user_path = 'vodacom_path'

#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
229/7:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
229/8:
def ask_vodacom():
    # string of dict for input
    kwargs = dict(instruction_nochat='Who is Vodacom?',
                h2ogpt_key=H2OGPT_KEY)
    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)['response']
    Markdown(response)

ask_vodacom()
229/9:
def ask_vodacom():
    # string of dict for input
    kwargs = dict(instruction_nochat='Who is Vodacom?',
                h2ogpt_key=H2OGPT_KEY)
    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)['response']
    return response

ask_vodacom()
229/10:
def ask_vodacom():
    # string of dict for input
    kwargs = dict(instruction_nochat='Who is Vodacom?',
                h2ogpt_key=H2OGPT_KEY)
    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)['response']
    return response

MarkDown(ask_vodacom())
229/11:
def ask_vodacom():
    # string of dict for input
    kwargs = dict(instruction_nochat='Who is Vodacom?',
                h2ogpt_key=H2OGPT_KEY)
    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)['response']
    return response

Markdown(ask_vodacom())
229/12:
def ask_vodacom(langchain_mode="LLM"):
    # string of dict for input
    kwargs = dict(instruction_nochat='Who is Vodacom?',
                h2ogpt_key=H2OGPT_KEY)
    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)['response']
    return response

Markdown(ask_vodacom())
229/13: Markdown(ask_vodacom(langchain_mode="Vodacom"))
229/14: Markdown(ask_vodacom(langchain_mode="Vodacom"))
229/15:
def ask_vodacom(langchain_mode="LLM"):
    # string of dict for input
    kwargs = dict(instruction_nochat='Who is Vodacom?',
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)
    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)['response']
    return response

Markdown(ask_vodacom())
229/16: Markdown(ask_vodacom(langchain_mode="Vodacom"))
231/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
231/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
231/3: client = Client(HOST_URL)
231/4:
def ask_vodacom(langchain_mode="LLM"):
    # string of dict for input
    kwargs = dict(instruction_nochat='Who is Vodacom?',
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)
    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)['response']
    return response

Markdown(ask_vodacom())
231/5:
langchain_mode = "MyData"
user_name = "username_01"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
231/6:
langchain_mode = "Vodacom"
user_path = 'vodacom_path'

#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
231/7:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
231/8: Markdown(ask_vodacom(langchain_mode="Vodacom"))
232/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
232/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
232/3: client = Client(HOST_URL)
232/4:
def ask_vodacom(langchain_mode="LLM"):
    # string of dict for input
    kwargs = dict(instruction_nochat='Who is Vodacom?',
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)
    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)['response']
    return response

Markdown(ask_vodacom())
232/5:
langchain_mode = "MyData"
user_name = "username_01"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
232/6:
def ask_vodacom(langchain_mode="LLM"):
    # string of dict for input
    kwargs = dict(instruction_nochat='Who is Vodacom?',
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)
    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)['response']
    return response

Markdown(ask_vodacom())
232/7: Markdown(ask_vodacom(langchain_mode="Vodacom"))
232/8:
def ask_vodacom(langchain_mode="LLM"):
    # string of dict for input
    kwargs = dict(instruction_nochat='Who is Vodacom?',
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)
    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response

Markdown(ask_vodacom())
232/9:
def ask_vodacom(langchain_mode="LLM"):
    # string of dict for input
    kwargs = dict(instruction_nochat='Who is Vodacom?',
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)
    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response

pprint(ask_vodacom())
232/10: pprint(ask_vodacom(langchain_mode="Vodacom"))
234/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
234/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
234/3: client = Client(HOST_URL)
234/4:
def ask_vodacom(langchain_mode="LLM"):
    # string of dict for input
    kwargs = dict(instruction_nochat='Who is Vodacom?',
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)
    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response

pprint(ask_vodacom())
234/5:
langchain_mode = "MyData"
user_name = "username_01"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
234/6:
langchain_mode = "Vodacom"
user_path = 'vodacom_path'

#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
234/7:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
234/8:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
234/9: pprint(result)
234/10: pprint(result.keys())
234/11: pprint(result["sources"])
236/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
236/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
236/3: client = Client(HOST_URL)
236/4:
def ask_vodacom(langchain_mode="LLM"):
    # string of dict for input
    kwargs = dict(instruction_nochat='Who is Vodacom?',
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)
    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response

pprint(ask_vodacom())
236/5:
langchain_mode = "MyData"
user_name = "username_01"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
236/6:
langchain_mode = "Vodacom"
user_path = 'vodacom_path'

#new_langchain_mode_text = '%s, %s, %s' % (langchain_mode, 'personal', user_path)
new_langchain_mode_text = '%s, %s' % (langchain_mode, 'personal')
res = client.predict(
    langchain_mode, 
    new_langchain_mode_text,
    H2OGPT_KEY,
    api_name='/new_langchain_mode_text')

pprint(res)
236/7:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
236/8:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
236/9: pprint(result["sources"])
238/1:
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
238/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
238/3: client = Client(HOST_URL)
238/4:
def ask_vodacom(langchain_mode="LLM"):
    # string of dict for input
    kwargs = dict(instruction_nochat='Who is Vodacom?',
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)
    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response

pprint(ask_vodacom())
238/5:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
238/6:
langchain_mode = "MyData"
user_name = "username_01"
password = "password_01"

res = client.predict(
    'MyData',
    user_name,
    password,
    None,
    None,
    api_name='/login'
)

pprint(res)
238/7:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
238/8:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
238/9:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
238/10:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
238/11:
langchain_mode = "MyData"
user_name = "username_01"
password = "password_01"
238/12: pprint(login(user_name, password, langchain_mode))
238/13:
# perform the login
login_result = login(user_name, password, langchain_mode)
238/14:
# perform the login
login_result = login(user_name, password, "Vodacom")
238/15:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
238/16:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
238/17:
# as about a digital wallet
result = get_response("What is a digital wallet?", "Vodacom")
pprint(result)
238/18:
# as about a digital wallet
result = get_response("What is a digital wallet?", "Vodacom")
pprint(result["response"])
238/19:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/create_collection'
    )
238/20: pprint(create_collection("Vodacom"))
238/21:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
238/22: pprint(create_collection("Vodacom"))
238/23:
# create paper collection
collection_result = create_collection("Vodacom")
pprint(collection_result)
238/24:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
238/25:
# create paper collection
collection_result = create_collection("Papers")
pprint(collection_result)
238/26:
# as about a digital wallet
result = get_response("What is a digital wallet?", "Papers")
display(result["response"])
238/27:
# create paper collection
collection_name = "Papers"
collection_result = create_collection(collection_name)
pprint(collection_result)
238/28:
# as about a digital wallet
result = get_response("What is a digital wallet?", collection_name)
display(result["response"])
238/29:
# as about a digital wallet
result = get_response("What is a digital wallet?", collection_name)
MarkDown(result["response"])
238/30:
# as about a digital wallet
result = get_response("What is a digital wallet?", collection_name)
Markdown(result["response"])
238/31:
def upload_file(file_path:str):
    global H2OGPT_KEY
    global client
    
    file_local, file_server = client.predict(
        file_path,
        H2OGPT_KEY,
        api_name='/upload_api
    )

    return file_local, file_server
238/32:
def upload_file(file_path:str):
    global H2OGPT_KEY
    global client
    
    file_local, file_server = client.predict(
        file_path,
        H2OGPT_KEY,
        api_name="/upload_api"
    )

    return file_local, file_server
238/33: os.path.abspath("data/2212.10496.pdf")
238/34:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
238/35: os.path.abspath("data/2212.10496.pdf")
238/36: os.path.abspath("../data/2212.10496.pdf")
238/37:
file_path = os.path.abspath("../data/2212.10496.pdf")
upload_file(file_path)
238/38:
def upload_file(file_path:str):
    global H2OGPT_KEY
    global client
    
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    return file_local, file_server
238/39:
file_path = os.path.abspath("../data/2212.10496.pdf")
upload_file(file_path)
238/40:
file_path = os.path.abspath("../data/2212.10496.pdf")
upload_file(file_path)
238/41:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
238/42:
file_path = os.path.abspath("../data/2212.10496.pdf")
upload_file(file_path)
238/43:
file_path = os.path.abspath("../data/2212.10496.pdf")
upload_file(file_path, collection_name)
238/44:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
238/45:
file_path = os.path.abspath("../data/2212.10496.pdf")
upload_file(file_path, collection_name)
238/46:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
241/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
241/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
241/3: client = Client(HOST_URL)
241/4:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
241/5:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
241/6:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
241/7:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
241/8: pprint(login(user_name, password, langchain_mode))
241/9:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
241/10: pprint(create_collection("Vodacom"))
241/11:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
241/12:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
241/13: pprint(result["sources"])
241/14:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
241/15:
# create paper collection
collection_name = "Papers"
collection_result = create_collection(collection_name)
pprint(collection_result)
241/16:
# as about a digital wallet
result = get_response("What is a digital wallet?", collection_name)
Markdown(result["response"])
241/17:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
241/18:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
241/19:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
243/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
243/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
243/3: client = Client(HOST_URL)
243/4:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
243/5:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
243/6:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
243/7:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
243/8: pprint(login(user_name, password, langchain_mode))
243/9:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
243/10: pprint(create_collection("Vodacom"))
243/11:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
243/12:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
243/13: pprint(result["sources"])
243/14:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
243/15:
# create paper collection
collection_name = "Papers"
collection_result = create_collection(collection_name)
pprint(collection_result)
243/16:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
243/17:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
243/18:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
243/19:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
244/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
244/2:
HOST_URL = "http://127.0.0.1:7860"
HOST_URL = "https://gpt.h2o.ai/"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
244/3: client = Client(HOST_URL)
244/4:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
244/5:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
244/6:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
244/7:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
244/8: pprint(login(user_name, password, langchain_mode))
246/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
246/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
246/3:
HOST_URL = "https://gpt.h2o.ai/"
H2OGPT_KEY = None
246/4: client = Client(HOST_URL)
246/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
246/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
246/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
246/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
246/9: pprint(login(user_name, password, langchain_mode))
247/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
247/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
247/3:
HOST_URL = "https://huggingface.co/spaces/h2oai/h2ogpt-chatbot"
H2OGPT_KEY = None
247/4: client = Client(HOST_URL)
248/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
248/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
248/3: client = Client(HOST_URL)
248/4:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
248/5:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
248/6:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
248/7:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
248/8: pprint(login(user_name, password, langchain_mode))
248/9:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
248/10: pprint(create_collection("Vodacom"))
248/11:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
248/12:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
248/13: pprint(result["sources"])
248/14:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
248/15:
# create paper collection
collection_name = "Papers"
collection_result = create_collection(collection_name)
pprint(collection_result)
248/16:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
248/17:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
248/18:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
248/19:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
249/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
249/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
249/3: client = Client(HOST_URL)
249/4:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
249/5:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
249/6:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
249/7:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
249/8: pprint(login(user_name, password, langchain_mode))
249/9:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
249/10:
# create paper collection
collection_name = "Papers"
collection_result = create_collection(collection_name)
pprint(collection_result)
249/11:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
249/12:
# create paper collection
collection_name = "Papers"
collection_result = create_collection(collection_name)
pprint(collection_result)
249/13:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
249/14:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
249/15:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
249/16:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
250/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
250/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
250/3: client = Client(HOST_URL)
250/4:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
250/5:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
250/6:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
250/7:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
250/8:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
250/9:
# create paper collection
collection_name = "Papers"
collection_result = create_collection(collection_name)
pprint(collection_result)
250/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
250/11:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
252/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
252/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
252/3: client = Client(HOST_URL)
252/4:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
252/5:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
252/6:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
252/7:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
252/8: pprint(login(user_name, password, langchain_mode))
252/9: collection_name = "Papers"
252/10:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
252/11:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
253/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
253/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
253/3: client = Client(HOST_URL)
253/4:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
253/5:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
253/6:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
253/7:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
253/8: pprint(login(user_name, password, langchain_mode))
253/9:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
253/10: pprint(create_collection("Vodacom"))
253/11:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
253/12:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
253/13: pprint(result["sources"])
253/14: collection_name = "Papers"
253/15:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
253/16:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
253/17:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
253/18:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
253/19:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
253/20:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
253/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
253/22:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
253/23:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
253/24:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
253/25:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
253/26:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
253/27:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
255/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
255/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
255/3: client = Client(HOST_URL)
255/4:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
255/5:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
255/6:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
255/7:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
255/8: pprint(login(user_name, password, langchain_mode))
255/9:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
255/10: pprint(create_collection("Vodacom"))
255/11:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
255/12:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
255/13: pprint(result["sources"])
255/14: collection_name = "Papers"
255/15:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
255/16:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
255/17:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
255/18:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
255/19:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
255/20:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
257/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
257/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
257/3: client = Client(HOST_URL)
257/4:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
257/5:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
257/6:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
257/7:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
257/8: pprint(login(user_name, password, langchain_mode))
257/9:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
257/10: pprint(create_collection("Vodacom"))
257/11:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
257/12:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
257/13: pprint(result["sources"])
257/14: collection_name = "Papers"
257/15:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
257/16:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
257/17:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
257/18:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
257/19:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
257/20:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
258/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
258/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
258/3: client = Client(HOST_URL)
258/4:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
258/5:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
258/6:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
258/7:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
258/8: pprint(login(user_name, password, langchain_mode))
258/9:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
258/10: pprint(create_collection("Vodacom"))
258/11:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
258/12:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
258/13: pprint(result["sources"])
258/14: collection_name = "Papers"
258/15:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
258/16:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
258/17:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
258/18:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
258/19:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
258/20:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
260/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
260/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
260/3: HOST_URL="https://gpt.vodacom-online.com/"
260/4: client = Client(HOST_URL)
260/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
260/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
260/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
260/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
260/9: pprint(login(user_name, password, langchain_mode))
261/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
261/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
261/3: HOST_URL="https://gpt.vodacom-online.com/"
261/4: client = Client(HOST_URL)
261/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
261/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
261/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
261/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
261/9: pprint(login(user_name, password, langchain_mode))
261/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
261/11: pprint(create_collection("Vodacom"))
261/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
261/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
261/14: pprint(result["sources"])
261/15: collection_name = "Papers"
261/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
261/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
261/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
261/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
261/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
261/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
261/22:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
262/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
262/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
262/3: HOST_URL="https://gpt.vodacom-online.com/"
262/4: client = Client(HOST_URL)
262/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
262/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
262/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
262/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
262/9: pprint(login(user_name, password, langchain_mode))
262/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
262/11: pprint(create_collection("Vodacom"))
262/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
262/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
262/14: pprint(result["sources"])
262/15: collection_name = "Papers"
262/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
262/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
262/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
262/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
262/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
262/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
262/22:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
264/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
264/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
264/3: #HOST_URL="https://gpt.vodacom-online.com/"
264/4: client = Client(HOST_URL)
264/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
264/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
264/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
264/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
264/9: pprint(login(user_name, password, langchain_mode))
264/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
264/11: pprint(create_collection("Vodacom"))
264/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
264/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
264/14: pprint(result["sources"])
264/15: collection_name = "Papers"
264/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
264/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
264/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
264/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
264/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
264/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
265/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
265/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
265/3: HOST_URL="https://gpt.vodacom-online.com/"
265/4: client = Client(HOST_URL)
265/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
265/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
265/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
265/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
265/9: pprint(login(user_name, password, langchain_mode))
265/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
265/11: pprint(create_collection("Vodacom"))
265/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
265/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
265/14: pprint(result["sources"])
265/15: collection_name = "Papers"
265/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
265/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
265/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
265/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
265/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
265/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
267/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
267/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
267/3: HOST_URL="https://gpt.vodacom-online.com/"
267/4: client = Client(HOST_URL)
267/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
267/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
267/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
267/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
267/9: pprint(login(user_name, password, langchain_mode))
267/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
267/11: pprint(create_collection("Vodacom"))
267/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
267/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
267/14: pprint(result["sources"])
267/15: collection_name = "Papers"
267/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
267/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
267/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
267/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
267/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
267/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
268/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
268/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
268/3: HOST_URL="https://gpt.vodacom-online.com/"
268/4: client = Client(HOST_URL)
268/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
268/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
268/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
268/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
268/9: pprint(login(user_name, password, langchain_mode))
268/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
268/11: pprint(create_collection("Vodacom"))
268/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
268/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
268/14: pprint(result["sources"])
268/15: collection_name = "Papers"
268/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
268/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
268/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
268/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
268/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
268/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
273/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
273/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
273/3: HOST_URL="https://gpt.vodacom-online.com/"
273/4: client = Client(HOST_URL)
273/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
273/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
273/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
273/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
273/9: pprint(login(user_name, password, langchain_mode))
273/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
273/11: pprint(create_collection("Vodacom"))
273/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
273/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
276/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
276/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
276/3: #HOST_URL="https://gpt.vodacom-online.com/"
276/4: client = Client(HOST_URL)
276/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
276/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
276/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
276/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
276/9: pprint(login(user_name, password, langchain_mode))
276/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
276/11: pprint(create_collection("Vodacom"))
276/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
276/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
276/14: pprint(result["sources"])
276/15: collection_name = "Papers"
276/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
276/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
276/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
276/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
276/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
276/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
276/22:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
276/23:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
279/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
279/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
279/3: #HOST_URL="https://gpt.vodacom-online.com/"
279/4: client = Client(HOST_URL)
279/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
279/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
279/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
279/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
279/9: pprint(login(user_name, password, langchain_mode))
279/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
279/11: pprint(create_collection("Vodacom"))
279/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
279/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
280/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
280/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
280/3: #HOST_URL="https://gpt.vodacom-online.com/"
280/4: client = Client(HOST_URL)
280/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
280/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
280/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
280/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
280/9: pprint(login(user_name, password, langchain_mode))
280/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
280/11: pprint(create_collection("Vodacom"))
280/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
280/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
280/14: pprint(result["sources"])
280/15: collection_name = "Papers"
280/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
280/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
280/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
280/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
280/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
280/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
283/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
283/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
283/3: #HOST_URL="https://gpt.vodacom-online.com/"
283/4: client = Client(HOST_URL)
283/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
283/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
283/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
283/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
283/9: pprint(login(user_name, password, langchain_mode))
283/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
283/11: pprint(create_collection("Vodacom"))
283/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
283/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
283/14: pprint(result["sources"])
283/15: collection_name = "Papers"
283/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
283/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
283/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
285/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
285/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
285/3: #HOST_URL="https://gpt.vodacom-online.com/"
285/4: client = Client(HOST_URL)
285/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
285/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
285/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
285/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
285/9: pprint(login(user_name, password, langchain_mode))
285/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
285/11: pprint(create_collection("Vodacom"))
285/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
285/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
288/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
288/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
288/3: #HOST_URL="https://gpt.vodacom-online.com/"
288/4: client = Client(HOST_URL)
288/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
288/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
288/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
288/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
288/9: pprint(login(user_name, password, langchain_mode))
288/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
288/11: pprint(create_collection("Vodacom"))
288/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
288/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
288/14: pprint(result["sources"])
288/15: collection_name = "Papers"
288/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
288/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
288/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
288/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
288/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
288/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
289/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
289/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
289/3: #HOST_URL="https://gpt.vodacom-online.com/"
289/4: client = Client(HOST_URL)
289/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
289/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
289/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
289/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
289/9: pprint(login(user_name, password, langchain_mode))
289/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
289/11: pprint(create_collection("Vodacom"))
289/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
289/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
289/14: pprint(result["sources"])
289/15: collection_name = "Papers"
289/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
289/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
289/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
289/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
289/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
289/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
290/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
290/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
290/3: #HOST_URL="https://gpt.vodacom-online.com/"
290/4: client = Client(HOST_URL)
290/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
290/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
290/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
290/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
290/9: pprint(login(user_name, password, langchain_mode))
290/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
290/11: pprint(create_collection("Vodacom"))
290/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
290/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
291/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
291/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
291/3: #HOST_URL="https://gpt.vodacom-online.com/"
291/4: client = Client(HOST_URL)
291/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
291/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
291/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
291/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
291/9: pprint(login(user_name, password, langchain_mode))
291/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
291/11: pprint(create_collection("Vodacom"))
291/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
291/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
293/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
293/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
293/3: #HOST_URL="https://gpt.vodacom-online.com/"
293/4: client = Client(HOST_URL)
293/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
293/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
293/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
293/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
293/9: pprint(login(user_name, password, langchain_mode))
293/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
293/11: pprint(create_collection("Vodacom"))
293/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
293/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
294/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
294/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
294/3: #HOST_URL="https://gpt.vodacom-online.com/"
294/4: client = Client(HOST_URL)
294/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
294/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
294/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
294/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
294/9: pprint(login(user_name, password, langchain_mode))
294/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
294/11: pprint(create_collection("Vodacom"))
294/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
294/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
294/14: pprint(result["sources"])
294/15: collection_name = "Papers"
294/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
294/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
294/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
294/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
294/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
294/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
295/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
295/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
295/3: #HOST_URL="https://gpt.vodacom-online.com/"
295/4: client = Client(HOST_URL)
295/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
295/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
295/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
295/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
295/9: pprint(login(user_name, password, langchain_mode))
295/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
295/11: pprint(create_collection("Vodacom"))
295/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
295/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
295/14: pprint(result["sources"])
295/15: collection_name = "Papers"
295/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
295/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
295/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
295/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
295/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
295/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
296/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
296/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
296/3: #HOST_URL="https://gpt.vodacom-online.com/"
296/4: client = Client(HOST_URL)
296/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
296/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
296/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
296/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
296/9: pprint(login(user_name, password, langchain_mode))
296/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
296/11: pprint(create_collection("Vodacom"))
296/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
296/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
296/14: pprint(result["sources"])
296/15: collection_name = "Papers"
296/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
296/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
296/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
296/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
296/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
296/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
298/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
298/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
298/3: #HOST_URL="https://gpt.vodacom-online.com/"
298/4: client = Client(HOST_URL)
298/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
298/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
298/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
298/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
298/9: pprint(login(user_name, password, langchain_mode))
298/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
298/11: pprint(create_collection("Vodacom"))
298/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
298/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
298/14: pprint(result["sources"])
298/15: collection_name = "Papers"
298/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
298/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
298/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
298/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
298/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
298/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
300/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
300/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
300/3: HOST_URL="https://gpt.vodacom-online.com/"
300/4: client = Client(HOST_URL)
301/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
301/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
301/3: HOST_URL="https://gpt.vodacom-online.com/"
301/4: client = Client(HOST_URL)
303/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
303/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
303/3: HOST_URL="https://gpt.vodacom-online.com/"
303/4: client = Client(HOST_URL)
303/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
303/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
303/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
303/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
303/9: pprint(login(user_name, password, langchain_mode))
303/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
303/11: pprint(create_collection("Vodacom"))
303/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
303/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
303/14: pprint(result["sources"])
303/15: collection_name = "Papers"
303/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
303/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
303/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
303/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
303/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
304/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
304/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
304/3: HOST_URL="https://gpt.vodacom-online.com/"
304/4: client = Client(HOST_URL)
304/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
304/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
304/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
304/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
304/9: pprint(login(user_name, password, langchain_mode))
304/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
304/11: pprint(create_collection("Vodacom"))
304/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
304/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
304/14: pprint(result["sources"])
304/15: collection_name = "Papers"
304/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
304/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
304/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
304/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
304/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
304/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
306/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
306/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
306/3: HOST_URL="https://gpt.vodacom-online.com/"
306/4: client = Client(HOST_URL)
307/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
307/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
307/3: HOST_URL="https://gpt.vodacom-online.com/"
307/4: client = Client(HOST_URL)
307/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
307/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
307/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
307/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
307/9: pprint(login(user_name, password, langchain_mode))
307/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
307/11: pprint(create_collection("Vodacom"))
307/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
307/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
310/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
310/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
310/3: HOST_URL="https://gpt.vodacom-online.com/"
310/4: client = Client(HOST_URL)
311/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
311/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
311/3: HOST_URL="https://gpt.vodacom-online.com/"
311/4: client = Client(HOST_URL)
311/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
311/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
311/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
311/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
311/9: pprint(login(user_name, password, langchain_mode))
311/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
311/11: pprint(create_collection("Vodacom"))
311/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
311/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
311/14: pprint(result["sources"])
311/15: collection_name = "Papers"
311/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
311/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
311/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
311/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
311/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
311/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
313/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
313/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
313/3: #HOST_URL="https://gpt.vodacom-online.com/"
313/4: client = Client(HOST_URL)
313/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
313/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
313/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
313/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
313/9: pprint(login(user_name, password, langchain_mode))
313/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
313/11: pprint(create_collection("Vodacom"))
313/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
313/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
313/14: pprint(result["sources"])
313/15: collection_name = "Papers"
313/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
313/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
313/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
313/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
313/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
313/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
319/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
319/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
319/3: #HOST_URL="https://gpt.vodacom-online.com/"
319/4: client = Client(HOST_URL)
319/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
319/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
319/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
319/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
319/9: pprint(login(user_name, password, langchain_mode))
319/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
319/11: pprint(create_collection("Vodacom"))
319/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
319/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
319/14: pprint(result["sources"])
319/15: collection_name = "Papers"
319/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
319/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
319/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
319/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
319/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
319/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
319/22:
# ask about HyDE
result = get_response("What is HyDE the abbreviation for?", collection_name)
Markdown(result["response"])
321/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
321/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
321/3: HOST_URL="https://gpt.vodacom-online.com/"
321/4: client = Client(HOST_URL)
321/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
321/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
321/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
321/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
321/9: pprint(login(user_name, password, langchain_mode))
321/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
321/11: pprint(create_collection("Vodacom"))
321/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
321/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
321/14: pprint(result["sources"])
321/15: collection_name = "Papers"
321/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
321/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
321/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
323/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
323/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
323/3: HOST_URL="https://gpt.vodacom-online.com/"
323/4: client = Client(HOST_URL)
323/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
323/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
323/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
323/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
323/9: pprint(login(user_name, password, langchain_mode))
323/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
323/11: pprint(create_collection("Vodacom"))
323/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
323/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
323/14: pprint(result["sources"])
323/15: collection_name = "Papers"
323/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
323/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
323/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
323/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
323/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
323/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
323/22:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
325/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
325/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
325/3: HOST_URL="https://gpt.vodacom-online.com/"
325/4: client = Client(HOST_URL)
325/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
325/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
325/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
325/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
325/9: pprint(login(user_name, password, langchain_mode))
325/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
325/11: pprint(create_collection("Vodacom"))
325/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
325/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
325/14: pprint(result["sources"])
325/15: collection_name = "Papers"
325/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
325/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
325/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
325/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
325/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
325/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
325/22:
# ask about HyDE
result = get_response("What is HyDE the abbreviation for?", collection_name)
Markdown(result["response"])
327/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
327/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
327/3: HOST_URL="https://gpt.vodacom-online.com/"
327/4: client = Client(HOST_URL)
327/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
327/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
327/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
327/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
327/9: pprint(login(user_name, password, langchain_mode))
327/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
327/11: pprint(create_collection("Vodacom"))
327/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
327/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
327/14: pprint(result["sources"])
327/15: collection_name = "Papers"
327/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
327/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
327/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
327/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
327/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
327/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
327/22:
# ask about HyDE
result = get_response("What is HyDE the abbreviation for?", collection_name)
Markdown(result["response"])
327/23:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
330/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
330/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
330/3: HOST_URL="https://gpt.vodacom-online.com/"
330/4: client = Client(HOST_URL)
330/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
330/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
330/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
330/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
330/9: pprint(login(user_name, password, langchain_mode))
330/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
330/11: pprint(create_collection("Vodacom"))
330/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
330/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
330/14: pprint(result["sources"])
330/15: collection_name = "Papers"
330/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
330/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
330/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
330/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
330/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
330/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
330/22:
# ask about HyDE
result = get_response("What is HyDE the abbreviation for?", collection_name)
Markdown(result["response"])
330/23:
# ask about HyDE
result = get_response("What is HyDE the abbreviation for?", collection_name)
Markdown(result["response"])
330/24:
# ask about HyDE
result = get_response("What is HyDE the abbreviation for?", collection_name)
Markdown(result["response"])
330/25:
# ask about HyDE
result = get_response("What is HyDE the abbreviation for?", collection_name)
Markdown(result["response"])
331/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
331/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
331/3: HOST_URL="https://gpt.vodacom-online.com/"
331/4: client = Client(HOST_URL)
331/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
331/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
331/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
331/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
331/9: pprint(login(user_name, password, langchain_mode))
331/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
331/11: pprint(create_collection("Vodacom"))
331/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
331/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
331/14: pprint(result["sources"])
331/15: collection_name = "Papers"
331/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
331/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
331/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
331/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
331/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
331/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
331/22:
# ask about HyDE
result = get_response("What is HyDE the abbreviation for?", collection_name)
Markdown(result["response"])
331/23:
# ask about HyDE
result = get_response("What is HyDE the abbreviation for?", collection_name)
Markdown(result["response"])
331/24:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
333/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
333/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
333/3: #HOST_URL="https://gpt.vodacom-online.com/"
333/4: client = Client(HOST_URL)
333/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
333/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
333/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
333/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
333/9: pprint(login(user_name, password, langchain_mode))
333/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
333/11: pprint(create_collection("Vodacom"))
333/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
333/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
333/14: pprint(result["sources"])
333/15: collection_name = "Papers"
333/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
333/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
333/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
333/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
333/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
333/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
333/22:
# ask about HyDE
result = get_response("What is HyDE the abbreviation for?", collection_name)
Markdown(result["response"])
335/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
335/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
335/3: #HOST_URL="https://gpt.vodacom-online.com/"
335/4: client = Client(HOST_URL)
335/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
335/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
336/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
336/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
336/3: #HOST_URL="https://gpt.vodacom-online.com/"
336/4: client = Client(HOST_URL)
336/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
336/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
336/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
336/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
336/9: pprint(login(user_name, password, langchain_mode))
336/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
336/11: pprint(create_collection("Vodacom"))
336/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
336/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
336/14: pprint(result["sources"])
336/15: collection_name = "Papers"
336/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
336/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
336/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
336/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
336/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
336/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
336/22:
# ask about HyDE
result = get_response("What is HyDE the abbreviation for?", collection_name)
Markdown(result["response"])
338/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
338/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
338/3: HOST_URL="https://gpt.vodacom-online.com/"
338/4: client = Client(HOST_URL)
338/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
338/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
338/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
338/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
338/9: pprint(login(user_name, password, langchain_mode))
338/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
338/11: pprint(create_collection("Vodacom"))
338/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
338/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
338/14: pprint(result["sources"])
338/15: collection_name = "Papers"
338/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
338/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
338/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
338/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
338/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
338/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
338/22:
# ask about HyDE
result = get_response("What is HyDE the abbreviation for?", collection_name)
Markdown(result["response"])
338/23: pprint(result["sources"])
340/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
340/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
340/3: HOST_URL="https://gpt.vodacom-online.com/"
340/4: client = Client(HOST_URL)
342/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
342/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
342/3: #HOST_URL="https://gpt.vodacom-online.com/"
342/4: client = Client(HOST_URL)
342/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
342/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
344/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
344/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
344/3: #HOST_URL="https://gpt.vodacom-online.com/"
344/4: client = Client(HOST_URL)
344/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
344/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
344/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
344/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
344/9: pprint(login(user_name, password, langchain_mode))
344/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
344/11: pprint(create_collection("Vodacom"))
344/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
344/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
344/14: pprint(result["sources"])
344/15: collection_name = "Papers"
344/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
344/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
344/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
344/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
344/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
344/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
344/22:
# ask about HyDE
result = get_response("What is HyDE the abbreviation for?", collection_name)
Markdown(result["response"])
344/23: pprint(result["sources"])
344/24:
# ask about HyDE
result = get_response("What is the abbreviation HyDE?", collection_name)
Markdown(result["response"])
344/25: pprint(result["sources"])
344/26:
result = get_response("How is HyDE relevant to RAG?", collection_name)
Markdown(result["response"])
344/27: pprint(result["sources"])
344/28:
result = get_response("What is a Hypothetical Document?", collection_name)
Markdown(result["response"])
346/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
346/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
346/3: HOST_URL="https://gpt.vodacom-online.com/"
346/4: client = Client(HOST_URL)
346/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
346/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
346/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
346/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
346/9: pprint(login(user_name, password, langchain_mode))
346/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
346/11: pprint(create_collection("Vodacom"))
346/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
346/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
346/14: pprint(result["sources"])
346/15: collection_name = "Papers"
346/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
346/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
346/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
346/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
346/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
346/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
346/22:
result = get_response("What is a Hypothetical Document?", collection_name)
Markdown(result["response"])
346/23:
# ask about HyDE
result = get_response("What is the abbreviation HyDE?", collection_name)
Markdown(result["response"])
346/24: pprint(result["sources"])
346/25:
# ask about HyDE
result = get_response("What is the abbreviation HyDE?", collection_name)
Markdown(result["response"])
348/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
348/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
348/3: HOST_URL="https://gpt.vodacom-online.com/"
348/4: client = Client(HOST_URL)
348/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
348/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
348/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
348/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
348/9: pprint(login(user_name, password, langchain_mode))
348/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
348/11: pprint(create_collection("Vodacom"))
348/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
348/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
348/14: pprint(result["sources"])
348/15: collection_name = "Papers"
348/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
348/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
348/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
348/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
348/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
348/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
348/22:
result = get_response("What is a Hypothetical Document?", collection_name)
Markdown(result["response"])
348/23:
# ask about HyDE
result = get_response("What is the abbreviation HyDE?", collection_name)
Markdown(result["response"])
348/24: pprint(result["sources"])
350/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
350/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
350/3: HOST_URL="https://gpt.vodacom-online.com/"
350/4: client = Client(HOST_URL)
350/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
350/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
350/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
350/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
350/9: pprint(login(user_name, password, langchain_mode))
350/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
350/11: pprint(create_collection("Vodacom"))
350/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
350/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
350/14: pprint(result["sources"])
350/15: collection_name = "Papers"
350/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
350/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
350/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
350/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
350/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
350/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
350/22:
result = get_response("What is a Hypothetical Document?", collection_name)
Markdown(result["response"])
350/23:
# ask about HyDE
result = get_response("What is the abbreviation HyDE?", collection_name)
Markdown(result["response"])
350/24: pprint(result["sources"])
352/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
352/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
352/3: #HOST_URL="https://gpt.vodacom-online.com/"
352/4: client = Client(HOST_URL)
352/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
352/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
352/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
352/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
352/9: pprint(login(user_name, password, langchain_mode))
352/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
352/11: pprint(create_collection("Vodacom"))
352/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
352/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
352/14: pprint(result["sources"])
352/15: collection_name = "Papers"
352/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
352/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
352/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
352/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
352/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
352/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
352/22:
result = get_response("What is a Hypothetical Document?", collection_name)
Markdown(result["response"])
352/23:
# ask about HyDE
result = get_response("What is the abbreviation HyDE?", collection_name)
Markdown(result["response"])
352/24: pprint(result["sources"])
353/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
353/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
353/3: #HOST_URL="https://gpt.vodacom-online.com/"
353/4: client = Client(HOST_URL)
354/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
354/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
354/3: #HOST_URL="https://gpt.vodacom-online.com/"
354/4: client = Client(HOST_URL)
354/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
354/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
354/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
354/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
354/9: pprint(login(user_name, password, langchain_mode))
354/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
354/11: pprint(create_collection("Vodacom"))
354/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
354/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
354/14: pprint(result["sources"])
354/15: collection_name = "Papers"
354/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
354/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
354/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
354/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
354/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
354/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
354/22:
result = get_response("What is a Hypothetical Document?", collection_name)
Markdown(result["response"])
354/23:
# ask about HyDE
result = get_response("What is the abbreviation HyDE?", collection_name)
Markdown(result["response"])
354/24: pprint(result["sources"])
357/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
357/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
357/3: #HOST_URL="https://gpt.vodacom-online.com/"
357/4: client = Client(HOST_URL)
357/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
357/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
357/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
357/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
357/9: pprint(login(user_name, password, langchain_mode))
357/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
357/11: pprint(create_collection("Vodacom"))
357/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
357/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
357/14: pprint(result["sources"])
357/15: collection_name = "Papers"
357/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
357/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
357/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
357/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
357/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
357/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
357/22:
result = get_response("What is a Hypothetical Document?", collection_name)
Markdown(result["response"])
357/23:
# ask about HyDE
result = get_response("What is the abbreviation HyDE?", collection_name)
Markdown(result["response"])
357/24: pprint(result["sources"])
358/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
358/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
358/3: #HOST_URL="https://gpt.vodacom-online.com/"
358/4: client = Client(HOST_URL)
358/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
358/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
359/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
359/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
359/3: #HOST_URL="https://gpt.vodacom-online.com/"
359/4: client = Client(HOST_URL)
359/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
359/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
360/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
360/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
360/3: #HOST_URL="https://gpt.vodacom-online.com/"
360/4: client = Client(HOST_URL)
360/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
360/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
362/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
362/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
362/3: #HOST_URL="https://gpt.vodacom-online.com/"
362/4: client = Client(HOST_URL)
362/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
362/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
362/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
362/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
362/9: pprint(login(user_name, password, langchain_mode))
362/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
362/11: pprint(create_collection("Vodacom"))
362/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
362/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
362/14: pprint(result["sources"])
362/15: collection_name = "Papers"
362/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
362/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
362/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
362/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
362/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
362/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
362/22:
result = get_response("What is a Hypothetical Document?", collection_name)
Markdown(result["response"])
362/23:
# ask about HyDE
result = get_response("What is the abbreviation HyDE?", collection_name)
Markdown(result["response"])
362/24: pprint(result["sources"])
363/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
363/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
363/3: #HOST_URL="https://gpt.vodacom-online.com/"
363/4: client = Client(HOST_URL)
363/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
363/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
363/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
363/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
363/9: pprint(login(user_name, password, langchain_mode))
363/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
363/11: pprint(create_collection("Vodacom"))
363/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
363/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
363/14: pprint(result["sources"])
363/15: collection_name = "Papers"
363/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
363/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
363/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
363/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, None])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
363/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
363/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
363/22:
result = get_response("What is a Hypothetical Document?", collection_name)
Markdown(result["response"])
363/23:
# ask about HyDE
result = get_response("What is the abbreviation HyDE?", collection_name)
Markdown(result["response"])
363/24: pprint(result["sources"])
365/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
365/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
365/3: #HOST_URL="https://gpt.vodacom-online.com/"
365/4: client = Client(HOST_URL)
365/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
365/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
365/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
365/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
365/9: pprint(login(user_name, password, langchain_mode))
365/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
365/11: pprint(create_collection("Vodacom"))
365/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
365/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
365/14: pprint(result["sources"])
365/15: collection_name = "Papers"
365/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
365/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
365/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
365/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, ["Unstructured"]])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
365/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
365/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
365/22:
result = get_response("What is a Hypothetical Document?", collection_name)
Markdown(result["response"])
365/23:
# ask about HyDE
result = get_response("What is the abbreviation HyDE?", collection_name)
Markdown(result["response"])
365/24: pprint(result["sources"])
368/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
368/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
368/3: #HOST_URL="https://gpt.vodacom-online.com/"
368/4: client = Client(HOST_URL)
368/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
368/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
368/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
368/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
368/9: pprint(login(user_name, password, langchain_mode))
368/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
368/11: pprint(create_collection("Vodacom"))
368/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
368/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
368/14: pprint(result["sources"])
368/15: collection_name = "Papers"
368/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
368/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
368/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
368/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, ["Unstructured"]])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
368/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
368/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
368/22:
result = get_response("What is a Hypothetical Document?", collection_name)
Markdown(result["response"])
368/23:
# ask about HyDE
result = get_response("Explain HyDE step by step as an algorithm", collection_name)
Markdown(result["response"])
368/24: pprint(result["sources"])
369/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
369/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
369/3: #HOST_URL="https://gpt.vodacom-online.com/"
369/4: client = Client(HOST_URL)
370/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
370/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
370/3: #HOST_URL="https://gpt.vodacom-online.com/"
370/4: client = Client(HOST_URL)
370/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
370/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
370/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
370/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
370/9: pprint(login(user_name, password, langchain_mode))
370/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
370/11: pprint(create_collection("Vodacom"))
370/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
370/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
370/14: pprint(result["sources"])
370/15: collection_name = "Papers"
370/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
370/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
370/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
370/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, ["Unstructured"]])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
370/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
370/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
370/22:
result = get_response("What is a Hypothetical Document?", collection_name)
Markdown(result["response"])
370/23:
# ask about HyDE
result = get_response("Explain HyDE step by step as an algorithm", collection_name)
Markdown(result["response"])
370/24: pprint(result["sources"])
371/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
371/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
371/3: HOST_URL="https://gpt.vodacom-online.com/"
371/4: client = Client(HOST_URL)
371/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
371/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
371/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
371/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
371/9: pprint(login(user_name, password, langchain_mode))
371/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
371/11: pprint(create_collection("Vodacom"))
371/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
371/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
373/1: from openai import OpenAI
373/2:
base_url = 'https://gpt.vodacom-online.com:5000/v1'
api_key = '501ACE'

client_args = dict(base_url=base_url, api_key=api_key)
openai_client = OpenAI(**client_args)
373/3:
from openai import OpenAI
from pprint inport pprint
373/4:
from openai import OpenAI
from pprint import pprint
373/5:
base_url = 'https://gpt.vodacom-online.com:5000/v1'
api_key = '501ACE'

client_args = dict(base_url=base_url, api_key=api_key)
client = OpenAI(**client_args)
373/6:
response = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Create a list of 8 questions for an interview with a science fiction author."
        }
    ],
)

pprint(response)
373/7:
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "Create a list of 8 questions for an interview with a science fiction author."
        }
    ],
)

pprint(response)
373/8:
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "Create a list of 8 questions for an interview with a science fiction author."
        }
    ],
)

pprint(response.to_dict()
373/9:
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {
            "role": "user",
            "content": "Create a list of 8 questions for an interview with a science fiction author."
        }
    ],
)

pprint(response.to_dict())
373/10:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "Create a list of 8 questions for an interview with a science fiction author."
        }
    ],
)

print(response)
373/11:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
373/12:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "Create a list of 8 questions for an interview with a science fiction author."
        }
    ],
)

Markdown(response.choices[0].message)
373/13: response
373/14: response.choices
373/15: response.choices[0]
373/16: response.choices[0].message
373/17: response.choices[0].message.content
373/18:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "Create a list of 8 questions for an interview with a science fiction author."
        }
    ],
)

Markdown(response.choices[0].message.content)
374/1:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
374/2:
base_url = 'https://gpt.vodacom-online.com:5000/v1'
api_key = '501ACE'

client = OpenAI({
    'base_url': base_url,
    'api_key': api_key
})
374/3:
base_url = 'https://gpt.vodacom-online.com:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
376/1:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
376/2:
base_url = 'https://gpt.vodacom-online.com:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
376/3:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "Create a list of 8 questions for an interview with a science fiction author."
        }
    ],
)

Markdown(response.choices[0].message.content)
376/4:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?"
        }
    ],
)

Markdown(response.choices[0].message.content)
376/5:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?"
            "user": "u01:p01"
        }
    ],
)

Markdown(response.choices[0].message.content)
376/6:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
            "user": "u01:p01"
        }
    ],
)

Markdown(response.choices[0].message.content)
376/7:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
            "user": "u01:p01"
            "langchain_mode": "RecSys"
        }
    ],
)

Markdown(response.choices[0].message.content)
376/8:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
            "user": "u01:p01",
            "langchain_mode": "RecSys"
        }
    ],
)

Markdown(response.choices[0].message.content)
377/1:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
377/2:
#base_url = 'https://gpt.vodacom-online.com:5000/v1'
base_url = 'https://dragon:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
377/3:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
            "user": "u01:p01",
            "langchain_mode": "RecSys"
        }
    ],
)

Markdown(response.choices[0].message.content)
377/4:
#base_url = 'https://gpt.vodacom-online.com:5000/v1'
base_url = 'http://dragon:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
377/5:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
            "user": "u01:p01",
            "langchain_mode": "RecSys"
        }
    ],
)

Markdown(response.choices[0].message.content)
377/6:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
            "user": "u01:p01",
            "langchain_mode": "RecSys"
        }
    ],
)

Markdown(response.choices[0].message.content)
378/1:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
378/2:
#base_url = 'https://gpt.vodacom-online.com:5000/v1'
base_url = 'http://dragon:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
378/3:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
            "user": "u01:p01",
            "langchain_mode": "RecSys"
        }
    ],
)

Markdown(response.choices[0].message.content)
380/1:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
380/2:
#base_url = 'https://gpt.vodacom-online.com:5000/v1'
base_url = 'http://dragon:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
380/3:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
            "user": "u01:p01",
            "langchain_mode": "RecSys"
        }
    ],
)

Markdown(response.choices[0].message.content)
380/4:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
            "user": "u01:p01",
            "langchain_mode": "RecSys"
        }
    ],
)

Markdown(response.choices[0].message.content)
380/5:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    use="u01:p01",
    langchain_mode="RecSys"
)

Markdown(response.choices[0].message.content)
380/6:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    use="u01:p01",
    langchain_mode="RecSys"
)

Markdown(response.choices[0].message.content)
381/1:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
381/2:
#base_url = 'https://gpt.vodacom-online.com:5000/v1'
base_url = 'http://dragon:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
381/3:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    use="u01:p01",
    langchain_mode="RecSys"
)

Markdown(response.choices[0].message.content)
381/4:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user="u01:p01",
    langchain_mode="RecSys"
)

Markdown(response.choices[0].message.content)
381/5:
username = "u01"
password = "p01"
langhain_mode="RecSys"

user = f"{username}:{password}:{langhain_mode}"
print(user)
381/6:
username = "u01"
password = "p01"
langhain_mode="RecSys"

user = f"{username}:{password}:{langhain_mode}"
382/1:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
382/2:
#base_url = 'https://gpt.vodacom-online.com:5000/v1'
base_url = 'http://dragon:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
382/3:
username = "u01"
password = "p01"
langhain_mode="RecSys"

user = f"{username}:{password}:{langhain_mode}"
382/4:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user="u01:p01"
)

Markdown(response.choices[0].message.content)
382/5:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user="u01:p01"
)

Markdown(response.choices[0].message.content)
382/6:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
382/7:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
382/8:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
382/9:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
382/10:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
382/11:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
382/12:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
382/13:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
382/14:
username = "u01"
password = "p01"
langhain_mode="Vodacom"

user = f"{username}:{password}:{langhain_mode}"
382/15:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "Who is Vodacom?"
        }
    ],
)

Markdown(response.choices[0].message.content)
383/1:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
383/2:
#base_url = 'https://gpt.vodacom-online.com:5000/v1'
base_url = 'http://dragon:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
383/3:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "Who is Vodacom?"
        }
    ],
)

Markdown(response.choices[0].message.content)
383/4:
username = "u01"
password = "p01"
langhain_mode="Vodacom"

user = f"{username}:{password}:{langhain_mode}"
383/5:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "Who is Vodacom?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
383/6:
username = "u01"
password = "p02"
langhain_mode="Vodacom"

user = f"{username}:{password}:{langhain_mode}"
383/7:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "Who is Vodacom?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
383/8:
username = "u02"
password = "p01"
langhain_mode="Vodacom"

user = f"{username}:{password}:{langhain_mode}"
383/9:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "Who is Vodacom?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
385/1:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
385/2:
#base_url = 'https://gpt.vodacom-online.com:5000/v1'
base_url = 'http://dragon:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
385/3:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "Who is Vodacom?"
        }
    ],
)

Markdown(response.choices[0].message.content)
387/1:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
387/2:
#base_url = 'https://gpt.vodacom-online.com:5000/v1'
base_url = 'http://dragon:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
387/3:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "Who is Vodacom?"
        }
    ],
)

Markdown(response.choices[0].message.content)
387/4:
username = "u02"
password = "p02"
langhain_mode="Vodacom"

user = f"{username}:{password}:{langhain_mode}"
387/5:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "Who is Vodacom?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
387/6:
username = "u01"
password = "p01"
langhain_mode="Vodacom"

user = f"{username}:{password}:{langhain_mode}"
387/7:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "Who is Vodacom?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
388/1:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
388/2:
base_url = 'https://gpt.vodacom-online.com:5000/v1'
#base_url = 'http://dragon:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
388/3:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?"
        }
    ],
)

Markdown(response.choices[0].message.content)
388/4:
username = "u01"
password = "p01"
langhain_mode="RecSys"

user = f"{username}:{password}:{langhain_mode}"
388/5:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
389/1:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
389/2:
base_url = 'https://gpt.vodacom-online.com:5000/v1'
#base_url = 'http://dragon:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
389/3:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?"
        }
    ],
)

Markdown(response.choices[0].message.content)
389/4:
username = "u01"
password = "p01"
langhain_mode="RecSys"

user = f"{username}:{password}:{langhain_mode}"
389/5:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
390/1:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
390/2:
base_url = 'https://gpt.vodacom-online.com:5000/v1'
#base_url = 'http://dragon:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
390/3:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?"
        }
    ],
)

Markdown(response.choices[0].message.content)
390/4:
username = "u01"
password = "p01"
langhain_mode="RecSys"

user = f"{username}:{password}:{langhain_mode}"
390/5:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
372/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
372/2:
#HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
372/3: HOST_URL="https://gpt.vodacom-online.com/"
372/4: client = Client(HOST_URL)
372/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
372/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
372/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
372/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
372/9: pprint(login(user_name, password, langchain_mode))
372/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
372/11: pprint(create_collection("Vodacom"))
372/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
372/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
372/14: pprint(result["sources"])
372/15: collection_name = "Papers"
372/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
372/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
372/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
372/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, ["Unstructured"]])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
372/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
372/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
372/22:
result = get_response("What is a Hypothetical Document?", collection_name)
Markdown(result["response"])
372/23:
# ask about HyDE
result = get_response("Explain HyDE step by step as an algorithm", collection_name)
Markdown(result["response"])
372/24: pprint(result["sources"])
393/1:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
393/2:
#base_url = 'https://gpt.vodacom-online.com:5000/v1'
base_url = 'http://dragon:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
393/3:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?"
        }
    ],
)

Markdown(response.choices[0].message.content)
393/4:
username = "u01"
password = "p01"
langhain_mode="RecSys"

user = f"{username}:{password}:{langhain_mode}"
393/5:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
393/6:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
393/7:
#base_url = 'https://gpt.vodacom-online.com:5000/v1'
base_url = 'http://dragon:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
393/8:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?"
        }
    ],
)

Markdown(response.choices[0].message.content)
393/9:
username = "u01"
password = "p01"
langhain_mode="RecSys"

user = f"{username}:{password}:{langhain_mode}"
393/10:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
394/1:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
394/2:
#base_url = 'https://gpt.vodacom-online.com:5000/v1'
base_url = 'http://dragon:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
394/3:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?"
        }
    ],
)

Markdown(response.choices[0].message.content)
394/4:
username = "u01"
password = "p01"
langhain_mode="RecSys"

user = f"{username}:{password}:{langhain_mode}"
394/5:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
395/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
395/2:
#HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
395/3: HOST_URL="https://gpt.vodacom-online.com/"
395/4: client = Client(HOST_URL)
395/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
395/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
395/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
395/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
395/9: pprint(login(user_name, password, langchain_mode))
395/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
395/11: pprint(create_collection("Vodacom"))
395/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execite the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
395/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
395/14: pprint(result["sources"])
395/15: collection_name = "Papers"
395/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
395/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
395/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
395/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, ["Unstructured"]])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
395/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
395/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
395/22:
result = get_response("What is a Hypothetical Document?", collection_name)
Markdown(result["response"])
395/23:
# ask about HyDE
result = get_response("Explain HyDE step by step as an algorithm", collection_name)
Markdown(result["response"])
395/24: pprint(result["sources"])
398/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
398/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
398/3: #HOST_URL="https://gpt.vodacom-online.com/"
398/4: client = Client(HOST_URL)
398/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
398/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
398/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
398/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
398/9: pprint(login(user_name, password, langchain_mode))
398/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
398/11: pprint(create_collection("Vodacom"))
398/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execute the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
398/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
398/14: pprint(result["sources"])
398/15: collection_name = "Papers"
398/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
398/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
398/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
398/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, ["Unstructured"]])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
398/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
398/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
398/22:
result = get_response("What is a Hypothetical Document?", collection_name)
Markdown(result["response"])
398/23:
# ask about HyDE
result = get_response("Explain HyDE step by step as an algorithm", collection_name)
Markdown(result["response"])
398/24: pprint(result["sources"])
399/1:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
399/2:
#base_url = 'https://gpt.vodacom-online.com:5000/v1'
base_url = 'http://dragon:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
399/3:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?"
        }
    ],
)

Markdown(response.choices[0].message.content)
399/4:
username = "u01"
password = "p01"
langhain_mode="RecSys"

user = f"{username}:{password}:{langhain_mode}"
399/5:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
400/1:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
400/2:
#base_url = 'https://gpt.vodacom-online.com:5000/v1'
base_url = 'http://dragon:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
400/3:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?"
        }
    ],
)

Markdown(response.choices[0].message.content)
400/4:
username = "u01"
password = "p01"
langhain_mode="Papers"

user = f"{username}:{password}:{langhain_mode}"
400/5:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
398/25:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
398/26:
#HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
398/27: HOST_URL="https://gpt.vodacom-online.com/"
398/28: client = Client(HOST_URL)
398/29:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
398/30:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
398/31:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
398/32:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
398/33: pprint(login(user_name, password, langchain_mode))
398/34:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
398/35: pprint(create_collection("Vodacom"))
398/36: collection_name = "Papers"
398/37:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
398/38:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
398/39:
# ask about HyDE
result = get_response("Explain HyDE step by step as an algorithm", collection_name)
Markdown(result["response"])
398/40: pprint(result["sources"])
400/6:
username = "u01"
password = "p01"
langhain_mode="RecSys"

user = f"{username}:{password}:{langhain_mode}"
400/7:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
401/1:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
401/2:
#base_url = 'https://gpt.vodacom-online.com:5000/v1'
base_url = 'http://dragon:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
401/3:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?"
        }
    ],
)

Markdown(response.choices[0].message.content)
402/1:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
402/2:
base_url = 'https://gpt.vodacom-online.com:5000/v1'
#base_url = 'http://dragon:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
402/3:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?"
        }
    ],
)

Markdown(response.choices[0].message.content)
402/4:
username = "u01"
password = "p01"
langhain_mode="RecSys"

user = f"{username}:{password}:{langhain_mode}"
402/5:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
405/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
405/2:
#HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
405/3: #HOST_URL="https://gpt.vodacom-online.com/"
405/4: client = Client(HOST_URL)
406/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
406/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
406/3: #HOST_URL="https://gpt.vodacom-online.com/"
406/4: client = Client(HOST_URL)
406/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
406/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
406/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
406/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
406/9: pprint(login(user_name, password, langchain_mode))
406/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
406/11: pprint(create_collection("Vodacom"))
406/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execute the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
406/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
406/14: pprint(result["sources"])
406/15: collection_name = "Papers"
406/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
406/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
406/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
406/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, ["Unstructured"]])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
406/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
406/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
406/22:
result = get_response("What is a Hypothetical Document?", collection_name)
Markdown(result["response"])
406/23:
# ask about HyDE
result = get_response("Explain HyDE step by step as an algorithm", collection_name)
Markdown(result["response"])
406/24: pprint(result["sources"])
407/1:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
407/2:
#base_url = 'https://gpt.vodacom-online.com:5000/v1'
base_url = 'http://dragon:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
407/3:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?"
        }
    ],
)

Markdown(response.choices[0].message.content)
407/4:
username = "u01"
password = "p01"
langhain_mode="RecSys"

user = f"{username}:{password}:{langhain_mode}"
407/5:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
408/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
408/2:
HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
408/3: #HOST_URL="https://gpt.vodacom-online.com/"
408/4: client = Client(HOST_URL)
408/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
408/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
408/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
408/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
408/9: pprint(login(user_name, password, langchain_mode))
408/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
408/11: pprint(create_collection("Vodacom"))
408/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execute the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
408/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
408/14: pprint(result["sources"])
408/15: collection_name = "Papers"
408/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
408/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
408/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
408/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, ["Unstructured"]])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
408/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
408/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
408/22:
result = get_response("What is a Hypothetical Document?", collection_name)
Markdown(result["response"])
408/23:
# ask about HyDE
result = get_response("Explain HyDE step by step as an algorithm", collection_name)
Markdown(result["response"])
408/24: pprint(result["sources"])
409/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
409/2:
#HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
409/3: HOST_URL="https://gpt.vodacom-online.com/"
409/4: client = Client(HOST_URL)
409/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
409/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
409/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
409/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
409/9: pprint(login(user_name, password, langchain_mode))
409/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
409/11: pprint(create_collection("Vodacom"))
409/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execute the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
409/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
409/14: pprint(result["sources"])
409/15: collection_name = "Papers"
409/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
409/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
409/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
409/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, ["Unstructured"]])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
409/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
409/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
409/22:
result = get_response("What is a Hypothetical Document?", collection_name)
Markdown(result["response"])
409/23:
# ask about HyDE
result = get_response("Explain HyDE step by step as an algorithm", collection_name)
Markdown(result["response"])
409/24: pprint(result["sources"])
410/1:
import os
from gradio_client import Client
import ast

from IPython.display import display, Markdown
from pprint import pprint
410/2:
#HOST_URL = "http://127.0.0.1:7860"
H2OGPT_KEY = "b3sty"
LANGCHAIN_MODE = "Vodacom"
410/3: HOST_URL="https://gpt.vodacom-online.com/"
410/4: client = Client(HOST_URL)
410/5:
def get_response(prompt:str, langchain_mode:str):
    global H2OGPT_KEY
    
    # string of dict for input
    kwargs = dict(instruction_nochat=prompt,
                  langchain_mode=langchain_mode,
                  h2ogpt_key=H2OGPT_KEY)

    res = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')

    # string of dict for output
    response = ast.literal_eval(res)
    return response
410/6:
def ask_vodacom(langchain_mode="LLM"):
    return get_response("Who is Vodacom?", langchain_mode)

pprint(ask_vodacom())
410/7:
def login(user_name:str, password:str, langchain_mode:str="LLM"):
    global client
    
    return client.predict(
        langchain_mode,
        user_name,
        password,
        None,
        None,
        api_name='/login'
    )
410/8:
langchain_mode = "MyData"
user_name = "u01"
password = "p01"
410/9: pprint(login(user_name, password, langchain_mode))
410/10:
def create_collection(collection_name:str):
    global H2OGPT_KEY
    global client

    new_langchain_mode_text = '%s, %s' % (collection_name, 'personal')
    
    return client.predict(
        collection_name,
        new_langchain_mode_text,
        H2OGPT_KEY,
        api_name='/new_langchain_mode_text'
    )
410/11: pprint(create_collection("Vodacom"))
410/12:
langchain_mode="Vodacom"
chunk = True
chunk_size = 512
embed = True

jq_schema = ".[]"
extract_frames = 10
llava_prompt = "auto"

#loaders = tuple([None, "PyMuPDF", "Unstructured"])
loaders = tuple([None, None, ["Unstructured"]])
doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
extra_options = tuple([jq_schema, extract_frames, llava_prompt])

# execute the api call
res = client.predict(
    "https://www.vodacom.co.za",
    *doc_options,
    *loaders,
    *extra_options,
    H2OGPT_KEY,
    api_name='/add_url')

# show the results
pprint(res)
410/13:
result = ask_vodacom(langchain_mode="Vodacom")
Markdown(result["response"])
410/14: pprint(result["sources"])
410/15: collection_name = "Papers"
410/16:
# perform the login
login_result = login(user_name, password, langchain_mode="MyData")
410/17:
# create paper collection
collection_result = create_collection(collection_name)
pprint(collection_result)
410/18:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
410/19:
def upload_file(file_path:str, langchain_mode:str):
    global H2OGPT_KEY
    global client
    
    # upload the local file to the server
    file_local, file_server = client.predict(
        file_path,
        api_name="/upload_api"
    )

    # collectopm settings
    chunk = True
    chunk_size = 512
    embed = True

    jq_schema = ".[]"
    extract_frames = 10
    llava_prompt = "auto"

    # collection parameters
    loaders = tuple([None, None, ["Unstructured"]])
    doc_options = tuple([langchain_mode, chunk, chunk_size, embed])
    extra_options = tuple([jq_schema, extract_frames, llava_prompt])

    # add the file to the collection
    return client.predict(
        file_server,
        *doc_options,
        *loaders,
        *extra_options,
        H2OGPT_KEY,
        api_name='/add_file_api'
    )
410/20:
file_path = os.path.abspath("../data/2212.10496.pdf")
pprint(upload_file(file_path, collection_name))
410/21:
# ask about HyDE
result = get_response("What is HyDE?", collection_name)
Markdown(result["response"])
410/22:
result = get_response("What is a Hypothetical Document?", collection_name)
Markdown(result["response"])
410/23:
# ask about HyDE
result = get_response("Explain HyDE step by step as an algorithm", collection_name)
Markdown(result["response"])
410/24: pprint(result["sources"])
411/1:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
411/2:
base_url = 'https://gpt.vodacom-online.com:5000/v1'
#base_url = 'http://dragon:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
411/3:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?"
        }
    ],
)

Markdown(response.choices[0].message.content)
411/4:
username = "u01"
password = "p01"
langhain_mode="RecSys"

user = f"{username}:{password}:{langhain_mode}"
411/5:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=user
)

Markdown(response.choices[0].message.content)
411/6:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=f"{username}:{password}",
    extra_body=dict(langchain_mode='UserData')    
)

Markdown(response.choices[0].message.content)
411/7:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=f"{username}:{password}",
    extra_body=dict(langchain_mode='RecSys')    
)

Markdown(response.choices[0].message.content)
412/1:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=f"{username}:{password}",
    extra_body=dict(
        langchain_mode='RecSys'
    )    
)

Markdown(response.choices[0].message.content)
412/2:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=f"{username}:{password}",
    extra_body=dict(
        langchain_mode='RecSys'
    )    
)

Markdown(response.choices[0].message.content)
413/1:
from openai import OpenAI

from IPython.display import display, Markdown
from pprint import pprint
413/2:
base_url = 'https://gpt.vodacom-online.com:5000/v1'
#base_url = 'http://dragon:5000/v1'
api_key = '501ACE'

client = OpenAI(
    base_url=base_url,
    api_key=api_key
)
413/3:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?"
        }
    ],
)

Markdown(response.choices[0].message.content)
413/4:
username = "u01"
password = "p01"
langhain_mode="RecSys"

#user = f"{username}:{password}:{langhain_mode}"
413/5:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=f"{username}:{password}",
    extra_body=dict(
        langchain_mode='RecSys'
    )    
)

Markdown(response.choices[0].message.content)
413/6:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?"
        }
    ],
)

Markdown(response.choices[0].message.content)
413/7:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "Who is Vodacom?",
        }
    ],
    user=f"{username}:{password}",
    extra_body=dict(
        langchain_mode='Vodacom'
    )    
)

Markdown(response.choices[0].message.content)
413/8:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "Who is Vodacom?",
        }
    ],
    user=f"{username}:{password}",
    extra_body=dict(
        langchain_mode='LLM'
    )    
)

Markdown(response.choices[0].message.content)
413/9:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "Who is Vodacom?",
        }
    ],
    user=f"{username}:{password}",
    extra_body=dict(
        langchain_mode='Vodacom'
    )    
)

Markdown(response.choices[0].message.content)
413/10:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "Who is Vodacom?",
        }
    ],
    user=f"{username}:{password}",
    extra_body=dict(
        langchain_mode='Vodacom',
        hyde_level=2
    )    
)

Markdown(response.choices[0].message.content)
413/11:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "Who is Vodacom?",
        }
    ],
    user=f"{username}:{password}",
    extra_body=dict(
        langchain_mode='RecSys',
        hyde_level=2
    )    
)

Markdown(response.choices[0].message.content)
413/12:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=f"{username}:{password}",
    extra_body=dict(
        langchain_mode='RecSys',
        hyde_level=2
    )    
)

Markdown(response.choices[0].message.content)
413/13:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=f"{username}:{password}",
    extra_body=dict(
        langchain_mode='RecSys',
        hyde_level=2,
        hyde_show_only_final=False,
    )    
)

Markdown(response.choices[0].message.content)
413/14:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "Who is Vodacom?",
        }
    ],
    user=f"{username}:{password}",
    extra_body=dict(
        langchain_mode='Vodacom',
        hyde_level=2,
        hyde_show_only_final=True
    )    
)

Markdown(response.choices[0].message.content)
413/15: response
413/16: print(str(response))
413/17: pprint(str(response))
413/18:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "Who is Vodacom?",
        }
    ],
    user=f"{username}:{password}",
    extra_body=dict(
        langchain_mode='Vodacom',
        hyde_level=2,
        hyde_show_only_final=True
    )    
)

Markdown(response.choices[0].message.content)
413/19:
response = client.chat.completions.create(
    model="HuggingFaceH4/zephyr-7b-beta",
    messages=[
        {
            "role": "user",
            "content": "What are sequential recommendation systems?",
        }
    ],
    user=f"{username}:{password}",
    extra_body=dict(
        langchain_mode='RecSys',
        hyde_level=2,
        hyde_show_only_final=True,
    )    
)

Markdown(response.choices[0].message.content)
413/20: ?response
413/21: response
413/22: response
413/23: response
413/24: pprint(str(response.model_dump_json()))
413/25: response.model_dump_json()
413/26: dict(response)
413/27: dict(response['choices'])
413/28: dict(response.choices)
413/29: dict(response)
413/30: dict(response)['choices']
413/31: dict(response)
413/32: response.choices[0]
413/33: response.choices[0].message
413/34: response.choices[0].message.content
413/35: pprint(response.choices[0].message.content)
414/1: import pandas as pd
415/1: import pandas as pd
415/2: data_path = '../../data/'
415/3:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
415/4:
# file paths for the data files
data_path = '../../data/'
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
415/5:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
415/6:
# file paths for the data files
data_path = '../../data'
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
415/7:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
415/8:
# file paths for the data files
data_path = '../../data/coveo'
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
415/9:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
415/10: df_train.describe()
415/11:
print('Train:', df_train.shape)
print('Validation:', df_validation.shape)
print('Test:', df_test.shape)
415/12: display(df_train.head())
415/13:
# unique sessions in each dataset
print('Train:', df_train.session_id.nunique())
print('Validation:', df_validation.session_id.nunique())
print('Test:', df_test.session_id.nunique())
415/14:
# unique sessions in each dataset
print('Train:', df_train.SessionId.nunique())
print('Validation:', df_validation.SessionId.nunique())
print('Test:', df_test.SessionId.nunique())
415/15:
# show the shape of the datasets
print('Train      :', df_train.shape)
print('Validation :', df_validation.shape)
print('Test       :', df_test.shape)
415/16:
# unique sessions in each dataset
print('Train      :', df_train.SessionId.nunique())
print('Validation :', df_validation.SessionId.nunique())
print('Test       :', df_test.SessionId.nunique())
415/17:
# average session length in each dataset
print('Train      :', df_train.groupby('SessionId').size().mean())
print('Validation :', df_validation.groupby('SessionId').size().mean())
print('Test       :', df_test.groupby('SessionId').size().mean())
415/18:
# Sort by SessionId and Time to ensure the order is correct
df_sorted = df_train.sort_values(by=['SessionId', 'Time'])
415/19:
# Sort by SessionId and Time to ensure the order is correct
df_sorted = df_train.sort_values(by=['SessionId', 'Time'])

# Sort by SessionId and Time to ensure the order is correct
415/20:
# Sort by SessionId and Time to ensure the order is correct
df_sorted = df_train.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
sequences = df_sorted.groupby('SessionId')['ItemId'].apply(list)
415/21: sequences
415/22:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()
415/23:
import pandas as pd
from keras.utils import to_categorical
417/1:
import pandas as pd
from keras.utils import to_categorical
417/2:
# file paths for the data files
data_path = '../../data/coveo'
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
417/3:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
417/4:
# show the shape of the datasets
print('Train      :', df_train.shape)
print('Validation :', df_validation.shape)
print('Test       :', df_test.shape)
417/5:
# head of the training set
display(df_train.head())
417/6:
# unique sessions in each dataset
print('Train      :', df_train.SessionId.nunique())
print('Validation :', df_validation.SessionId.nunique())
print('Test       :', df_test.SessionId.nunique())
417/7:
# average session length in each dataset
print('Train      :', df_train.groupby('SessionId').size().mean())
print('Validation :', df_validation.groupby('SessionId').size().mean())
print('Test       :', df_test.groupby('SessionId').size().mean())
417/8:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()
417/9:
# Sort by SessionId and Time to ensure the order is correct
df_sorted = df_train.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
sequences = df_sorted.groupby('SessionId')['ItemId'].apply(list)
417/10: sequences
418/1: import pandas as pd
418/2:
# file paths for the data files
data_path = '../../data/coveo'
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
418/3:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
418/4:
# show the shape of the datasets
print('Train      :', df_train.shape)
print('Validation :', df_validation.shape)
print('Test       :', df_test.shape)
418/5:
# head of the training set
display(df_train.head())
418/6:
# unique sessions in each dataset
print('Train      :', df_train.SessionId.nunique())
print('Validation :', df_validation.SessionId.nunique())
print('Test       :', df_test.SessionId.nunique())
418/7:
# average session length in each dataset
print('Train      :', df_train.groupby('SessionId').size().mean())
print('Validation :', df_validation.groupby('SessionId').size().mean())
print('Test       :', df_test.groupby('SessionId').size().mean())
418/8:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()
418/9:
# Sort by SessionId and Time to ensure the order is correct
df_sorted = df_train.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
sequences = df_sorted.groupby('SessionId')['ItemId'].apply(list)
418/10: sequences
419/1:
import pandas as pd
from keras.preprocessing.text import Tokenizer
419/2:
# file paths for the data files
data_path = '../../data/coveo'
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
419/3:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
419/4:
# show the shape of the datasets
print('Train      :', df_train.shape)
print('Validation :', df_validation.shape)
print('Test       :', df_test.shape)
419/5:
# head of the training set
display(df_train.head())
419/6:
# unique sessions in each dataset
print('Train      :', df_train.SessionId.nunique())
print('Validation :', df_validation.SessionId.nunique())
print('Test       :', df_test.SessionId.nunique())
419/7:
# average session length in each dataset
print('Train      :', df_train.groupby('SessionId').size().mean())
print('Validation :', df_validation.groupby('SessionId').size().mean())
print('Test       :', df_test.groupby('SessionId').size().mean())
419/8:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.pkl'
tokenizer.to_json(tokenizer_path)
419/9:
import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
419/10:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# convert unique_items to string because tokenizer works with string data
unique_items_str = [str(item) for item in unique_items]

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.pkl'
tokenizer.to_json(tokenizer_path)
419/11:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# convert unique_items to string because tokenizer works with string data
unique_items_str = [str(item) for item in unique_items]

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items_str) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.pkl'
tokenizer.to_json(tokenizer_path)
419/12:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# convert unique_items to string because tokenizer works with string data
unique_items_str = [str(item) for item in unique_items]

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items_str) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items_str)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.pkl'
tokenizer.to_json(tokenizer_path)
419/13:
data_path = '../../data/coveo'
model_path = '../../models/coveo'

# file paths for the data files
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
419/14:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# convert unique_items to string because tokenizer works with string data
unique_items_str = [str(item) for item in unique_items]

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items_str) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items_str)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.pkl'
tokenizer.to_json(tokenizer_path)
419/15:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# convert unique_items to string because tokenizer works with string data
unique_items_str = [str(item) for item in unique_items]

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items_str) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items_str)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.pkl'
tokenizer.to_json()
419/16:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# convert unique_items to string because tokenizer works with string data
unique_items_str = [str(item) for item in unique_items]

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items_str) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items_str)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.pkl'
with open(tokenizer_path, 'w') as file:
    tokenizer.to_json()
419/17: df_train['ItemId'].min()
419/18: df_train['ItemId'].min(), df_train['ItemId'].max()
419/19:
# Convert the item ids in the datasets to their corresponding integer values
df_train['ItemId'] = tokenizer.texts_to_sequences(df_train['ItemId'].astype(str).tolist())
df_validation['ItemId'] = tokenizer.texts_to_sequences(df_validation['ItemId'].astype(str).tolist())
df_test['ItemId'] = tokenizer.texts_to_sequences(df_test['ItemId'].astype(str).tolist())
419/20: df_train['ItemId'].min(), df_train['ItemId'].max()
419/21:
# Sort by SessionId and Time to ensure the order is correct
df_sorted = df_train.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
sequences = df_sorted.groupby('SessionId')['ItemId'].apply(list)
420/1:
import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
420/2:
data_path = '../../data/coveo'
model_path = '../../models/coveo'

# file paths for the data files
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
420/3:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
420/4:
# show the shape of the datasets
print('Train      :', df_train.shape)
print('Validation :', df_validation.shape)
print('Test       :', df_test.shape)
420/5:
# head of the training set
display(df_train.head())
420/6:
# unique sessions in each dataset
print('Train      :', df_train.SessionId.nunique())
print('Validation :', df_validation.SessionId.nunique())
print('Test       :', df_test.SessionId.nunique())
420/7:
# average session length in each dataset
print('Train      :', df_train.groupby('SessionId').size().mean())
print('Validation :', df_validation.groupby('SessionId').size().mean())
print('Test       :', df_test.groupby('SessionId').size().mean())
420/8:
# Sort by SessionId and Time to ensure the order is correct
df_sorted = df_train.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
sequences = df_sorted.groupby('SessionId')['ItemId'].apply(list)
420/9: sequences
420/10: type(sequences)
420/11:
# Sort by SessionId and Time to ensure the order is correct
df_train_sorted = df_train.sort_values(by=['SessionId', 'Time'])
df_validation_sorted = df_validation.sort_values(by=['SessionId', 'Time'])
df_test_sorted = df_test.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
train_sequences = df_train_sorted.groupby('SessionId')['ItemId'].apply(list)
validation_sequences = df_validation_sorted.groupby('SessionId')['ItemId'].apply(list)
test_sequences = df_test_sorted.groupby('SessionId')['ItemId'].apply(list)
420/12: type(train_sequences)
420/13: train_sequences
420/14:
# tokenize the sequences
train_sequences_tokenized = tokenizer.texts_to_sequences(train_sequences)
validation_sequences_tokenized = tokenizer.texts_to_sequences(validation_sequences)
test_sequences_tokenized = tokenizer.texts_to_sequences(test_sequences)
420/15:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# convert unique_items to string because tokenizer works with string data
unique_items_str = [str(item) for item in unique_items]

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items_str) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items_str)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.pkl'
with open(tokenizer_path, 'w') as file:
    tokenizer.to_json()
420/16:
# tokenize the sequences
train_sequences_tokenized = tokenizer.texts_to_sequences(train_sequences)
validation_sequences_tokenized = tokenizer.texts_to_sequences(validation_sequences)
test_sequences_tokenized = tokenizer.texts_to_sequences(test_sequences)
420/17:
# convert the items ids to strings
df_train['ItemId'] = df_train['ItemId'].astype(str)
df_validation['ItemId'] = df_validation['ItemId'].astype(str)
df_test['ItemId'] = df_test['ItemId'].astype(str)
421/1:
import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
421/2:
data_path = '../../data/coveo'
model_path = '../../models/coveo'

# file paths for the data files
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
421/3:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
421/4:
# show the shape of the datasets
print('Train      :', df_train.shape)
print('Validation :', df_validation.shape)
print('Test       :', df_test.shape)
421/5:
# head of the training set
display(df_train.head())
421/6:
# unique sessions in each dataset
print('Train      :', df_train.SessionId.nunique())
print('Validation :', df_validation.SessionId.nunique())
print('Test       :', df_test.SessionId.nunique())
421/7:
# average session length in each dataset
print('Train      :', df_train.groupby('SessionId').size().mean())
print('Validation :', df_validation.groupby('SessionId').size().mean())
print('Test       :', df_test.groupby('SessionId').size().mean())
421/8:
# convert the items ids to strings
df_train['ItemId'] = df_train['ItemId'].astype(str)
df_validation['ItemId'] = df_validation['ItemId'].astype(str)
df_test['ItemId'] = df_test['ItemId'].astype(str)
421/9:
# Sort by SessionId and Time to ensure the order is correct
df_train_sorted = df_train.sort_values(by=['SessionId', 'Time'])
df_validation_sorted = df_validation.sort_values(by=['SessionId', 'Time'])
df_test_sorted = df_test.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
train_sequences = df_train_sorted.groupby('SessionId')['ItemId'].apply(list)
validation_sequences = df_validation_sorted.groupby('SessionId')['ItemId'].apply(list)
test_sequences = df_test_sorted.groupby('SessionId')['ItemId'].apply(list)
421/10: train_sequences
421/11:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.pkl'
with open(tokenizer_path, 'w') as file:
    tokenizer.to_json()
421/12:
# tokenize the sequences
train_sequences_tokenized = tokenizer.texts_to_sequences(train_sequences)
validation_sequences_tokenized = tokenizer.texts_to_sequences(validation_sequences)
test_sequences_tokenized = tokenizer.texts_to_sequences(test_sequences)
421/13: train_sequences_tokenized
421/14: type(train_sequences_tokenized)
421/15: train_sequences.head(5)
421/16: train_sequences_tokenized[:5]
421/17:
# average session length in each dataset
print('Train      :', df_train.groupby('SessionId').size().describe())
print('Validation :', df_validation.groupby('SessionId').size().mean())
print('Test       :', df_test.groupby('SessionId').size().mean())
421/18:
# average session length in each dataset
print('--- Train ---')
print(df_train.groupby('SessionId').size().describe())
print('--- Validation ---')
print(df_validation.groupby('SessionId').size().describe())
print('--- Test ---')
print(df_test.groupby('SessionId').size().describe())
421/19:
# average session length in each dataset
print('--- Train ---')
print(df_train.groupby('SessionId').size().describe().T)
print('--- Validation ---')
print(df_validation.groupby('SessionId').size().describe())
print('--- Test ---')
print(df_test.groupby('SessionId').size().describe())
421/20:
# average session length in each dataset
print('--- Train ---')
print(df_train.groupby('SessionId').size().describe().T)
print('--- Validation ---')
print(df_validation.groupby('SessionId').size().describe())
print('--- Test ---')
print(df_test.groupby('SessionId').size().describe())
421/21:
# average session length in each dataset
print('--- Train ---')
print(df_train.groupby('SessionId').size().describe())
print('--- Validation ---')
print(df_validation.groupby('SessionId').size().describe())
print('--- Test ---')
print(df_test.groupby('SessionId').size().describe())
421/22:
# Determine the maximum sequence length for padding
max_sequence_length = train_sequences.map(len).max()
421/23:
# Determine the maximum sequence length for padding
max_sequence_length = train_sequences.map(len).max()

# pad the sequences
train_sequences_padded = pad_sequences(train_sequences_tokenized, maxlen=max_sequence_length, padding='post')
validation_sequences_padded = pad_sequences(validation_sequences_tokenized, maxlen=max_sequence_length, padding='post')
test_sequences_padded = pad_sequences(test_sequences_tokenized, maxlen=max_sequence_length, padding='post')
421/24: train_sequences_padded[:5]
421/25:
import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense
421/26: model = Sequential()
421/27: tokenizer
421/28:
# get the maximum item id from the tokenizer
max_item_id = max(tokenizer.index_word.keys())
421/29:
# get the maximum item id from the tokenizer
max_item_id = max(tokenizer.index_word.keys())
model = Sequential()
421/30:
# get the maximum item id from the tokenizer
max_item_id = max(tokenizer.index_word.keys())
max_item_id
421/31: max(unique_items)
421/32: max(unique_items.count(), 1000)
421/33: unique_items
421/34: len(unique_items)
421/35:
max_item_id = max(tokenizer.index_word.keys())  # get the maximum item id from the tokenizer
embedding_dim = 64                              # Size of the embedding vectords
max_item_id
421/36:
max_item_id = max(tokenizer.index_word.keys())  # get the maximum item id from the tokenizer
embedding_dim = 64                              # Size of the embedding vectors
422/1:
import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense
422/2:
data_path = '../../data/coveo'
model_path = '../../models/coveo'

# file paths for the data files
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
422/3:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
422/4:
# show the shape of the datasets
print('Train      :', df_train.shape)
print('Validation :', df_validation.shape)
print('Test       :', df_test.shape)
422/5:
# head of the training set
display(df_train.head())
422/6:
# unique sessions in each dataset
print('Train      :', df_train.SessionId.nunique())
print('Validation :', df_validation.SessionId.nunique())
print('Test       :', df_test.SessionId.nunique())
422/7:
# average session length in each dataset
print('--- Train ---')
print(df_train.groupby('SessionId').size().describe())
print('--- Validation ---')
print(df_validation.groupby('SessionId').size().describe())
print('--- Test ---')
print(df_test.groupby('SessionId').size().describe())
422/8:
# convert the items ids to strings
df_train['ItemId'] = df_train['ItemId'].astype(str)
df_validation['ItemId'] = df_validation['ItemId'].astype(str)
df_test['ItemId'] = df_test['ItemId'].astype(str)
422/9:
# Sort by SessionId and Time to ensure the order is correct
df_train_sorted = df_train.sort_values(by=['SessionId', 'Time'])
df_validation_sorted = df_validation.sort_values(by=['SessionId', 'Time'])
df_test_sorted = df_test.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
train_sequences = df_train_sorted.groupby('SessionId')['ItemId'].apply(list)
validation_sequences = df_validation_sorted.groupby('SessionId')['ItemId'].apply(list)
test_sequences = df_test_sorted.groupby('SessionId')['ItemId'].apply(list)
422/10: train_sequences.head(5)
422/11:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.pkl'
with open(tokenizer_path, 'w') as file:
    tokenizer.to_json()
422/12:
# tokenize the sequences
train_sequences_tokenized = tokenizer.texts_to_sequences(train_sequences)
validation_sequences_tokenized = tokenizer.texts_to_sequences(validation_sequences)
test_sequences_tokenized = tokenizer.texts_to_sequences(test_sequences)
422/13: train_sequences_tokenized[:5]
422/14:
# Determine the maximum sequence length for padding
max_sequence_length = train_sequences.map(len).max()

# pad the sequences
train_sequences_padded = pad_sequences(train_sequences_tokenized, maxlen=max_sequence_length, padding='post')
validation_sequences_padded = pad_sequences(validation_sequences_tokenized, maxlen=max_sequence_length, padding='post')
test_sequences_padded = pad_sequences(test_sequences_tokenized, maxlen=max_sequence_length, padding='post')
422/15: train_sequences_padded[:5]
422/16: model = Sequential()
422/17:
max_item_id = max(tokenizer.index_word.keys())  # get the maximum item id from the tokenizer
embedding_dim = 64                              # Size of the embedding vectors
422/18:
max_item_id = max(tokenizer.index_word.keys())  # get the maximum item id from the tokenizer
embedding_dim = 64                              # Size of the embedding vectors

# Embedding layer for vectorizing ItemIds into dense vectors
model.add(Embedding(input_dim=max_item_id + 1, output_dim=embedding_dim, input_length=max_sequence_length))
422/19:
import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense
422/20:
# Conv1D + MaxPooling1D for local pattern recognition and dimensionality reduction
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(MaxPooling1D(5))
425/1:
import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense
425/2:
data_path = '../../data/coveo'
model_path = '../../models/coveo'

# file paths for the data files
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
425/3:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
425/4:
# show the shape of the datasets
print('Train      :', df_train.shape)
print('Validation :', df_validation.shape)
print('Test       :', df_test.shape)
425/5:
# head of the training set
display(df_train.head())
425/6:
# unique sessions in each dataset
print('Train      :', df_train.SessionId.nunique())
print('Validation :', df_validation.SessionId.nunique())
print('Test       :', df_test.SessionId.nunique())
425/7:
# average session length in each dataset
print('--- Train ---')
print(df_train.groupby('SessionId').size().describe())
print('--- Validation ---')
print(df_validation.groupby('SessionId').size().describe())
print('--- Test ---')
print(df_test.groupby('SessionId').size().describe())
425/8:
# convert the items ids to strings
df_train['ItemId'] = df_train['ItemId'].astype(str)
df_validation['ItemId'] = df_validation['ItemId'].astype(str)
df_test['ItemId'] = df_test['ItemId'].astype(str)
425/9:
# Sort by SessionId and Time to ensure the order is correct
df_train_sorted = df_train.sort_values(by=['SessionId', 'Time'])
df_validation_sorted = df_validation.sort_values(by=['SessionId', 'Time'])
df_test_sorted = df_test.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
train_sequences = df_train_sorted.groupby('SessionId')['ItemId'].apply(list)
validation_sequences = df_validation_sorted.groupby('SessionId')['ItemId'].apply(list)
test_sequences = df_test_sorted.groupby('SessionId')['ItemId'].apply(list)
425/10: train_sequences.head(5)
425/11:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.pkl'
with open(tokenizer_path, 'w') as file:
    tokenizer.to_json()
425/12:
# tokenize the sequences
train_sequences_tokenized = tokenizer.texts_to_sequences(train_sequences)
validation_sequences_tokenized = tokenizer.texts_to_sequences(validation_sequences)
test_sequences_tokenized = tokenizer.texts_to_sequences(test_sequences)
425/13: train_sequences_tokenized[:5]
425/14:
# Determine the maximum sequence length for padding
max_sequence_length = train_sequences.map(len).max()

# pad the sequences
train_sequences_padded = pad_sequences(train_sequences_tokenized, maxlen=max_sequence_length, padding='post')
validation_sequences_padded = pad_sequences(validation_sequences_tokenized, maxlen=max_sequence_length, padding='post')
test_sequences_padded = pad_sequences(test_sequences_tokenized, maxlen=max_sequence_length, padding='post')
425/15: train_sequences_padded[:5]
425/16: model = Sequential()
425/17:
max_item_id = max(tokenizer.index_word.keys())  # get the maximum item id from the tokenizer
embedding_dim = 64                              # Size of the embedding vectors

# Embedding layer for vectorizing ItemIds into dense vectors
model.add(Embedding(input_dim=max_item_id + 1, output_dim=embedding_dim, input_length=max_sequence_length))
425/18:
# Conv1D + MaxPooling1D for local pattern recognition and dimensionality reduction
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(MaxPooling1D(5))

# Additional Conv1D + GlobalMaxPooling1D for deeper pattern recognition
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(GlobalMaxPooling1D())
425/19: model.add(LSTM(32))  # LSTM layer with 32 units. Adjust the number of units based on your dataset and complexity.
426/1:
import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dense
426/2:
data_path = '../../data/coveo'
model_path = '../../models/coveo'

# file paths for the data files
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
426/3:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
426/4:
# show the shape of the datasets
print('Train      :', df_train.shape)
print('Validation :', df_validation.shape)
print('Test       :', df_test.shape)
426/5:
# head of the training set
display(df_train.head())
426/6:
# unique sessions in each dataset
print('Train      :', df_train.SessionId.nunique())
print('Validation :', df_validation.SessionId.nunique())
print('Test       :', df_test.SessionId.nunique())
426/7:
# average session length in each dataset
print('--- Train ---')
print(df_train.groupby('SessionId').size().describe())
print('--- Validation ---')
print(df_validation.groupby('SessionId').size().describe())
print('--- Test ---')
print(df_test.groupby('SessionId').size().describe())
426/8:
# convert the items ids to strings
df_train['ItemId'] = df_train['ItemId'].astype(str)
df_validation['ItemId'] = df_validation['ItemId'].astype(str)
df_test['ItemId'] = df_test['ItemId'].astype(str)
426/9:
# Sort by SessionId and Time to ensure the order is correct
df_train_sorted = df_train.sort_values(by=['SessionId', 'Time'])
df_validation_sorted = df_validation.sort_values(by=['SessionId', 'Time'])
df_test_sorted = df_test.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
train_sequences = df_train_sorted.groupby('SessionId')['ItemId'].apply(list)
validation_sequences = df_validation_sorted.groupby('SessionId')['ItemId'].apply(list)
test_sequences = df_test_sorted.groupby('SessionId')['ItemId'].apply(list)
426/10: train_sequences.head(5)
426/11:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.pkl'
with open(tokenizer_path, 'w') as file:
    tokenizer.to_json()
426/12:
# tokenize the sequences
train_sequences_tokenized = tokenizer.texts_to_sequences(train_sequences)
validation_sequences_tokenized = tokenizer.texts_to_sequences(validation_sequences)
test_sequences_tokenized = tokenizer.texts_to_sequences(test_sequences)
426/13: train_sequences_tokenized[:5]
426/14:
# Determine the maximum sequence length for padding
max_sequence_length = train_sequences.map(len).max()

# pad the sequences
train_sequences_padded = pad_sequences(train_sequences_tokenized, maxlen=max_sequence_length, padding='post')
validation_sequences_padded = pad_sequences(validation_sequences_tokenized, maxlen=max_sequence_length, padding='post')
test_sequences_padded = pad_sequences(test_sequences_tokenized, maxlen=max_sequence_length, padding='post')
426/15: train_sequences_padded[:5]
426/16: model = Sequential()
426/17:
max_item_id = max(tokenizer.index_word.keys())  # get the maximum item id from the tokenizer
embedding_dim = 64                              # Size of the embedding vectors

# Embedding layer for vectorizing ItemIds into dense vectors
model.add(Embedding(input_dim=max_item_id + 1, output_dim=embedding_dim, input_length=max_sequence_length))
426/18:
# Conv1D + MaxPooling1D for local pattern recognition and dimensionality reduction
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(MaxPooling1D(5))

# Additional Conv1D + GlobalMaxPooling1D for deeper pattern recognition
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(GlobalMaxPooling1D())
426/19: model.add(LSTM(32))  # LSTM layer with 32 units. Adjust the number of units based on your dataset and complexity.
427/1:
import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dense
427/2:
data_path = '../../data/coveo'
model_path = '../../models/coveo'

# file paths for the data files
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
427/3:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
427/4:
# show the shape of the datasets
print('Train      :', df_train.shape)
print('Validation :', df_validation.shape)
print('Test       :', df_test.shape)
427/5:
# head of the training set
display(df_train.head())
427/6:
# unique sessions in each dataset
print('Train      :', df_train.SessionId.nunique())
print('Validation :', df_validation.SessionId.nunique())
print('Test       :', df_test.SessionId.nunique())
427/7:
# average session length in each dataset
print('--- Train ---')
print(df_train.groupby('SessionId').size().describe())
print('--- Validation ---')
print(df_validation.groupby('SessionId').size().describe())
print('--- Test ---')
print(df_test.groupby('SessionId').size().describe())
427/8:
# convert the items ids to strings
df_train['ItemId'] = df_train['ItemId'].astype(str)
df_validation['ItemId'] = df_validation['ItemId'].astype(str)
df_test['ItemId'] = df_test['ItemId'].astype(str)
427/9:
# Sort by SessionId and Time to ensure the order is correct
df_train_sorted = df_train.sort_values(by=['SessionId', 'Time'])
df_validation_sorted = df_validation.sort_values(by=['SessionId', 'Time'])
df_test_sorted = df_test.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
train_sequences = df_train_sorted.groupby('SessionId')['ItemId'].apply(list)
validation_sequences = df_validation_sorted.groupby('SessionId')['ItemId'].apply(list)
test_sequences = df_test_sorted.groupby('SessionId')['ItemId'].apply(list)
427/10: train_sequences.head(5)
427/11:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.pkl'
with open(tokenizer_path, 'w') as file:
    tokenizer.to_json()
427/12:
# tokenize the sequences
train_sequences_tokenized = tokenizer.texts_to_sequences(train_sequences)
validation_sequences_tokenized = tokenizer.texts_to_sequences(validation_sequences)
test_sequences_tokenized = tokenizer.texts_to_sequences(test_sequences)
427/13: train_sequences_tokenized[:5]
427/14:
# Determine the maximum sequence length for padding
max_sequence_length = train_sequences.map(len).max()

# pad the sequences
train_sequences_padded = pad_sequences(train_sequences_tokenized, maxlen=max_sequence_length, padding='post')
validation_sequences_padded = pad_sequences(validation_sequences_tokenized, maxlen=max_sequence_length, padding='post')
test_sequences_padded = pad_sequences(test_sequences_tokenized, maxlen=max_sequence_length, padding='post')
427/15: train_sequences_padded[:5]
427/16: model = Sequential()
427/17:
max_item_id = max(tokenizer.index_word.keys())  # get the maximum item id from the tokenizer
embedding_dim = 64                              # Size of the embedding vectors

# Embedding layer for vectorizing ItemIds into dense vectors
model.add(Embedding(input_dim=max_item_id + 1, output_dim=embedding_dim, input_length=max_sequence_length))
427/18:
# Conv1D + MaxPooling1D for local pattern recognition and dimensionality reduction
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(MaxPooling1D(5))

# Additional Conv1D + GlobalMaxPooling1D for deeper pattern recognition
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(GlobalMaxPooling1D())
427/19: model.add(LSTM(32))  # LSTM layer with 32 units. Adjust the number of units based on your dataset and complexity.
427/20: model.show()
427/21: model
427/22: model.summary()
427/23:
mode.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
427/24:
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
427/25:
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
427/26:
model.compile()
model.summary()
427/27:
model.compile()
#model.summary()
427/28: #model.summary()
427/29: model.summary()
427/30:
model.compile()
model.build()
#model.summary()
427/31:
model.compile()
model.build()
model.summary()
427/32: model.add(LSTM(32))  # LSTM layer with 32 units. Adjust the number of units based on your dataset and complexity.
427/33:
model.compile()
model.build()
model.summary()
428/1:
import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dense
428/2:
data_path = '../../data/coveo'
model_path = '../../models/coveo'

# file paths for the data files
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
428/3:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
428/4:
# show the shape of the datasets
print('Train      :', df_train.shape)
print('Validation :', df_validation.shape)
print('Test       :', df_test.shape)
428/5:
# head of the training set
display(df_train.head())
428/6:
# unique sessions in each dataset
print('Train      :', df_train.SessionId.nunique())
print('Validation :', df_validation.SessionId.nunique())
print('Test       :', df_test.SessionId.nunique())
428/7:
# average session length in each dataset
print('--- Train ---')
print(df_train.groupby('SessionId').size().describe())
print('--- Validation ---')
print(df_validation.groupby('SessionId').size().describe())
print('--- Test ---')
print(df_test.groupby('SessionId').size().describe())
428/8:
# convert the items ids to strings
df_train['ItemId'] = df_train['ItemId'].astype(str)
df_validation['ItemId'] = df_validation['ItemId'].astype(str)
df_test['ItemId'] = df_test['ItemId'].astype(str)
428/9:
# Sort by SessionId and Time to ensure the order is correct
df_train_sorted = df_train.sort_values(by=['SessionId', 'Time'])
df_validation_sorted = df_validation.sort_values(by=['SessionId', 'Time'])
df_test_sorted = df_test.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
train_sequences = df_train_sorted.groupby('SessionId')['ItemId'].apply(list)
validation_sequences = df_validation_sorted.groupby('SessionId')['ItemId'].apply(list)
test_sequences = df_test_sorted.groupby('SessionId')['ItemId'].apply(list)
428/10: train_sequences.head(5)
428/11:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.pkl'
with open(tokenizer_path, 'w') as file:
    tokenizer.to_json()
428/12:
# tokenize the sequences
train_sequences_tokenized = tokenizer.texts_to_sequences(train_sequences)
validation_sequences_tokenized = tokenizer.texts_to_sequences(validation_sequences)
test_sequences_tokenized = tokenizer.texts_to_sequences(test_sequences)
428/13: train_sequences_tokenized[:5]
428/14:
# Determine the maximum sequence length for padding
max_sequence_length = train_sequences.map(len).max()

# pad the sequences
train_sequences_padded = pad_sequences(train_sequences_tokenized, maxlen=max_sequence_length, padding='post')
validation_sequences_padded = pad_sequences(validation_sequences_tokenized, maxlen=max_sequence_length, padding='post')
test_sequences_padded = pad_sequences(test_sequences_tokenized, maxlen=max_sequence_length, padding='post')
428/15: train_sequences_padded[:5]
428/16: model = Sequential()
428/17:
max_item_id = max(tokenizer.index_word.keys())  # get the maximum item id from the tokenizer
embedding_dim = 64                              # Size of the embedding vectors

# Embedding layer for vectorizing ItemIds into dense vectors
model.add(Embedding(input_dim=max_item_id + 1, output_dim=embedding_dim, input_length=max_sequence_length))
428/18:
# Conv1D + MaxPooling1D for local pattern recognition and dimensionality reduction
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(MaxPooling1D(5))

# Additional Conv1D + GlobalMaxPooling1D for deeper pattern recognition
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
#model.add(GlobalMaxPooling1D())
428/19:
#model.compile()
#model.build()
#model.summary()
428/20: model.add(LSTM(32))  # LSTM layer with 32 units. Adjust the number of units based on your dataset and complexity.
428/21:
# Output layer for predicting the next item in the sequence
model.add(Dense(num_unique_items, activation='softmax'))
428/22:
# Output layer for predicting the next item in the sequence
model.add(Dense(max_item_id, activation='softmax'))
428/23:
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
429/1:
import pandas as pd
from keras import backend as K
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dense
430/1:
import pandas as pd
from keras import backend as K
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dense
430/2:
data_path = '../../data/coveo'
model_path = '../../models/coveo'

# file paths for the data files
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
430/3:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
430/4:
# show the shape of the datasets
print('Train      :', df_train.shape)
print('Validation :', df_validation.shape)
print('Test       :', df_test.shape)
430/5:
# head of the training set
display(df_train.head())
430/6:
# unique sessions in each dataset
print('Train      :', df_train.SessionId.nunique())
print('Validation :', df_validation.SessionId.nunique())
print('Test       :', df_test.SessionId.nunique())
430/7:
# average session length in each dataset
print('--- Train ---')
print(df_train.groupby('SessionId').size().describe())
print('--- Validation ---')
print(df_validation.groupby('SessionId').size().describe())
print('--- Test ---')
print(df_test.groupby('SessionId').size().describe())
430/8:
# convert the items ids to strings
df_train['ItemId'] = df_train['ItemId'].astype(str)
df_validation['ItemId'] = df_validation['ItemId'].astype(str)
df_test['ItemId'] = df_test['ItemId'].astype(str)
430/9:
# Sort by SessionId and Time to ensure the order is correct
df_train_sorted = df_train.sort_values(by=['SessionId', 'Time'])
df_validation_sorted = df_validation.sort_values(by=['SessionId', 'Time'])
df_test_sorted = df_test.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
train_sequences = df_train_sorted.groupby('SessionId')['ItemId'].apply(list)
validation_sequences = df_validation_sorted.groupby('SessionId')['ItemId'].apply(list)
test_sequences = df_test_sorted.groupby('SessionId')['ItemId'].apply(list)
430/10: train_sequences.head(5)
430/11:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.pkl'
with open(tokenizer_path, 'w') as file:
    tokenizer.to_json()
430/12:
# tokenize the sequences
train_sequences_tokenized = tokenizer.texts_to_sequences(train_sequences)
validation_sequences_tokenized = tokenizer.texts_to_sequences(validation_sequences)
test_sequences_tokenized = tokenizer.texts_to_sequences(test_sequences)
430/13: train_sequences_tokenized[:5]
430/14:
# Determine the maximum sequence length for padding
max_sequence_length = train_sequences.map(len).max()

# pad the sequences
train_sequences_padded = pad_sequences(train_sequences_tokenized, maxlen=max_sequence_length, padding='post')
validation_sequences_padded = pad_sequences(validation_sequences_tokenized, maxlen=max_sequence_length, padding='post')
test_sequences_padded = pad_sequences(test_sequences_tokenized, maxlen=max_sequence_length, padding='post')
430/15: train_sequences_padded[:5]
430/16: model = Sequential()
430/17:
max_item_id = max(tokenizer.index_word.keys())  # get the maximum item id from the tokenizer
embedding_dim = 64                              # Size of the embedding vectors

# Embedding layer for vectorizing ItemIds into dense vectors
model.add(Embedding(input_dim=max_item_id + 1, output_dim=embedding_dim, input_length=max_sequence_length))
430/18:
# Conv1D + MaxPooling1D for local pattern recognition and dimensionality reduction
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(MaxPooling1D(5))

# Additional Conv1D + GlobalMaxPooling1D for deeper pattern recognition
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
#model.add(GlobalMaxPooling1D())
430/19: model.add(LSTM(32))  # LSTM layer with 32 units. Adjust the number of units based on your dataset and complexity.
430/20:
# Output layer for predicting the next item in the sequence
model.add(Dense(max_item_id, activation='softmax'))
430/21:
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
432/1:
import pandas as pd
from keras import backend as K
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dense
432/2:
data_path = '../../data/coveo'
model_path = '../../models/coveo'

# file paths for the data files
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
432/3:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
432/4:
# show the shape of the datasets
print('Train      :', df_train.shape)
print('Validation :', df_validation.shape)
print('Test       :', df_test.shape)
432/5:
# head of the training set
display(df_train.head())
432/6:
# unique sessions in each dataset
print('Train      :', df_train.SessionId.nunique())
print('Validation :', df_validation.SessionId.nunique())
print('Test       :', df_test.SessionId.nunique())
432/7:
# average session length in each dataset
print('--- Train ---')
print(df_train.groupby('SessionId').size().describe())
print('--- Validation ---')
print(df_validation.groupby('SessionId').size().describe())
print('--- Test ---')
print(df_test.groupby('SessionId').size().describe())
432/8:
# convert the items ids to strings
df_train['ItemId'] = df_train['ItemId'].astype(str)
df_validation['ItemId'] = df_validation['ItemId'].astype(str)
df_test['ItemId'] = df_test['ItemId'].astype(str)
432/9:
# Sort by SessionId and Time to ensure the order is correct
df_train_sorted = df_train.sort_values(by=['SessionId', 'Time'])
df_validation_sorted = df_validation.sort_values(by=['SessionId', 'Time'])
df_test_sorted = df_test.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
train_sequences = df_train_sorted.groupby('SessionId')['ItemId'].apply(list)
validation_sequences = df_validation_sorted.groupby('SessionId')['ItemId'].apply(list)
test_sequences = df_test_sorted.groupby('SessionId')['ItemId'].apply(list)
432/10: train_sequences.head(5)
432/11:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.pkl'
with open(tokenizer_path, 'w') as file:
    tokenizer.to_json()
432/12:
# tokenize the sequences
train_sequences_tokenized = tokenizer.texts_to_sequences(train_sequences)
validation_sequences_tokenized = tokenizer.texts_to_sequences(validation_sequences)
test_sequences_tokenized = tokenizer.texts_to_sequences(test_sequences)
432/13: train_sequences_tokenized[:5]
432/14:
# Determine the maximum sequence length for padding
max_sequence_length = train_sequences.map(len).max()

# pad the sequences
train_sequences_padded = pad_sequences(train_sequences_tokenized, maxlen=max_sequence_length, padding='post')
validation_sequences_padded = pad_sequences(validation_sequences_tokenized, maxlen=max_sequence_length, padding='post')
test_sequences_padded = pad_sequences(test_sequences_tokenized, maxlen=max_sequence_length, padding='post')
432/15: train_sequences_padded[:5]
432/16: model = Sequential()
432/17:
max_item_id = max(tokenizer.index_word.keys())  # get the maximum item id from the tokenizer
embedding_dim = 64                              # Size of the embedding vectors

# Embedding layer for vectorizing ItemIds into dense vectors
model.add(Embedding(input_dim=max_item_id + 1, output_dim=embedding_dim, input_length=max_sequence_length))
432/18:
# Conv1D + MaxPooling1D for local pattern recognition and dimensionality reduction
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(MaxPooling1D(5))

# Additional Conv1D + GlobalMaxPooling1D for deeper pattern recognition
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
#model.add(GlobalMaxPooling1D())
432/19: model.add(LSTM(32))  # LSTM layer with 32 units. Adjust the number of units based on your dataset and complexity.
432/20:
# Output layer for predicting the next item in the sequence
model.add(Dense(max_item_id, activation='softmax'))
432/21:
def recall_at_k(y_true, y_pred, k=10):
    # Assuming y_true and y_pred are batch-wise lists of true labels and predicted labels, respectively
    top_k_preds = K.np.argsort(y_pred)[:, -k:]  # Get indices of top-k predictions
    relevant_items = K.sum(K.any(K.in1d(top_k_preds, y_true), axis=1))  # Count relevant items in top-k predictions
    return relevant_items / K.cast(K.shape(y_true)[0], K.floatx())  # Compute recall

def mrr_at_k(y_true, y_pred, k=10):
    top_k_preds = K.np.argsort(y_pred)[:, -k:]
    ranks = K.np.argmax(K.in1d(top_k_preds, y_true), axis=1) + 1  # Find ranks of true items
    ranks[ranks > k] = 0  # Set ranks > k to 0 as they are not found within top-k
    return K.mean(1.0 / ranks[ranks > 0])  # Compute mean reciprocal rank

# Example usage in Keras model
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=[recall_at_k, mrr_at_k])
432/22:
# compile the model
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=[recall_at_k, mrr_at_k])

model.summary()
433/1:
import pandas as pd
from keras import backend as K
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dense
433/2:
data_path = '../../data/coveo'
model_path = '../../models/coveo'

# file paths for the data files
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
433/3:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
433/4:
# show the shape of the datasets
print('Train      :', df_train.shape)
print('Validation :', df_validation.shape)
print('Test       :', df_test.shape)
433/5:
# head of the training set
display(df_train.head())
433/6:
# unique sessions in each dataset
print('Train      :', df_train.SessionId.nunique())
print('Validation :', df_validation.SessionId.nunique())
print('Test       :', df_test.SessionId.nunique())
433/7:
# average session length in each dataset
print('--- Train ---')
print(df_train.groupby('SessionId').size().describe())
print('--- Validation ---')
print(df_validation.groupby('SessionId').size().describe())
print('--- Test ---')
print(df_test.groupby('SessionId').size().describe())
433/8:
# convert the items ids to strings
df_train['ItemId'] = df_train['ItemId'].astype(str)
df_validation['ItemId'] = df_validation['ItemId'].astype(str)
df_test['ItemId'] = df_test['ItemId'].astype(str)
433/9:
# Sort by SessionId and Time to ensure the order is correct
df_train_sorted = df_train.sort_values(by=['SessionId', 'Time'])
df_validation_sorted = df_validation.sort_values(by=['SessionId', 'Time'])
df_test_sorted = df_test.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
train_sequences = df_train_sorted.groupby('SessionId')['ItemId'].apply(list)
validation_sequences = df_validation_sorted.groupby('SessionId')['ItemId'].apply(list)
test_sequences = df_test_sorted.groupby('SessionId')['ItemId'].apply(list)
433/10: train_sequences.head(5)
433/11:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.pkl'
with open(tokenizer_path, 'w') as file:
    tokenizer.to_json()
433/12:
# tokenize the sequences
train_sequences_tokenized = tokenizer.texts_to_sequences(train_sequences)
validation_sequences_tokenized = tokenizer.texts_to_sequences(validation_sequences)
test_sequences_tokenized = tokenizer.texts_to_sequences(test_sequences)
433/13: train_sequences_tokenized[:5]
433/14:
# Determine the maximum sequence length for padding
max_sequence_length = train_sequences.map(len).max()

# pad the sequences
train_sequences_padded = pad_sequences(train_sequences_tokenized, maxlen=max_sequence_length, padding='post')
validation_sequences_padded = pad_sequences(validation_sequences_tokenized, maxlen=max_sequence_length, padding='post')
test_sequences_padded = pad_sequences(test_sequences_tokenized, maxlen=max_sequence_length, padding='post')
433/15: train_sequences_padded[:5]
433/16: model = Sequential()
433/17:
max_item_id = max(tokenizer.index_word.keys())  # get the maximum item id from the tokenizer
embedding_dim = 64                              # Size of the embedding vectors

# Embedding layer for vectorizing ItemIds into dense vectors
model.add(Embedding(input_dim=max_item_id + 1, output_dim=embedding_dim, input_length=max_sequence_length))
433/18:
# Conv1D + MaxPooling1D for local pattern recognition and dimensionality reduction
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(MaxPooling1D(5))

# Additional Conv1D + GlobalMaxPooling1D for deeper pattern recognition
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
#model.add(GlobalMaxPooling1D())
433/19: model.add(LSTM(32))  # LSTM layer with 32 units. Adjust the number of units based on your dataset and complexity.
433/20:
# Output layer for predicting the next item in the sequence
model.add(Dense(max_item_id, activation='softmax'))
433/21:
def recall_at_k(y_true, y_pred, k=10):
    # Assuming y_true and y_pred are batch-wise lists of true labels and predicted labels, respectively
    top_k_preds = K.np.argsort(y_pred)[:, -k:]  # Get indices of top-k predictions
    relevant_items = K.sum(K.any(K.in1d(top_k_preds, y_true), axis=1))  # Count relevant items in top-k predictions
    return relevant_items / K.cast(K.shape(y_true)[0], K.floatx())  # Compute recall

def mrr_at_k(y_true, y_pred, k=10):
    top_k_preds = K.np.argsort(y_pred)[:, -k:]
    ranks = K.np.argmax(K.in1d(top_k_preds, y_true), axis=1) + 1  # Find ranks of true items
    ranks[ranks > k] = 0  # Set ranks > k to 0 as they are not found within top-k
    return K.mean(1.0 / ranks[ranks > 0])  # Compute mean reciprocal rank
433/22:
# compile the model
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=[recall_at_k, mrr_at_k])

model.summary()
434/1:
import pandas as pd
from keras import backend as K
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dense
from keras.optimizers import Adam
437/1:
import pandas as pd
from keras import backend as K
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dense
from keras.optimizers import Adam
437/2:
data_path = '../../data/coveo'
model_path = '../../models/coveo'

# file paths for the data files
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
437/3:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
437/4:
# show the shape of the datasets
print('Train      :', df_train.shape)
print('Validation :', df_validation.shape)
print('Test       :', df_test.shape)
437/5:
# head of the training set
display(df_train.head())
437/6:
# unique sessions in each dataset
print('Train      :', df_train.SessionId.nunique())
print('Validation :', df_validation.SessionId.nunique())
print('Test       :', df_test.SessionId.nunique())
437/7:
# average session length in each dataset
print('--- Train ---')
print(df_train.groupby('SessionId').size().describe())
print('--- Validation ---')
print(df_validation.groupby('SessionId').size().describe())
print('--- Test ---')
print(df_test.groupby('SessionId').size().describe())
437/8:
# convert the items ids to strings
df_train['ItemId'] = df_train['ItemId'].astype(str)
df_validation['ItemId'] = df_validation['ItemId'].astype(str)
df_test['ItemId'] = df_test['ItemId'].astype(str)
437/9:
# Sort by SessionId and Time to ensure the order is correct
df_train_sorted = df_train.sort_values(by=['SessionId', 'Time'])
df_validation_sorted = df_validation.sort_values(by=['SessionId', 'Time'])
df_test_sorted = df_test.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
train_sequences = df_train_sorted.groupby('SessionId')['ItemId'].apply(list)
validation_sequences = df_validation_sorted.groupby('SessionId')['ItemId'].apply(list)
test_sequences = df_test_sorted.groupby('SessionId')['ItemId'].apply(list)
437/10: train_sequences.head(5)
437/11:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.json'
with open(tokenizer_path, 'w') as file:
    file.write(tokenizer.to_json())
437/12:
# tokenize the sequences
train_sequences_tokenized = tokenizer.texts_to_sequences(train_sequences)
validation_sequences_tokenized = tokenizer.texts_to_sequences(validation_sequences)
test_sequences_tokenized = tokenizer.texts_to_sequences(test_sequences)
437/13: train_sequences_tokenized[:5]
437/14:
# Determine the maximum sequence length for padding
max_sequence_length = train_sequences.map(len).max()

# pad the sequences
train_sequences_padded = pad_sequences(train_sequences_tokenized, maxlen=max_sequence_length, padding='post')
validation_sequences_padded = pad_sequences(validation_sequences_tokenized, maxlen=max_sequence_length, padding='post')
test_sequences_padded = pad_sequences(test_sequences_tokenized, maxlen=max_sequence_length, padding='post')
437/15: train_sequences_padded[:5]
437/16: model = Sequential()
437/17:
max_item_id = max(tokenizer.index_word.keys())  # get the maximum item id from the tokenizer
embedding_dim = 64                              # Size of the embedding vectors

# Embedding layer for vectorizing ItemIds into dense vectors
model.add(Embedding(input_dim=max_item_id + 1, output_dim=embedding_dim, input_length=max_sequence_length))
437/18:
# Conv1D + MaxPooling1D for local pattern recognition and dimensionality reduction
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(MaxPooling1D(5))

# Additional Conv1D + GlobalMaxPooling1D for deeper pattern recognition
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
#model.add(GlobalMaxPooling1D())
437/19: model.add(LSTM(32))  # LSTM layer with 32 units. Adjust the number of units based on your dataset and complexity.
437/20:
# Output layer for predicting the next item in the sequence
model.add(Dense(max_item_id, activation='softmax'))
437/21:
def recall_at_k(y_true, y_pred, k=10):
    # Assuming y_true and y_pred are batch-wise lists of true labels and predicted labels, respectively
    top_k_preds = K.np.argsort(y_pred)[:, -k:]  # Get indices of top-k predictions
    relevant_items = K.sum(K.any(K.in1d(top_k_preds, y_true), axis=1))  # Count relevant items in top-k predictions
    return relevant_items / K.cast(K.shape(y_true)[0], K.floatx())  # Compute recall

def mrr_at_k(y_true, y_pred, k=10):
    top_k_preds = K.np.argsort(y_pred)[:, -k:]
    ranks = K.np.argmax(K.in1d(top_k_preds, y_true), axis=1) + 1  # Find ranks of true items
    ranks[ranks > k] = 0  # Set ranks > k to 0 as they are not found within top-k
    return K.mean(1.0 / ranks[ranks > 0])  # Compute mean reciprocal rank
437/22:
# compile the model
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=[recall_at_k, mrr_at_k])

model.summary()
438/1:
import pandas as pd
from keras import backend as K
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dense
from keras.optimizers import Adam
438/2:
data_path = '../../data/coveo'
model_path = '../../models/coveo'

# file paths for the data files
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
438/3:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
438/4:
# show the shape of the datasets
print('Train      :', df_train.shape)
print('Validation :', df_validation.shape)
print('Test       :', df_test.shape)
438/5:
# head of the training set
display(df_train.head())
438/6:
# unique sessions in each dataset
print('Train      :', df_train.SessionId.nunique())
print('Validation :', df_validation.SessionId.nunique())
print('Test       :', df_test.SessionId.nunique())
438/7:
# average session length in each dataset
print('--- Train ---')
print(df_train.groupby('SessionId').size().describe())
print('--- Validation ---')
print(df_validation.groupby('SessionId').size().describe())
print('--- Test ---')
print(df_test.groupby('SessionId').size().describe())
438/8:
# convert the items ids to strings
df_train['ItemId'] = df_train['ItemId'].astype(str)
df_validation['ItemId'] = df_validation['ItemId'].astype(str)
df_test['ItemId'] = df_test['ItemId'].astype(str)
438/9:
# Sort by SessionId and Time to ensure the order is correct
df_train_sorted = df_train.sort_values(by=['SessionId', 'Time'])
df_validation_sorted = df_validation.sort_values(by=['SessionId', 'Time'])
df_test_sorted = df_test.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
train_sequences = df_train_sorted.groupby('SessionId')['ItemId'].apply(list)
validation_sequences = df_validation_sorted.groupby('SessionId')['ItemId'].apply(list)
test_sequences = df_test_sorted.groupby('SessionId')['ItemId'].apply(list)
438/10: train_sequences.head(5)
438/11:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.json'
with open(tokenizer_path, 'w') as file:
    file.write(tokenizer.to_json())
438/12:
# tokenize the sequences
train_sequences_tokenized = tokenizer.texts_to_sequences(train_sequences)
validation_sequences_tokenized = tokenizer.texts_to_sequences(validation_sequences)
test_sequences_tokenized = tokenizer.texts_to_sequences(test_sequences)
438/13: train_sequences_tokenized[:5]
438/14:
# use the last item as the target and the rest as the input
def split_input_target(sequence):
    return sequence[:-1], sequence[-1]

# split the sequences
train_sequences_input = list(map(split_input_target, train_sequences_tokenized))
validation_sequences_input = list(map(split_input_target, validation_sequences_tokenized))
test_sequences_input = list(map(split_input_target, test_sequences_tokenized))
438/15: train_sequences_input
438/16: train_sequences_input[:5]
438/17:
# use the last item as the target and the rest as the input
def split_input_target(sequence):
    return sequence[:-1], sequence[-1]

# split the sequences
train_sequences_input, train_sequences_target = list(map(split_input_target, train_sequences_tokenized))
validation_sequences_input, validation_sequences_target = list(map(split_input_target, validation_sequences_tokenized))
test_sequences_input, test_sequences_target = list(map(split_input_target, test_sequences_tokenized))
438/18:
# use the last item as the target and the rest as the input
def split_input_target(sequence):
    return sequence[:-1], sequence[-1]

# split the sequences
train_sequences_input, train_sequences_target = list(map(split_input_target, train_sequences_tokenized))
validation_sequences_input, validation_sequences_target = list(map(split_input_target, validation_sequences_tokenized))
test_sequences_input, test_sequences_target = list(map(split_input_target, test_sequences_tokenized))
438/19: list(map(split_input_target, train_sequences_tokenized))
438/20:
# use the last item as the target and the rest as the input
def split_input_target(sequence):
    return sequence[:-1], sequence[-1]

# split the sequences
train_sequences_input, ta = list(map(split_input_target, train_sequences_tokenized))
validation_sequences_input = list(map(split_input_target, validation_sequences_tokenized))
test_sequences_input = list(map(split_input_target, test_sequences_tokenized))
438/21:
# use the last item as the target and the rest as the input
def split_input_target(sequence):
    return sequence[:-1], sequence[-1]

# split the sequences
train_sequences_input = list(map(split_input_target, train_sequences_tokenized))
validation_sequences_input = list(map(split_input_target, validation_sequences_tokenized))
test_sequences_input = list(map(split_input_target, test_sequences_tokenized))
438/22: train_sequences_input[:5]
438/23:
# Determine the maximum sequence length for padding
max_sequence_length = max(map(len, train_sequences_tokenized))
438/24:
# Determine the maximum sequence length for padding
max_sequence_length = max(map(len, train_sequences_tokenized))

max_sequence_length
438/25:
# Determine the maximum sequence length for padding
max_sequence_length = train_sequences.map(len).max()

# pad the sequences
train_sequences_padded = pad_sequences(train_sequences_tokenized, maxlen=max_sequence_length, padding='post')
validation_sequences_padded = pad_sequences(validation_sequences_tokenized, maxlen=max_sequence_length, padding='post')
test_sequences_padded = pad_sequences(test_sequences_tokenized, maxlen=max_sequence_length, padding='post')
438/26: max_sequence_length
438/27: max_sequence_length
438/28:
# Determine the maximum sequence length for padding from the train_sequences_input
max_sequence_length = max(map(len, train_sequences_input))

max_sequence_length
438/29:
# Determine the maximum sequence length for padding from the train_sequences_input
# which is a list of tuples with an array for the sequence, and an integer for the target
max_sequence_length = max(map(lambda x: len(x[0]), train_sequences_input))

max_sequence_length
438/30:
# Determine the maximum sequence length for padding from the train_sequences_input
max_sequence_length = max(map(lambda x: len(x[0]), train_sequences_input))

max_sequence_length
438/31:
# use the last item as the target and the rest as the input
def split_input_target(sequence):
    return sequence[:-1], sequence[-1]

# split the sequences
train_sequences_input = list(map(split_input_target, train_sequences_tokenized))
# validation_sequences_input = list(map(split_input_target, validation_sequences_tokenized))
# test_sequences_input = list(map(split_input_target, test_sequences_tokenized))
438/32: train_sequences_input[:5]
438/33:
# use the last item as the target and the rest as the input
def split_input_target(sequence):
    return sequence[:-1], sequence[-1]

# split the sequences
train_sequences, train_target = zip(*map(split_input_target, train_sequences_tokenized))
#train_sequences_input = list(map(split_input_target, train_sequences_tokenized))
# validation_sequences_input = list(map(split_input_target, validation_sequences_tokenized))
# test_sequences_input = list(map(split_input_target, test_sequences_tokenized))
438/34: train_sequences_input[:5]
438/35: train_sequences[:5]
438/36: train_sequences
438/37: type(train_sequences)
438/38: train_sequences
438/39: train_target
438/40:
# use the last item as the target and the rest as the input
def split_input_target(sequence):
    return sequence[:-1], sequence[-1]

# split the sequences
train_sequences, train_target = *map(split_input_target, train_sequences_tokenized)
#train_sequences_input = list(map(split_input_target, train_sequences_tokenized))
# validation_sequences_input = list(map(split_input_target, validation_sequences_tokenized))
# test_sequences_input = list(map(split_input_target, test_sequences_tokenized))
438/41:
# use the last item as the target and the rest as the input
def split_input_target(sequence):
    return sequence[:-1], sequence[-1]

# split the sequences
train_sequences, train_target = zip(*map(split_input_target, train_sequences_tokenized))
#train_sequences_input = list(map(split_input_target, train_sequences_tokenized))
# validation_sequences_input = list(map(split_input_target, validation_sequences_tokenized))
# test_sequences_input = list(map(split_input_target, test_sequences_tokenized))
438/42:
# use the last item as the target and the rest as the input
def split_input_target(sequence):
    return sequence[:-1], sequence[-1]

# split the sequences and get the input and target arrays
train_sequences, train_target = zip(*map(split_input_target, train_sequences_tokenized))

#train_sequences_input = list(map(split_input_target, train_sequences_tokenized))
# validation_sequences_input = list(map(split_input_target, validation_sequences_tokenized))
# test_sequences_input = list(map(split_input_target, test_sequences_tokenized))
438/43: train_sequences
438/44: train_sequences[:5]
438/45:
# use the last item as the target and the rest as the input
def split_input_target(sequence):
    return sequence[:-1], sequence[-1]

# split the sequences and get the input and target arrays
train_sequences, train_target = list(zip(*map(split_input_target, train_sequences_tokenized)))

#train_sequences_input = list(map(split_input_target, train_sequences_tokenized))
# validation_sequences_input = list(map(split_input_target, validation_sequences_tokenized))
# test_sequences_input = list(map(split_input_target, test_sequences_tokenized))
438/46: train_sequences[:5]
438/47: train_sequences[:5]
438/48:
# use the last item as the target and the rest as the input
def split_input_target(sequence):
    return sequence[:-1], sequence[-1]

# split the sequences and get the input and target arrays
train_sequences, train_target = list(zip(*map(split_input_target, train_sequences_tokenized)))

#train_sequences_input = list(map(split_input_target, train_sequences_tokenized))
# validation_sequences_input = list(map(split_input_target, validation_sequences_tokenized))
# test_sequences_input = list(map(split_input_target, test_sequences_tokenized))
438/49: train_sequences[:5]
438/50: map(split_input_target, train_sequences_tokenized)
438/51: a = map(split_input_target, train_sequences_tokenized)
438/52:
a = map(split_input_target, train_sequences_tokenized)
a
438/53:
a = map(split_input_target, train_sequences_tokenized)
a[:5]
438/54:
a = list(map(split_input_target, train_sequences_tokenized))
a[:5]
438/55: a[1]
438/56: a[1]p0
438/57: a[1][0]
438/58: a[1][1]
438/59: a[0]
438/60: a[0][1]
438/61:
# use the last item as the target and the rest as the input
def split_input_target(sequence):
    return sequence[:-1], sequence[-1]

# split the sequences and get the input and target arrays
train_sequences, train_target = list(zip(*map(split_input_target, train_sequences_tokenized)))

#train_sequences_input = list(map(split_input_target, train_sequences_tokenized))
# validation_sequences_input = list(map(split_input_target, validation_sequences_tokenized))
# test_sequences_input = list(map(split_input_target, test_sequences_tokenized))
438/62: train_sequences[:5]
438/63: train_target[:5]
438/64:
# tuple to list
 list(train_target)
train_target[:5]
438/65:
# tuple to list
list(train_target)
#train_target[:5]
438/66:
# use the last item as the target and the rest as the input
def split_input_target(sequence):
    return sequence[:-1], sequence[-1]

# split the sequences and get the input and target arrays
train_sequences, train_target = zip(*map(split_input_target, train_sequences_tokenized))

#train_sequences_input = list(map(split_input_target, train_sequences_tokenized))
# validation_sequences_input = list(map(split_input_target, validation_sequences_tokenized))
# test_sequences_input = list(map(split_input_target, test_sequences_tokenized))
438/67:
a = list(map(split_input_target, train_sequences_tokenized))
a[:5]
438/68:
# use the last item as the target and the rest as the input
def split_input_target(sequence):
    return sequence[:-1], sequence[-1]

# split the sequences and get the input and target arrays
#train_sequences, train_target = zip(*map(split_input_target, train_sequences_tokenized))

train_sequences_input = list(map(split_input_target, train_sequences_tokenized))
validation_sequences_input = list(map(split_input_target, validation_sequences_tokenized))
test_sequences_input = list(map(split_input_target, test_sequences_tokenized))
438/69: train_sequences[:5]
438/70: train_sequences_input[:5]
438/71: train_sequences, train_target = map(list, zip(*train_sequences_input))
438/72:
train_sequences, train_target = map(list, zip(*train_sequences_input))
train_sequences
438/73:
train_sequences, train_target = map(list, zip(*train_sequences_input))
train_target
438/74:
# seperate into input and target arrays
train_sequences, train_target = map(list, zip(*train_sequences_input))
validation_sequences, validation_target = map(list, zip(*validation_sequences_input))
test_sequences, test_target = map(list, zip(*test_sequences_input))
438/75: train_sequences[:5]
438/76:
train_sequences[:5]
train_target[:5]
438/77:
print(train_sequences[:5])
print(train_target[:5])
438/78:
pprint(train_sequences[:5]
print(train_target[:5])
438/79:
pprint(train_sequences[:5])
print(train_target[:5])
438/80:
import pandas as pd
from pprint import pprint
from keras import backend as K
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dense
from keras.optimizers import Adam
438/81:
pprint(train_sequences[:5])
pprint(train_target[:5])
438/82:
pprint(train_sequences[:5])
print('-'*10)
pprint(train_target[:5])
439/1:
import pandas as pd
from pprint import pprint
from keras import backend as K
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dense
from keras.optimizers import Adam
439/2:
data_path = '../../data/coveo'
model_path = '../../models/coveo'

# file paths for the data files
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
439/3:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
439/4:
# show the shape of the datasets
print('Train      :', df_train.shape)
print('Validation :', df_validation.shape)
print('Test       :', df_test.shape)
439/5:
# head of the training set
display(df_train.head())
439/6:
# unique sessions in each dataset
print('Train      :', df_train.SessionId.nunique())
print('Validation :', df_validation.SessionId.nunique())
print('Test       :', df_test.SessionId.nunique())
439/7:
# average session length in each dataset
print('--- Train ---')
print(df_train.groupby('SessionId').size().describe())
print('--- Validation ---')
print(df_validation.groupby('SessionId').size().describe())
print('--- Test ---')
print(df_test.groupby('SessionId').size().describe())
439/8:
# convert the items ids to strings
df_train['ItemId'] = df_train['ItemId'].astype(str)
df_validation['ItemId'] = df_validation['ItemId'].astype(str)
df_test['ItemId'] = df_test['ItemId'].astype(str)
439/9:
# Sort by SessionId and Time to ensure the order is correct
df_train_sorted = df_train.sort_values(by=['SessionId', 'Time'])
df_validation_sorted = df_validation.sort_values(by=['SessionId', 'Time'])
df_test_sorted = df_test.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
train_sequences = df_train_sorted.groupby('SessionId')['ItemId'].apply(list)
validation_sequences = df_validation_sorted.groupby('SessionId')['ItemId'].apply(list)
test_sequences = df_test_sorted.groupby('SessionId')['ItemId'].apply(list)
439/10: train_sequences.head(5)
439/11:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.json'
with open(tokenizer_path, 'w') as file:
    file.write(tokenizer.to_json())
439/12:
# tokenize the sequences
train_sequences_tokenized = tokenizer.texts_to_sequences(train_sequences)
validation_sequences_tokenized = tokenizer.texts_to_sequences(validation_sequences)
test_sequences_tokenized = tokenizer.texts_to_sequences(test_sequences)
439/13: train_sequences_tokenized[:5]
439/14:
# use the last item as the target and the rest as the input
def split_input_target(sequence):
    return sequence[:-1], sequence[-1]

# split the sequences and get the input and target arrays
#train_sequences, train_target = zip(*map(split_input_target, train_sequences_tokenized))

train_sequences_input = list(map(split_input_target, train_sequences_tokenized))
validation_sequences_input = list(map(split_input_target, validation_sequences_tokenized))
test_sequences_input = list(map(split_input_target, test_sequences_tokenized))
439/15: train_sequences_input[:5]
439/16:
# seperate into input and target arrays
train_sequences, train_target = map(list, zip(*train_sequences_input))
validation_sequences, validation_target = map(list, zip(*validation_sequences_input))
test_sequences, test_target = map(list, zip(*test_sequences_input))
439/17:
pprint(train_sequences[:5])
print('-'*10)
pprint(train_target[:5])
439/18:
# separate into input and target arrays
train_input, train_target = map(list, zip(*train_sequences_input))
validation_input, validation_target = map(list, zip(*validation_sequences_input))
test_input, test_target = map(list, zip(*test_sequences_input))
441/1:
import pandas as pd
from pprint import pprint
from keras import backend as K
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dense
from keras.optimizers import Adam
441/2:
data_path = '../../data/coveo'
model_path = '../../models/coveo'

# file paths for the data files
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
441/3:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
441/4:
# show the shape of the datasets
print('Train      :', df_train.shape)
print('Validation :', df_validation.shape)
print('Test       :', df_test.shape)
441/5:
# head of the training set
display(df_train.head())
441/6:
# unique sessions in each dataset
print('Train      :', df_train.SessionId.nunique())
print('Validation :', df_validation.SessionId.nunique())
print('Test       :', df_test.SessionId.nunique())
441/7:
# average session length in each dataset
print('--- Train ---')
print(df_train.groupby('SessionId').size().describe())
print('--- Validation ---')
print(df_validation.groupby('SessionId').size().describe())
print('--- Test ---')
print(df_test.groupby('SessionId').size().describe())
441/8:
# convert the items ids to strings
df_train['ItemId'] = df_train['ItemId'].astype(str)
df_validation['ItemId'] = df_validation['ItemId'].astype(str)
df_test['ItemId'] = df_test['ItemId'].astype(str)
441/9:
# Sort by SessionId and Time to ensure the order is correct
df_train_sorted = df_train.sort_values(by=['SessionId', 'Time'])
df_validation_sorted = df_validation.sort_values(by=['SessionId', 'Time'])
df_test_sorted = df_test.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
train_sequences = df_train_sorted.groupby('SessionId')['ItemId'].apply(list)
validation_sequences = df_validation_sorted.groupby('SessionId')['ItemId'].apply(list)
test_sequences = df_test_sorted.groupby('SessionId')['ItemId'].apply(list)
441/10: train_sequences.head(5)
441/11:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.json'
with open(tokenizer_path, 'w') as file:
    file.write(tokenizer.to_json())
441/12:
# tokenize the sequences
train_sequences_tokenized = tokenizer.texts_to_sequences(train_sequences)
validation_sequences_tokenized = tokenizer.texts_to_sequences(validation_sequences)
test_sequences_tokenized = tokenizer.texts_to_sequences(test_sequences)
441/13: train_sequences_tokenized[:5]
441/14:
# use the last item as the target and the rest as the input
def split_input_target(sequence):
    return sequence[:-1], sequence[-1]

# split the sequences and get the input and target arrays
#train_sequences, train_target = zip(*map(split_input_target, train_sequences_tokenized))

train_sequences_input = list(map(split_input_target, train_sequences_tokenized))
validation_sequences_input = list(map(split_input_target, validation_sequences_tokenized))
test_sequences_input = list(map(split_input_target, test_sequences_tokenized))
441/15: train_sequences_input[:5]
441/16:
# separate into input and target arrays
train_input, train_target = map(list, zip(*train_sequences_input))
validation_input, validation_target = map(list, zip(*validation_sequences_input))
test_input, test_target = map(list, zip(*test_sequences_input))
441/17:
pprint(train_input[:5])
print('-'*10)
pprint(train_target[:5])
441/18:
# Determine the maximum sequence length for padding
max_sequence_length = train_input.map(len).max()

# pad the sequences
train_sequences_padded = pad_sequences(train_input, maxlen=max_sequence_length, padding='post')
validation_sequences_padded = pad_sequences(validation_input, maxlen=max_sequence_length, padding='post')
test_sequences_padded = pad_sequences(test_input, maxlen=max_sequence_length, padding='post')
441/19:
# Determine the maximum sequence length for padding
max_sequence_length = train_input.map(len).max()

# pad the sequences
train_sequences_padded = pad_sequences(train_sequences_tokenized, maxlen=max_sequence_length, padding='post')
validation_sequences_padded = pad_sequences(validation_sequences_tokenized, maxlen=max_sequence_length, padding='post')
test_sequences_padded = pad_sequences(test_sequences_tokenized, maxlen=max_sequence_length, padding='post')
441/20: train_input
441/21:
# Determine the maximum sequence length for padding from train_ibput
max_sequence_length = max(map(len, train_input))

# pad the sequences
train_sequences_padded = pad_sequences(train_sequences_tokenized, maxlen=max_sequence_length, padding='post')
validation_sequences_padded = pad_sequences(validation_sequences_tokenized, maxlen=max_sequence_length, padding='post')
test_sequences_padded = pad_sequences(test_sequences_tokenized, maxlen=max_sequence_length, padding='post')
441/22: max_sequence_length
441/23:
# Determine the maximum sequence length for padding from train_ibput
max_sequence_length = max(map(len, train_input))

# pad the sequences
train_sequences_padded = pad_sequences(train_sequences_tokenized, maxlen=max_sequence_length, padding='post')
validation_sequences_padded = pad_sequences(validation_sequences_tokenized, maxlen=max_sequence_length, padding='post')
test_sequences_padded = pad_sequences(test_sequences_tokenized, maxlen=max_sequence_length, padding='post')
441/24: train_sequences_padded[:5]
442/1:
import pandas as pd
from pprint import pprint
from keras import backend as K
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dense
from keras.optimizers import Adam
442/2:
data_path = '../../data/coveo'
model_path = '../../models/coveo'

# file paths for the data files
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
442/3:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
442/4:
# show the shape of the datasets
print('Train      :', df_train.shape)
print('Validation :', df_validation.shape)
print('Test       :', df_test.shape)
442/5:
# head of the training set
display(df_train.head())
442/6:
# unique sessions in each dataset
print('Train      :', df_train.SessionId.nunique())
print('Validation :', df_validation.SessionId.nunique())
print('Test       :', df_test.SessionId.nunique())
442/7:
# average session length in each dataset
print('--- Train ---')
print(df_train.groupby('SessionId').size().describe())
print('--- Validation ---')
print(df_validation.groupby('SessionId').size().describe())
print('--- Test ---')
print(df_test.groupby('SessionId').size().describe())
442/8:
# convert the items ids to strings
df_train['ItemId'] = df_train['ItemId'].astype(str)
df_validation['ItemId'] = df_validation['ItemId'].astype(str)
df_test['ItemId'] = df_test['ItemId'].astype(str)
442/9:
# Sort by SessionId and Time to ensure the order is correct
df_train_sorted = df_train.sort_values(by=['SessionId', 'Time'])
df_validation_sorted = df_validation.sort_values(by=['SessionId', 'Time'])
df_test_sorted = df_test.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
train_sequences = df_train_sorted.groupby('SessionId')['ItemId'].apply(list)
validation_sequences = df_validation_sorted.groupby('SessionId')['ItemId'].apply(list)
test_sequences = df_test_sorted.groupby('SessionId')['ItemId'].apply(list)
442/10: train_sequences.head(5)
442/11:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.json'
with open(tokenizer_path, 'w') as file:
    file.write(tokenizer.to_json())
442/12:
# tokenize the sequences
train_sequences_tokenized = tokenizer.texts_to_sequences(train_sequences)
validation_sequences_tokenized = tokenizer.texts_to_sequences(validation_sequences)
test_sequences_tokenized = tokenizer.texts_to_sequences(test_sequences)
442/13: train_sequences_tokenized[:5]
442/14:
# use the last item as the target and the rest as the input
def split_input_target(sequence):
    return sequence[:-1], sequence[-1]

train_sequences_input = list(map(split_input_target, train_sequences_tokenized))
validation_sequences_input = list(map(split_input_target, validation_sequences_tokenized))
test_sequences_input = list(map(split_input_target, test_sequences_tokenized))
442/15: train_sequences_input[:5]
442/16:
# separate into input and target arrays
train_input, train_target = map(list, zip(*train_sequences_input))
validation_input, validation_target = map(list, zip(*validation_sequences_input))
test_input, test_target = map(list, zip(*test_sequences_input))
442/17:
pprint(train_input[:5])
print('-'*10)
pprint(train_target[:5])
442/18:
# Determine the maximum sequence length for padding from train_ibput
max_sequence_length = max(map(len, train_input))

# pad the sequences
train_sequences_padded = pad_sequences(train_sequences_tokenized, maxlen=max_sequence_length, padding='post')
validation_sequences_padded = pad_sequences(validation_sequences_tokenized, maxlen=max_sequence_length, padding='post')
test_sequences_padded = pad_sequences(test_sequences_tokenized, maxlen=max_sequence_length, padding='post')
442/19: train_sequences_padded[:5]
442/20: model = Sequential()
442/21:
max_item_id = max(tokenizer.index_word.keys())  # get the maximum item id from the tokenizer
embedding_dim = 64                              # Size of the embedding vectors

# Embedding layer for vectorizing ItemIds into dense vectors
model.add(Embedding(input_dim=max_item_id + 1, output_dim=embedding_dim, input_length=max_sequence_length))
442/22:
# Conv1D + MaxPooling1D for local pattern recognition and dimensionality reduction
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(MaxPooling1D(5))

# Additional Conv1D + GlobalMaxPooling1D for deeper pattern recognition
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
#model.add(GlobalMaxPooling1D())
443/1:
import pandas as pd
from pprint import pprint
from keras import backend as K
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dense
from keras.optimizers import Adam
443/2:
data_path = '../../data/coveo'
model_path = '../../models/coveo'

# file paths for the data files
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
443/3:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
443/4:
# show the shape of the datasets
print('Train      :', df_train.shape)
print('Validation :', df_validation.shape)
print('Test       :', df_test.shape)
443/5:
# head of the training set
display(df_train.head())
443/6:
# unique sessions in each dataset
print('Train      :', df_train.SessionId.nunique())
print('Validation :', df_validation.SessionId.nunique())
print('Test       :', df_test.SessionId.nunique())
443/7:
# average session length in each dataset
print('--- Train ---')
print(df_train.groupby('SessionId').size().describe())
print('--- Validation ---')
print(df_validation.groupby('SessionId').size().describe())
print('--- Test ---')
print(df_test.groupby('SessionId').size().describe())
443/8:
# convert the items ids to strings
df_train['ItemId'] = df_train['ItemId'].astype(str)
df_validation['ItemId'] = df_validation['ItemId'].astype(str)
df_test['ItemId'] = df_test['ItemId'].astype(str)
443/9:
# Sort by SessionId and Time to ensure the order is correct
df_train_sorted = df_train.sort_values(by=['SessionId', 'Time'])
df_validation_sorted = df_validation.sort_values(by=['SessionId', 'Time'])
df_test_sorted = df_test.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
train_sequences = df_train_sorted.groupby('SessionId')['ItemId'].apply(list)
validation_sequences = df_validation_sorted.groupby('SessionId')['ItemId'].apply(list)
test_sequences = df_test_sorted.groupby('SessionId')['ItemId'].apply(list)
443/10: train_sequences.head(5)
443/11:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.json'
with open(tokenizer_path, 'w') as file:
    file.write(tokenizer.to_json())
443/12:
# tokenize the sequences
train_sequences_tokenized = tokenizer.texts_to_sequences(train_sequences)
validation_sequences_tokenized = tokenizer.texts_to_sequences(validation_sequences)
test_sequences_tokenized = tokenizer.texts_to_sequences(test_sequences)
443/13: train_sequences_tokenized[:5]
443/14:
# use the last item as the target and the rest as the input
def split_input_target(sequence):
    return sequence[:-1], sequence[-1]

train_sequences_input = list(map(split_input_target, train_sequences_tokenized))
validation_sequences_input = list(map(split_input_target, validation_sequences_tokenized))
test_sequences_input = list(map(split_input_target, test_sequences_tokenized))
443/15: train_sequences_input[:5]
443/16:
# separate into input and target arrays
train_input, train_target = map(list, zip(*train_sequences_input))
validation_input, validation_target = map(list, zip(*validation_sequences_input))
test_input, test_target = map(list, zip(*test_sequences_input))
443/17:
pprint(train_input[:5])
print('-'*10)
pprint(train_target[:5])
443/18:
# Determine the maximum sequence length for padding from train_ibput
max_sequence_length = max(map(len, train_input))

# pad the sequences
train_sequences_padded = pad_sequences(train_sequences_tokenized, maxlen=max_sequence_length, padding='post')
validation_sequences_padded = pad_sequences(validation_sequences_tokenized, maxlen=max_sequence_length, padding='post')
test_sequences_padded = pad_sequences(test_sequences_tokenized, maxlen=max_sequence_length, padding='post')
443/19: train_sequences_padded[:5]
443/20: model = Sequential()
443/21:
max_item_id = max(tokenizer.index_word.keys())  # get the maximum item id from the tokenizer
embedding_dim = 64                              # Size of the embedding vectors

# Embedding layer for vectorizing ItemIds into dense vectors
model.add(Embedding(input_dim=max_item_id + 1, output_dim=embedding_dim, input_length=max_sequence_length))
443/22:
# Conv1D + MaxPooling1D for local pattern recognition and dimensionality reduction
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(MaxPooling1D(5))

# Additional Conv1D + GlobalMaxPooling1D for deeper pattern recognition
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
#model.add(GlobalMaxPooling1D())
443/23: model.add(LSTM(32))  # LSTM layer with 32 units. Adjust the number of units based on your dataset and complexity.
443/24:
# Output layer for predicting the next item in the sequence
model.add(Dense(max_item_id, activation='softmax'))
443/25:
def recall_at_k(y_true, y_pred, k=10):
    # Assuming y_true and y_pred are batch-wise lists of true labels and predicted labels, respectively
    top_k_preds = K.np.argsort(y_pred)[:, -k:]  # Get indices of top-k predictions
    relevant_items = K.sum(K.any(K.in1d(top_k_preds, y_true), axis=1))  # Count relevant items in top-k predictions
    return relevant_items / K.cast(K.shape(y_true)[0], K.floatx())  # Compute recall

def mrr_at_k(y_true, y_pred, k=10):
    top_k_preds = K.np.argsort(y_pred)[:, -k:]
    ranks = K.np.argmax(K.in1d(top_k_preds, y_true), axis=1) + 1  # Find ranks of true items
    ranks[ranks > k] = 0  # Set ranks > k to 0 as they are not found within top-k
    return K.mean(1.0 / ranks[ranks > 0])  # Compute mean reciprocal rank
443/26:
# compile the model
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=[recall_at_k, mrr_at_k])

model.summary()
444/1:
# Determine the maximum sequence length for padding from train_ibput
max_sequence_length = max(map(len, train_input))

# pad the sequences
train_sequences_padded = pad_sequences(train_input, maxlen=max_sequence_length, padding='post')
validation_sequences_padded = pad_sequences(validation_input, maxlen=max_sequence_length, padding='post')
test_sequences_padded = pad_sequences(test_input, maxlen=max_sequence_length, padding='post')
445/1:
import pandas as pd
from pprint import pprint
from keras import backend as K
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dense
from keras.optimizers import Adam
445/2:
data_path = '../../data/coveo'
model_path = '../../models/coveo'

# file paths for the data files
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
445/3:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
445/4:
# show the shape of the datasets
print('Train      :', df_train.shape)
print('Validation :', df_validation.shape)
print('Test       :', df_test.shape)
445/5:
# head of the training set
display(df_train.head())
445/6:
# unique sessions in each dataset
print('Train      :', df_train.SessionId.nunique())
print('Validation :', df_validation.SessionId.nunique())
print('Test       :', df_test.SessionId.nunique())
445/7:
# average session length in each dataset
print('--- Train ---')
print(df_train.groupby('SessionId').size().describe())
print('--- Validation ---')
print(df_validation.groupby('SessionId').size().describe())
print('--- Test ---')
print(df_test.groupby('SessionId').size().describe())
445/8:
# convert the items ids to strings
df_train['ItemId'] = df_train['ItemId'].astype(str)
df_validation['ItemId'] = df_validation['ItemId'].astype(str)
df_test['ItemId'] = df_test['ItemId'].astype(str)
445/9:
# Sort by SessionId and Time to ensure the order is correct
df_train_sorted = df_train.sort_values(by=['SessionId', 'Time'])
df_validation_sorted = df_validation.sort_values(by=['SessionId', 'Time'])
df_test_sorted = df_test.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
train_sequences = df_train_sorted.groupby('SessionId')['ItemId'].apply(list)
validation_sequences = df_validation_sorted.groupby('SessionId')['ItemId'].apply(list)
test_sequences = df_test_sorted.groupby('SessionId')['ItemId'].apply(list)
445/10: train_sequences.head(5)
445/11:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.json'
with open(tokenizer_path, 'w') as file:
    file.write(tokenizer.to_json())
445/12:
# tokenize the sequences
train_sequences_tokenized = tokenizer.texts_to_sequences(train_sequences)
validation_sequences_tokenized = tokenizer.texts_to_sequences(validation_sequences)
test_sequences_tokenized = tokenizer.texts_to_sequences(test_sequences)
445/13: train_sequences_tokenized[:5]
445/14:
# use the last item as the target and the rest as the input
def split_input_target(sequence):
    return sequence[:-1], sequence[-1]

train_sequences_input = list(map(split_input_target, train_sequences_tokenized))
validation_sequences_input = list(map(split_input_target, validation_sequences_tokenized))
test_sequences_input = list(map(split_input_target, test_sequences_tokenized))
445/15: train_sequences_input[:5]
445/16:
# separate into input and target arrays
train_input, train_target = map(list, zip(*train_sequences_input))
validation_input, validation_target = map(list, zip(*validation_sequences_input))
test_input, test_target = map(list, zip(*test_sequences_input))
445/17:
pprint(train_input[:5])
print('-'*10)
pprint(train_target[:5])
445/18:
# Determine the maximum sequence length for padding from train_ibput
max_sequence_length = max(map(len, train_input))

# pad the sequences
train_sequences_padded = pad_sequences(train_input, maxlen=max_sequence_length, padding='post')
validation_sequences_padded = pad_sequences(validation_input, maxlen=max_sequence_length, padding='post')
test_sequences_padded = pad_sequences(test_input, maxlen=max_sequence_length, padding='post')
445/19: train_sequences_padded[:5]
445/20: model = Sequential()
445/21:
max_item_id = max(tokenizer.index_word.keys())  # get the maximum item id from the tokenizer
embedding_dim = 64                              # Size of the embedding vectors

# Embedding layer for vectorizing ItemIds into dense vectors
model.add(Embedding(input_dim=max_item_id + 1, output_dim=embedding_dim, input_length=max_sequence_length))
445/22:
# Conv1D + MaxPooling1D for local pattern recognition and dimensionality reduction
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(MaxPooling1D(5))

# Additional Conv1D + GlobalMaxPooling1D for deeper pattern recognition
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
#model.add(GlobalMaxPooling1D())
445/23: model.add(LSTM(32))  # LSTM layer with 32 units. Adjust the number of units based on your dataset and complexity.
445/24:
# Output layer for predicting the next item in the sequence
model.add(Dense(max_item_id, activation='softmax'))
445/25:
def recall_at_k(y_true, y_pred, k=10):
    # Assuming y_true and y_pred are batch-wise lists of true labels and predicted labels, respectively
    top_k_preds = K.np.argsort(y_pred)[:, -k:]  # Get indices of top-k predictions
    relevant_items = K.sum(K.any(K.in1d(top_k_preds, y_true), axis=1))  # Count relevant items in top-k predictions
    return relevant_items / K.cast(K.shape(y_true)[0], K.floatx())  # Compute recall

def mrr_at_k(y_true, y_pred, k=10):
    top_k_preds = K.np.argsort(y_pred)[:, -k:]
    ranks = K.np.argmax(K.in1d(top_k_preds, y_true), axis=1) + 1  # Find ranks of true items
    ranks[ranks > k] = 0  # Set ranks > k to 0 as they are not found within top-k
    return K.mean(1.0 / ranks[ranks > 0])  # Compute mean reciprocal rank
445/26:
# compile the model
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=[recall_at_k, mrr_at_k])

model.summary()
446/1:
# separate into input and target arrays
train_input, y_train = map(list, zip(*train_sequences_input))
validation_input, y_validation = map(list, zip(*validation_sequences_input))
test_input, y_test = map(list, zip(*test_sequences_input))
449/1:
import pandas as pd
from pprint import pprint
from keras import backend as K
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dense
from keras.optimizers import Adam
449/2:
data_path = '../../data/coveo'
model_path = '../../models/coveo'

# file paths for the data files
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
449/3:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
449/4:
# show the shape of the datasets
print('Train      :', df_train.shape)
print('Validation :', df_validation.shape)
print('Test       :', df_test.shape)
449/5:
# head of the training set
display(df_train.head())
449/6:
# unique sessions in each dataset
print('Train      :', df_train.SessionId.nunique())
print('Validation :', df_validation.SessionId.nunique())
print('Test       :', df_test.SessionId.nunique())
449/7:
# average session length in each dataset
print('--- Train ---')
print(df_train.groupby('SessionId').size().describe())
print('--- Validation ---')
print(df_validation.groupby('SessionId').size().describe())
print('--- Test ---')
print(df_test.groupby('SessionId').size().describe())
449/8:
# convert the items ids to strings
df_train['ItemId'] = df_train['ItemId'].astype(str)
df_validation['ItemId'] = df_validation['ItemId'].astype(str)
df_test['ItemId'] = df_test['ItemId'].astype(str)
449/9:
# Sort by SessionId and Time to ensure the order is correct
df_train_sorted = df_train.sort_values(by=['SessionId', 'Time'])
df_validation_sorted = df_validation.sort_values(by=['SessionId', 'Time'])
df_test_sorted = df_test.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
train_sequences = df_train_sorted.groupby('SessionId')['ItemId'].apply(list)
validation_sequences = df_validation_sorted.groupby('SessionId')['ItemId'].apply(list)
test_sequences = df_test_sorted.groupby('SessionId')['ItemId'].apply(list)
449/10: train_sequences.head(5)
449/11:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.json'
with open(tokenizer_path, 'w') as file:
    file.write(tokenizer.to_json())
449/12:
# tokenize the sequences
train_sequences_tokenized = tokenizer.texts_to_sequences(train_sequences)
validation_sequences_tokenized = tokenizer.texts_to_sequences(validation_sequences)
test_sequences_tokenized = tokenizer.texts_to_sequences(test_sequences)
449/13: train_sequences_tokenized[:5]
449/14:
# use the last item as the target and the rest as the input
def split_input_target(sequence):
    return sequence[:-1], sequence[-1]

train_sequences_input = list(map(split_input_target, train_sequences_tokenized))
validation_sequences_input = list(map(split_input_target, validation_sequences_tokenized))
test_sequences_input = list(map(split_input_target, test_sequences_tokenized))
449/15: train_sequences_input[:5]
449/16:
# separate into input and target arrays
train_input, y_train = map(list, zip(*train_sequences_input))
validation_input, y_validation = map(list, zip(*validation_sequences_input))
test_input, y_test = map(list, zip(*test_sequences_input))
449/17:
pprint(train_input[:5])
print('-'*10)
pprint(y_train[:5])
449/18:
# Determine the maximum sequence length for padding from train_ibput
max_sequence_length = max(map(len, train_input))

# pad the sequences
X_train = pad_sequences(train_input, maxlen=max_sequence_length, padding='post')
X_validation = pad_sequences(validation_input, maxlen=max_sequence_length, padding='post')
X_test = pad_sequences(test_input, maxlen=max_sequence_length, padding='post')
449/19: X_train[:5]
450/1:
import pandas as pd
from pprint import pprint
from keras import backend as K
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dense
from keras.optimizers import Adam
450/2:
data_path = '../../data/coveo'
model_path = '../../models/coveo'

# file paths for the data files
train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'
validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'
test_path = f'{data_path}/coveo_processed_view_test.tsv'
450/3:
# load the datasets
df_train = pd.read_csv(train_path, sep='\t')
df_validation = pd.read_csv(validation_path, sep='\t')
df_test = pd.read_csv(test_path, sep='\t')
450/4:
# show the shape of the datasets
print('Train      :', df_train.shape)
print('Validation :', df_validation.shape)
print('Test       :', df_test.shape)
450/5:
# head of the training set
display(df_train.head())
450/6:
# unique sessions in each dataset
print('Train      :', df_train.SessionId.nunique())
print('Validation :', df_validation.SessionId.nunique())
print('Test       :', df_test.SessionId.nunique())
450/7:
# average session length in each dataset
print('--- Train ---')
print(df_train.groupby('SessionId').size().describe())
print('--- Validation ---')
print(df_validation.groupby('SessionId').size().describe())
print('--- Test ---')
print(df_test.groupby('SessionId').size().describe())
450/8:
# convert the items ids to strings
df_train['ItemId'] = df_train['ItemId'].astype(str)
df_validation['ItemId'] = df_validation['ItemId'].astype(str)
df_test['ItemId'] = df_test['ItemId'].astype(str)
450/9:
# Sort by SessionId and Time to ensure the order is correct
df_train_sorted = df_train.sort_values(by=['SessionId', 'Time'])
df_validation_sorted = df_validation.sort_values(by=['SessionId', 'Time'])
df_test_sorted = df_test.sort_values(by=['SessionId', 'Time'])

# Create sequences of ItemIds grouped by SessionId
train_sequences = df_train_sorted.groupby('SessionId')['ItemId'].apply(list)
validation_sequences = df_validation_sorted.groupby('SessionId')['ItemId'].apply(list)
test_sequences = df_test_sorted.groupby('SessionId')['ItemId'].apply(list)
450/10: train_sequences.head(5)
450/11:
# get a list of the unique item ids across all datasets
unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()

# use keras to map the item ids to a sequential list of integer values,
# 0 should be for padding and 1 for out of vocabulary items
tokenizer = Tokenizer(num_words=len(unique_items) + 2, oov_token=1)
tokenizer.fit_on_texts(unique_items)

# save the tokenizer
tokenizer_path = f'{model_path}/item_id_tokenizer.json'
with open(tokenizer_path, 'w') as file:
    file.write(tokenizer.to_json())
450/12:
# tokenize the sequences
train_sequences_tokenized = tokenizer.texts_to_sequences(train_sequences)
validation_sequences_tokenized = tokenizer.texts_to_sequences(validation_sequences)
test_sequences_tokenized = tokenizer.texts_to_sequences(test_sequences)
450/13: train_sequences_tokenized[:5]
450/14:
# use the last item as the target and the rest as the input
def split_input_target(sequence):
    return sequence[:-1], sequence[-1]

train_sequences_input = list(map(split_input_target, train_sequences_tokenized))
validation_sequences_input = list(map(split_input_target, validation_sequences_tokenized))
test_sequences_input = list(map(split_input_target, test_sequences_tokenized))
450/15: train_sequences_input[:5]
450/16:
# separate into input and target arrays
train_input, y_train = map(list, zip(*train_sequences_input))
validation_input, y_validation = map(list, zip(*validation_sequences_input))
test_input, y_test = map(list, zip(*test_sequences_input))
450/17:
pprint(train_input[:5])
print('-'*10)
pprint(y_train[:5])
450/18:
# Determine the maximum sequence length for padding from train_ibput
max_sequence_length = max(map(len, train_input))

# pad the sequences
X_train = pad_sequences(train_input, maxlen=max_sequence_length, padding='post')
X_validation = pad_sequences(validation_input, maxlen=max_sequence_length, padding='post')
X_test = pad_sequences(test_input, maxlen=max_sequence_length, padding='post')
450/19: X_train[:5]
450/20: model = Sequential()
450/21:
max_item_id = max(tokenizer.index_word.keys())  # get the maximum item id from the tokenizer
embedding_dim = 64                              # Size of the embedding vectors

# Embedding layer for vectorizing ItemIds into dense vectors
model.add(Embedding(input_dim=max_item_id + 1, output_dim=embedding_dim, input_length=max_sequence_length))
450/22:
# Conv1D + MaxPooling1D for local pattern recognition and dimensionality reduction
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(MaxPooling1D(5))

# Additional Conv1D + GlobalMaxPooling1D for deeper pattern recognition
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
#model.add(GlobalMaxPooling1D())
450/23: model.add(LSTM(32))  # LSTM layer with 32 units. Adjust the number of units based on your dataset and complexity.
450/24:
# Output layer for predicting the next item in the sequence
model.add(Dense(max_item_id, activation='softmax'))
450/25:
def recall_at_k(y_true, y_pred, k=10):
    # Assuming y_true and y_pred are batch-wise lists of true labels and predicted labels, respectively
    top_k_preds = K.np.argsort(y_pred)[:, -k:]  # Get indices of top-k predictions
    relevant_items = K.sum(K.any(K.in1d(top_k_preds, y_true), axis=1))  # Count relevant items in top-k predictions
    return relevant_items / K.cast(K.shape(y_true)[0], K.floatx())  # Compute recall

def mrr_at_k(y_true, y_pred, k=10):
    top_k_preds = K.np.argsort(y_pred)[:, -k:]
    ranks = K.np.argmax(K.in1d(top_k_preds, y_true), axis=1) + 1  # Find ranks of true items
    ranks[ranks > k] = 0  # Set ranks > k to 0 as they are not found within top-k
    return K.mean(1.0 / ranks[ranks > 0])  # Compute mean reciprocal rank
450/26:
# compile the model
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=[recall_at_k, mrr_at_k])

model.summary()
450/27:
# Train the model
history = model.fit(X_train, y_train_categorical,
                    epochs=10,  # Number of epochs to train for
                    batch_size=128,  # Batch size
                    validation_data=(X_val, y_val_categorical))
450/28:
# Train the model
history = model.fit(X_train, y_train,
                    epochs=10,  # Number of epochs to train for
                    batch_size=128,  # Batch size
                    validation_data=(X_validation, y_validation))
450/29:
import pandas as pd
from pprint import pprint
from keras import backend as K
from keras.utils import to_categorical
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dense
from keras.optimizers import Adam
450/30:
# Train the model
history = model.fit(X_train, to_categorical(y_train, max_item_id+1),
                    epochs=10,  # Number of epochs to train for
                    batch_size=128,  # Batch size
                    validation_data=(X_validation, to_categorical(y_validation, max_item_id+1)))
   1: %history
   2: %history -g -f filename
