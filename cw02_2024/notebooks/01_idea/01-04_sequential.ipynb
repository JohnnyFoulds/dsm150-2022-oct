{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01-04 : Retrieval using a sequential model\n",
    "\n",
    "Recommender systems are often composed of two stages:\n",
    "\n",
    "1. The retrieval stage is responsible for selecting an initial set of hundreds of candidates from all possible candidates. The main objective of this model is to efficiently weed out all candidates that the user is not interested in. Because the retrieval model may be dealing with millions of candidates, it has to be computationally efficient.\n",
    "\n",
    "2. The ranking stage takes the outputs of the retrieval model and fine-tunes them to select the best possible handful of recommendations. Its task is to narrow down the set of items the user may be interested in to a shortlist of likely candidates.\n",
    "\n",
    "This notebook is going to focus on the first stage, retrieval.\n",
    "\n",
    "Retrieval models are often composed of two sub-models:\n",
    "\n",
    "1. A query model computing the query representation (normally a fixed-dimensionality embedding vector) using query features.\n",
    "\n",
    "2. A candidate model computing the candidate representation (an equally-sized vector) using the candidate features.\n",
    "\n",
    "The outputs of the two models are then multiplied together to give a query-candidate affinity score, with higher scores expressing a better match between the candidate and the query.\n",
    "\n",
    "## References\n",
    "\n",
    "- [Recommending movies: retrieval using a sequential model](https://www.tensorflow.org/recommenders/examples/sequential_retrieval)\n",
    "- [Item-to-item recommendation and sequential recommendation](https://www.youtube.com/watch?v=ZBaKzw938oM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-03 13:47:41.394022: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-03 13:47:41.394053: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-03 13:47:41.394970: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-03 13:47:41.399509: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-03 13:47:41.858246: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The dataset\n",
    "\n",
    "We will use the RetailRocket source dataset as prepared for the GRU4Rec paper:\n",
    "https://github.com/JohnnyFoulds/GRU4Rec/blob/master/notebooks/01_%20preprocess/01-02_retailrocket.ipynb\n",
    "\n",
    "The dataset is already split into training, validation, and test sets as tab separated files. The columns are:\n",
    "\n",
    "- `SessionId` - the id of the session. In one session there are one or many items.\n",
    "- `ItemId` - the id of the item.\n",
    "- `Time` - the event time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/RetailRocket'\n",
    "model_path = '../../models/RetailRocket'\n",
    "\n",
    "# file paths for the data files\n",
    "train_path = f'{data_path}/retailrocket_processed_view_train_tr.tsv'\n",
    "validation_path = f'{data_path}/retailrocket_processed_view_train_valid.tsv'\n",
    "test_path = f'{data_path}/retailrocket_processed_view_test.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datasets\n",
    "df_train = pd.read_csv(train_path, sep='\\t').sample(frac=0.3, random_state=42)\n",
    "df_validation = pd.read_csv(validation_path, sep='\\t')\n",
    "df_test = pd.read_csv(test_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train      : (215097, 3)\n",
      "Validation : (33812, 3)\n",
      "Test       : (29148, 3)\n"
     ]
    }
   ],
   "source": [
    "# show the shape of the datasets\n",
    "print('Train      :', df_train.shape)\n",
    "print('Validation :', df_validation.shape)\n",
    "print('Test       :', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150593</th>\n",
       "      <td>363804</td>\n",
       "      <td>451942</td>\n",
       "      <td>1433134557529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700326</th>\n",
       "      <td>1684469</td>\n",
       "      <td>441756</td>\n",
       "      <td>1440134153810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673447</th>\n",
       "      <td>1615632</td>\n",
       "      <td>357925</td>\n",
       "      <td>1434672250853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48855</th>\n",
       "      <td>117260</td>\n",
       "      <td>2129</td>\n",
       "      <td>1435772219980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515183</th>\n",
       "      <td>1231963</td>\n",
       "      <td>7804</td>\n",
       "      <td>1432004462904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        SessionId  ItemId           Time\n",
       "150593     363804  451942  1433134557529\n",
       "700326    1684469  441756  1440134153810\n",
       "673447    1615632  357925  1434672250853\n",
       "48855      117260    2129  1435772219980\n",
       "515183    1231963    7804  1432004462904"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# head of the training set\n",
    "display(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train      : 119950\n",
      "Validation : 9408\n",
      "Test       : 8036\n"
     ]
    }
   ],
   "source": [
    "# unique sessions in each dataset\n",
    "print('Train      :', df_train.SessionId.nunique())\n",
    "print('Validation :', df_validation.SessionId.nunique())\n",
    "print('Test       :', df_test.SessionId.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train ---\n",
      "count    119950.000000\n",
      "mean          1.793222\n",
      "std           2.249649\n",
      "min           1.000000\n",
      "25%           1.000000\n",
      "50%           1.000000\n",
      "75%           2.000000\n",
      "max          97.000000\n",
      "dtype: float64\n",
      "--- Validation ---\n",
      "count    9408.000000\n",
      "mean        3.593963\n",
      "std         5.100989\n",
      "min         2.000000\n",
      "25%         2.000000\n",
      "50%         2.000000\n",
      "75%         4.000000\n",
      "max       147.000000\n",
      "dtype: float64\n",
      "--- Test ---\n",
      "count    8036.000000\n",
      "mean        3.627178\n",
      "std         5.460967\n",
      "min         2.000000\n",
      "25%         2.000000\n",
      "50%         2.000000\n",
      "75%         4.000000\n",
      "max       200.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# average session length in each dataset\n",
    "print('--- Train ---')\n",
    "print(df_train.groupby('SessionId').size().describe())\n",
    "print('--- Validation ---')\n",
    "print(df_validation.groupby('SessionId').size().describe())\n",
    "print('--- Test ---')\n",
    "print(df_test.groupby('SessionId').size().describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the items ids to strings for tokenization\n",
    "df_train['ItemId'] = df_train['ItemId'].astype(str)\n",
    "df_validation['ItemId'] = df_validation['ItemId'].astype(str)\n",
    "df_test['ItemId'] = df_test['ItemId'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Sequence Creation\n",
    "\n",
    "The first step involves creating sequences of item interactions for each session. This requires grouping the data by SessionId and ordering it within each group based on the Time column. Each sequence represents a series of item interactions within a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by SessionId and Time to ensure the order is correct\n",
    "df_train_sorted = df_train.sort_values(by=['SessionId', 'Time'])\n",
    "df_validation_sorted = df_validation.sort_values(by=['SessionId', 'Time'])\n",
    "df_test_sorted = df_test.sort_values(by=['SessionId', 'Time'])\n",
    "\n",
    "# Create sequences of ItemIds grouped by SessionId\n",
    "train_sequences = df_train_sorted.groupby('SessionId')['ItemId'].apply(list)\n",
    "validation_sequences = df_validation_sorted.groupby('SessionId')['ItemId'].apply(list)\n",
    "test_sequences = df_test_sorted.groupby('SessionId')['ItemId'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SessionId\n",
       "2             [216305]\n",
       "6     [253615, 344723]\n",
       "8             [164941]\n",
       "74            [321706]\n",
       "79            [233200]\n",
       "Name: ItemId, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SessionId\n",
       "6                           [253615, 344723]\n",
       "133                          [169956, 45520]\n",
       "135                         [400946, 400946]\n",
       "211                           [248862, 1152]\n",
       "226    [397068, 18519, 27248, 10034, 254301]\n",
       "Name: ItemId, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the sessions with only one item\n",
    "train_sequences = train_sequences[train_sequences.map(len) > 1]\n",
    "train_sequences.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenization (Categorical Features Encoding)\n",
    "\n",
    "We need to ensure that ItemIds are treated as categorical inputs.\n",
    "\n",
    "Create a tokenizer to encode ItemIds as integers, 0 and 1 are special values, where 0 should be for padding and 1 for out of vocabulary items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of the unique item ids across all datasets\n",
    "unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()\n",
    "\n",
    "# use keras to map the item ids to a sequential list of integer values,\n",
    "# 0 should be for padding and 1 for out of vocabulary items\n",
    "tokenizer = Tokenizer(num_words=len(unique_items) + 2, oov_token=1)\n",
    "tokenizer.fit_on_texts(unique_items)\n",
    "\n",
    "# save the tokenizer\n",
    "tokenizer_path = f'{model_path}/item_id_tokenizer.json'\n",
    "with open(tokenizer_path, 'w') as file:\n",
    "    file.write(tokenizer.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the sequences\n",
    "train_sequences_tokenized = tokenizer.texts_to_sequences(train_sequences)\n",
    "validation_sequences_tokenized = tokenizer.texts_to_sequences(validation_sequences)\n",
    "test_sequences_tokenized = tokenizer.texts_to_sequences(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[965, 2568],\n",
       " [29597, 14106],\n",
       " [1455, 1455],\n",
       " [1260, 6973],\n",
       " [178, 18563, 10273, 14326, 2998]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences_tokenized[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Padding\n",
    "\n",
    "To handle sessions of varying lengths, we'll need to pad the sequences so that they all have the same length, making them suitable for batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the maximum sequence length for padding\n",
    "#max_sequence_length = max(map(len, train_input))\n",
    "max_sequence_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the last item as the target and the rest as the input\n",
    "def split_input_target(sequence):\n",
    "    return sequence[:-1], sequence[-1]\n",
    "\n",
    "train_sequences_input = list(map(split_input_target, train_sequences_tokenized))\n",
    "validation_sequences_input = list(map(split_input_target, validation_sequences_tokenized))\n",
    "test_sequences_input = list(map(split_input_target, test_sequences_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([965], 2568),\n",
       " ([29597], 14106),\n",
       " ([1455], 1455),\n",
       " ([1260], 6973),\n",
       " ([178, 18563, 10273, 14326], 2998)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences_input[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[965]\n",
      "[29597]\n",
      "[1455]\n",
      "[1260]\n",
      "[178, 18563, 10273, 14326]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'context_item_id': [965], 'label_item_id': [2568]},\n",
       " {'context_item_id': [29597], 'label_item_id': [14106]},\n",
       " {'context_item_id': [1455], 'label_item_id': [1455]},\n",
       " {'context_item_id': [1260], 'label_item_id': [6973]},\n",
       " {'context_item_id': [178, 18563, 10273, 14326], 'label_item_id': [2998]}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_features(sequence):\n",
    "    print(sequence[0])\n",
    "    #X_train = pad_sequences(list(sequence[0]), maxlen=max_sequence_length, padding='post')\n",
    "    return {\n",
    "        'context_item_id': sequence[0], \n",
    "        'label_item_id': [sequence[1]]\n",
    "    }\n",
    "\n",
    "list(map(map_features, train_sequences_input[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into input and target arrays\n",
    "train_input, y_train = map(list, zip(*train_sequences_input))\n",
    "validation_input, y_validation = map(list, zip(*validation_sequences_input))\n",
    "test_input, y_test = map(list, zip(*test_sequences_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[965], [29597], [1455], [1260], [178, 18563, 10273, 14326]]\n",
      "----------\n",
      "[2568, 14106, 1455, 6973, 2998]\n"
     ]
    }
   ],
   "source": [
    "pprint(train_input[:5])\n",
    "print('-'*10)\n",
    "pprint(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the maximum sequence length for padding\n",
    "#max_sequence_length = max(map(len, train_input))\n",
    "max_sequence_length = 10\n",
    "\n",
    "# pad the sequences\n",
    "X_train = pad_sequences(train_input, maxlen=max_sequence_length, padding='post')\n",
    "X_validation = pad_sequences(validation_input, maxlen=max_sequence_length, padding='post')\n",
    "X_test = pad_sequences(test_input, maxlen=max_sequence_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  965,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [29597,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [ 1455,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [ 1260,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [  178, 18563, 10273, 14326,     0,     0,     0,     0,     0,\n",
       "            0]], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Create TensorFlow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_item_id': array([965,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32),\n",
      " 'label_item_id': 2568}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-03 13:47:44.529879: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-03 13:47:44.557045: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-03 13:47:44.557241: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-03 13:47:44.558246: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-03 13:47:44.558406: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-03 13:47:44.558546: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-03 13:47:44.620216: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-03 13:47:44.620384: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-03 13:47:44.620533: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-03 13:47:44.620649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10031 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:08:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Map each element of the dataset to the corresponding feature\n",
    "def map_feature(sequence, target):\n",
    "    return {'context_item_id': sequence, 'label_item_id': target}\n",
    "\n",
    "# Create a TensorFlow dataset\n",
    "train_ds = tf.data.Dataset \\\n",
    "    .from_tensor_slices((X_train, y_train)).map(map_feature)\n",
    "validation_ds = tf.data.Dataset \\\n",
    "    .from_tensor_slices((X_validation, y_validation)).map(map_feature)\n",
    "test_ds = tf.data.Dataset \\\n",
    "    .from_tensor_slices((X_test, y_test)).map(map_feature)\n",
    "\n",
    "for x in train_ds.take(1).as_numpy_iterator():\n",
    "  pprint(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsm150-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
