{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01-01: Data Preparation\n",
    "\n",
    "The basic idea is to create a sequential recommendation system in Keras by combining Convolutional Neural Networks (CNNs) with Recurrent Neural Networks (RNNs) for sequence prediction.\n",
    "\n",
    "Before applying deep learning models, preprocessing the sequence data is necessary. This involves tokenizing the sequences and possibly using techniques like padding to ensure that all input sequences have the same length. For your dataset, you might consider each session as a sequence and each item interaction within a session as a token within that sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_FLAGS\"]=\"--xla_gpu_cuda_data_dir=/usr/local/cuda-12.2\"\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 12:32:18.243342: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-01 12:32:18.243368: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-01 12:32:18.244237: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-01 12:32:18.248369: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-01 12:32:18.694013: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Description\n",
    "\n",
    "- Describe the dataset in detail: sources, features, and statistics.\n",
    "- Discuss any limitations or biases present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/coveo'\n",
    "model_path = '../../models/coveo'\n",
    "\n",
    "# file paths for the data files\n",
    "train_path = f'{data_path}/coveo_processed_view_train_tr.tsv'\n",
    "validation_path = f'{data_path}/coveo_processed_view_train_valid.tsv'\n",
    "test_path = f'{data_path}/coveo_processed_view_test.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datasets\n",
    "df_train = pd.read_csv(train_path, sep='\\t')\n",
    "df_validation = pd.read_csv(validation_path, sep='\\t')\n",
    "df_test = pd.read_csv(test_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train      : (1368003, 3)\n",
      "Validation : (43032, 3)\n",
      "Test       : (52501, 3)\n"
     ]
    }
   ],
   "source": [
    "# show the shape of the datasets\n",
    "print('Train      :', df_train.shape)\n",
    "print('Validation :', df_validation.shape)\n",
    "print('Test       :', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1544909035879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1544909231588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1544909334734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1545550811488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1545550910607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SessionId  ItemId           Time\n",
       "0          2       1  1544909035879\n",
       "1          2       2  1544909231588\n",
       "2          2       3  1544909334734\n",
       "3          5       4  1545550811488\n",
       "4          5       5  1545550910607"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# head of the training set\n",
    "display(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train      : 159766\n",
      "Validation : 5905\n",
      "Test       : 7748\n"
     ]
    }
   ],
   "source": [
    "# unique sessions in each dataset\n",
    "print('Train      :', df_train.SessionId.nunique())\n",
    "print('Validation :', df_validation.SessionId.nunique())\n",
    "print('Test       :', df_test.SessionId.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train ---\n",
      "count    159766.000000\n",
      "mean          8.562541\n",
      "std          10.909689\n",
      "min           2.000000\n",
      "25%           3.000000\n",
      "50%           5.000000\n",
      "75%          10.000000\n",
      "max         454.000000\n",
      "dtype: float64\n",
      "--- Validation ---\n",
      "count    5905.000000\n",
      "mean        7.287384\n",
      "std         8.974551\n",
      "min         2.000000\n",
      "25%         3.000000\n",
      "50%         4.000000\n",
      "75%         8.000000\n",
      "max       134.000000\n",
      "dtype: float64\n",
      "--- Test ---\n",
      "count    7748.000000\n",
      "mean        6.776071\n",
      "std         8.341588\n",
      "min         2.000000\n",
      "25%         2.000000\n",
      "50%         4.000000\n",
      "75%         8.000000\n",
      "max       146.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# average session length in each dataset\n",
    "print('--- Train ---')\n",
    "print(df_train.groupby('SessionId').size().describe())\n",
    "print('--- Validation ---')\n",
    "print(df_validation.groupby('SessionId').size().describe())\n",
    "print('--- Test ---')\n",
    "print(df_test.groupby('SessionId').size().describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "- Detail the preprocessing steps with explanations for each decision.\n",
    "- Show visualizations of data distribution, session lengths, and item frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the items ids to strings\n",
    "df_train['ItemId'] = df_train['ItemId'].astype(str)\n",
    "df_validation['ItemId'] = df_validation['ItemId'].astype(str)\n",
    "df_test['ItemId'] = df_test['ItemId'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Sequence Creation\n",
    "\n",
    "The first step involves creating sequences of item interactions for each session. This requires grouping the data by SessionId and ordering it within each group based on the Time column. Each sequence represents a series of item interactions within a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by SessionId and Time to ensure the order is correct\n",
    "df_train_sorted = df_train.sort_values(by=['SessionId', 'Time'])\n",
    "df_validation_sorted = df_validation.sort_values(by=['SessionId', 'Time'])\n",
    "df_test_sorted = df_test.sort_values(by=['SessionId', 'Time'])\n",
    "\n",
    "# Create sequences of ItemIds grouped by SessionId\n",
    "train_sequences = df_train_sorted.groupby('SessionId')['ItemId'].apply(list)\n",
    "validation_sequences = df_validation_sorted.groupby('SessionId')['ItemId'].apply(list)\n",
    "test_sequences = df_test_sorted.groupby('SessionId')['ItemId'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SessionId\n",
       "2                                             [1, 2, 3]\n",
       "5                                                [4, 5]\n",
       "13                                             [10, 11]\n",
       "14                                 [12, 13, 14, 15, 16]\n",
       "18    [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 3...\n",
       "Name: ItemId, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenization (Categorical Features Encoding)\n",
    "\n",
    "We need to ensure that ItemIds are treated as categorical inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tokenizer to encode ItemIds as integers, 0 and 1 are special values, where 0 should be for padding and 1 for out of vocabulary items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of the unique item ids across all datasets\n",
    "unique_items = pd.concat([df_train, df_validation, df_test]).ItemId.unique()\n",
    "\n",
    "# use keras to map the item ids to a sequential list of integer values,\n",
    "# 0 should be for padding and 1 for out of vocabulary items\n",
    "tokenizer = Tokenizer(num_words=len(unique_items) + 2, oov_token=1)\n",
    "tokenizer.fit_on_texts(unique_items)\n",
    "\n",
    "# save the tokenizer\n",
    "tokenizer_path = f'{model_path}/item_id_tokenizer.json'\n",
    "with open(tokenizer_path, 'w') as file:\n",
    "    file.write(tokenizer.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the sequences\n",
    "train_sequences_tokenized = tokenizer.texts_to_sequences(train_sequences)\n",
    "validation_sequences_tokenized = tokenizer.texts_to_sequences(validation_sequences)\n",
    "test_sequences_tokenized = tokenizer.texts_to_sequences(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 4],\n",
       " [5, 6],\n",
       " [7, 8],\n",
       " [9, 10, 11, 12, 13],\n",
       " [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 24, 27, 28]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences_tokenized[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Padding\n",
    "\n",
    "To handle sessions of varying lengths, we'll need to pad the sequences so that they all have the same length, making them suitable for batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the last item as the target and the rest as the input\n",
    "def split_input_target(sequence):\n",
    "    return sequence[:-1], sequence[-1]\n",
    "\n",
    "train_sequences_input = list(map(split_input_target, train_sequences_tokenized))\n",
    "validation_sequences_input = list(map(split_input_target, validation_sequences_tokenized))\n",
    "test_sequences_input = list(map(split_input_target, test_sequences_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([2, 3], 4),\n",
       " ([5], 6),\n",
       " ([7], 8),\n",
       " ([9, 10, 11, 12], 13),\n",
       " ([14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 24, 27], 28)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences_input[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into input and target arrays\n",
    "train_input, y_train = map(list, zip(*train_sequences_input))\n",
    "validation_input, y_validation = map(list, zip(*validation_sequences_input))\n",
    "test_input, y_test = map(list, zip(*test_sequences_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3],\n",
      " [5],\n",
      " [7],\n",
      " [9, 10, 11, 12],\n",
      " [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 24, 27]]\n",
      "----------\n",
      "[4, 6, 8, 13, 28]\n"
     ]
    }
   ],
   "source": [
    "pprint(train_input[:5])\n",
    "print('-'*10)\n",
    "pprint(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the maximum sequence length for padding\n",
    "max_sequence_length = max(map(len, train_input))\n",
    "\n",
    "# pad the sequences\n",
    "X_train = pad_sequences(train_input, maxlen=max_sequence_length, padding='post')\n",
    "X_validation = pad_sequences(validation_input, maxlen=max_sequence_length, padding='post')\n",
    "X_test = pad_sequences(test_input, maxlen=max_sequence_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3,  0, ...,  0,  0,  0],\n",
       "       [ 5,  0,  0, ...,  0,  0,  0],\n",
       "       [ 7,  0,  0, ...,  0,  0,  0],\n",
       "       [ 9, 10, 11, ...,  0,  0,  0],\n",
       "       [14, 15, 16, ...,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 12:32:23.043517: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-03-01 12:32:23.070367: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Embedding Layers\n",
    "\n",
    "Embedding layers transform sparse, integer-encoded data into dense vectors of fixed size. This is particularly useful for categorical data with many categories, where one-hot encoding would result in vectors that are mostly zeros and inefficient both in terms of memory and computation. Embeddings capture more information per byte and allow the model to learn relationships between categories (in this case, ItemIds).\n",
    "\n",
    "**Why It Is Done:**\n",
    "\n",
    "Dimensionality Reduction: Embeddings reduce the dimensionality of the categorical data, representing items in a more compact and dense vector space.\n",
    "Semantic Representation: The dense vectors obtained from embeddings can capture semantic relationships between items, such as similarity in user interaction patterns, which is not possible with one-hot encoding or similar techniques.\n",
    "Improved Learning: By learning an optimal representation of items in a lower-dimensional space, embeddings can improve the model's performance on the recommendation task.\n",
    "Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_item_id = max(tokenizer.index_word.keys())  # get the maximum item id from the tokenizer\n",
    "embedding_dim = 64                              # Size of the embedding vectors\n",
    "\n",
    "# Embedding layer for vectorizing ItemIds into dense vectors\n",
    "model.add(Embedding(input_dim=max_item_id + 1, output_dim=embedding_dim, input_length=max_sequence_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 CNN Layers\n",
    "\n",
    "CNNs, particularly 1D CNNs (Conv1D), are adept at recognizing local patterns within sequences. The key advantage of using CNNs in sequence data processing is their ability to apply the same transformation across different parts of a sequence, making them translation invariant. This means that if a CNN learns a particular pattern (e.g., a specific sequence of item interactions) at one point in a session, it can recognize the same pattern at a different point, enhancing the model's ability to understand user behavior across sessions.\n",
    "\n",
    "**Why It Is Done:**\n",
    "\n",
    "- **Local Pattern Recognition**: CNNs excel at identifying patterns within localized regions of the input data. For sequence data, this means recognizing specific sequences of item interactions that are indicative of broader user preferences.\n",
    "Translation Invariance: The ability of CNNs to recognize patterns regardless of their position in the sequence is particularly valuable for session-based recommendation systems, where the order of interactions can vary significantly across sessions.\n",
    "\n",
    "- **Efficiency and Scalability**: CNNs can process data more efficiently than fully connected networks, especially for large datasets. This efficiency makes them suitable for handling the vast amounts of data typically involved in recommendation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv1D + MaxPooling1D for local pattern recognition and dimensionality reduction\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "\n",
    "# Additional Conv1D + GlobalMaxPooling1D for deeper pattern recognition\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "#model.add(GlobalMaxPooling1D())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3  Recurrent Neural Network Layers\n",
    "\n",
    "Incorporating Recurrent Neural Networks (RNNs) for sequence modeling into a deep learning architecture that already includes Convolutional Neural Networks (CNNs) layers is a strategic approach to handle sequence data effectively, especially for tasks like sequential recommendation systems. This combination leverages the strengths of both CNNs and RNNs, where CNNs excel in extracting local and position-invariant features, and RNNs are adept at capturing temporal or sequential dependencies.\n",
    "\n",
    "The primary purpose of incorporating RNN layers after CNN layers is to model the temporal or sequential dependencies in the data. While CNNs are excellent for reducing a long sequence into a shorter one with extracted features (as mentioned in the document context about processing long sequences with CNNs before RNNs), they do not inherently understand the order or sequence of these features. RNNs, with their internal state and loop mechanism, can take the sequence of extracted features and learn patterns over time, making them ideal for tasks where the sequence order matters, such as predicting the next item in a session.\n",
    "\n",
    "The CNN layers act as a preprocessing step that transforms the raw sequence data into a higher-level, downsampled representation. This transformation is crucial because it allows the RNN layers to process the data more efficiently by focusing on the most relevant features extracted by the CNNs. The RNN can then model the temporal dynamics of these features across the sequence. This setup is particularly beneficial for handling sequences that are too long to be processed directly by RNNs, as it reduces the sequence length and complexity before reaching the RNN layers.\n",
    "\n",
    "RNNs are specifically designed to handle sequence data, making them a natural choice for sequence modeling tasks. They can maintain information about previous states or inputs through their hidden states, allowing them to make predictions based on the entire history of a sequence. This capability is crucial for understanding the context in sequential recommendation systems, where the relevance of an item can depend on the items that were interacted with previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(32))  # LSTM layer with 32 units. Adjust the number of units based on your dataset and complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Output Layer\n",
    "\n",
    "The output layer of a sequential recommendation system is crucial as it directly influences the system's ability to make accurate and relevant recommendations. The design of this layer depends on the specific task at hand â€“ in the case of a sequential recommendation system, the task is typically to predict the next item(s) a user is likely to interact with, based on their previous interactions within a session.\n",
    "\n",
    "Given the sequential nature of the data, where the order of interactions matters significantly, the output layer must capture and utilize the rich representations learned by the preceding layers (embedding, CNNs for local pattern recognition, and RNNs for capturing temporal dynamics) to make these predictions.\n",
    "\n",
    "### How It Relates to Preceding Layers:\n",
    "The output layer receives processed and transformed data from the previous layers, which have worked in tandem to encode the input sequences into a meaningful representation that captures both the content and context of user interactions. The embedding layer first converts item IDs into dense vectors, capturing similarity and relationships between items. The CNN layers then extract local patterns within these embeddings, and the RNN layers model the sequential dependencies. The output layer must leverage this rich representation to make accurate predictions.\n",
    "\n",
    "### Output Layer Design for Recommendation System:\n",
    "For a sequential recommendation system, the output layer often uses a softmax activation function. This function is suitable because it can output a probability distribution over the set of possible items, indicating the likelihood of each item being the next interaction. This approach requires the output layer to have as many units as there are unique items in the dataset, with each unit corresponding to a specific item.\n",
    "\n",
    "### Practical Use in Recommendation Systems:\n",
    "In practice, the output of the model can be used to rank items based on their predicted likelihood of being the next user interaction. The system can then recommend the top-N items with the highest probabilities. This method allows for personalized recommendations that adapt based on the user's current session activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer for predicting the next item in the sequence\n",
    "model.add(Dense(max_item_id+1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Evaluation Metrics\n",
    "\n",
    "It is important to establish a robust evaluation protocol and optimizing model performance, we'll focus on implementing and optimizing for the metrics Recall@K and MRR@K (Mean Reciprocal Rank at K). These metrics are particularly relevant for recommendation systems and ranking problems, where the goal is to accurately predict and rank a set of items in order of relevance.\n",
    "\n",
    "### Evaluation Metrics: Recall@K and MRR@K\n",
    "\n",
    "**Recall@K** measures the fraction of relevant items that are retrieved among the top-K recommendations. It's crucial for assessing the model's ability to capture the most relevant items within the first K suggestions.\n",
    "\n",
    "**MRR@K** (Mean Reciprocal Rank at K) focuses on the rank of the first relevant item, averaging this metric over multiple queries. It's valuable for understanding how well the model ranks the first relevant item, emphasizing the importance of items ranked closer to the top.\n",
    "\n",
    "Rational for Choosing Recall@K and MRR@K\n",
    "\n",
    "- Recall@K is essential for ensuring that the top-K recommendations contain items truly relevant to the user, enhancing user satisfaction and engagement.\n",
    "- MRR@K complements Recall@K by ensuring that not only are relevant items included in the top-K recommendations, but they are also ranked as high as possible.\n",
    "Implementation\n",
    "\n",
    "Given the emphasis on hyperparameter optimization and model evaluation, we'll implement custom functions for Recall@K and MRR@K and integrate them into the model training and evaluation process. This approach allows us to directly optimize and evaluate the model based on metrics that align closely with our objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def recall_at_k(y_true, y_pred, k=10):\n",
    "#     # Assuming y_true and y_pred are batch-wise lists of true labels and predicted labels, respectively\n",
    "#     top_k_preds = K.np.argsort(y_pred)[:, -k:]  # Get indices of top-k predictions\n",
    "#     relevant_items = K.sum(K.any(K.in1d(top_k_preds, y_true), axis=1))  # Count relevant items in top-k predictions\n",
    "#     return relevant_items / K.cast(K.shape(y_true)[0], K.floatx())  # Compute recall\n",
    "\n",
    "# def mrr_at_k(y_true, y_pred, k=10):\n",
    "#     top_k_preds = K.np.argsort(y_pred)[:, -k:]\n",
    "#     ranks = K.np.argmax(K.in1d(top_k_preds, y_true), axis=1) + 1  # Find ranks of true items\n",
    "#     ranks[ranks > k] = 0  # Set ranks > k to 0 as they are not found within top-k\n",
    "#     return K.mean(1.0 / ranks[ranks > 0])  # Compute mean reciprocal rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 453, 64)           695680    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 449, 128)          41088     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 89, 128)           0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 85, 128)           82048     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                20608     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10870)             358710    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1198134 (4.57 MB)\n",
      "Trainable params: 1198134 (4.57 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy')\n",
    "              #metrics=[tf.metrics.recall_at_k])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1064/1249 [========================>.....] - ETA: 10s - loss: 8.8149"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, to_categorical(y_train, max_item_id+1),\n",
    "                    epochs=10,  # Number of epochs to train for\n",
    "                    batch_size=128,  # Batch size\n",
    "                    validation_data=(X_validation, to_categorical(y_validation, max_item_id+1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsm150-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
